[
    {
        "questionNumber": "Question #1 -- Topic 1",
        "questionIs": "You have two Hyper-V hosts named Host1 and Host2. Host1 has an Azure virtual machine named VM1 that was deployed by using a custom Azure Resource\nManager template.\nYou need to move VM1 to Host2.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. From the Update management blade, click Enable.",
            "B. From the Overview blade, move VM1 to a different subscription.",
            "C. From the Redeploy blade, click Redeploy. Most Voted",
            "D. From the Profile blade, modify the usage location."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "When you redeploy a VM, it moves the VM to a new node within the Azure infrastructure and then powers it back on, retaining all your configuration options and associated resources.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/redeploy-to-new-node",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 1",
        "questionIs": "DRAG DROP -\nYou have downloaded an Azure Resource Manager template to deploy numerous virtual machines. The template is based on a current virtual machine, but must be adapted to reference an administrative password.\nYou need to make sure that the password is not stored in plain text.\nYou are preparing to create the necessary components to achieve your goal.\nWhich of the following should you create to achieve your goal? Answer by dragging the correct option from the list to the answer area.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0000300001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 1",
        "questionIs": "Your company has an Azure Kubernetes Service (AKS) cluster that you manage from an Azure AD-joined device. The cluster is located in a resource group.\nDevelopers have created an application named MyApp. MyApp was packaged into a container image.\nYou need to deploy the YAML manifest file for the application.\nSolution: You install the Azure CLI on the device and run the kubectl apply `\"f myapp.yaml command.\nDoes this meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "kubectl apply -f myapp.yaml applies a configuration change to a resource from a file or stdin.\nReference:\nhttps://kubernetes.io/docs/reference/kubectl/overview/\nhttps://docs.microsoft.com/en-us/cli/azure/aks",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 1",
        "questionIs": "Your company has an Azure Kubernetes Service (AKS) cluster that you manage from an Azure AD-joined device. The cluster is located in a resource group.\nDevelopers have created an application named MyApp. MyApp was packaged into a container image.\nYou need to deploy the YAML manifest file for the application.\nSolution: You install the docker client on the device and run the docker run -it microsoft/azure-cli:0.10.17 command.\nDoes this meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 1",
        "questionIs": "Your company has a web app named WebApp1.\nYou use the WebJobs SDK to design a triggered App Service background task that automatically invokes a function in the code every time new data is received in a queue.\nYou are preparing to configure the service processes a queue data item.\nWhich of the following is the service you should use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Logic Apps",
            "B. WebJobs Most Voted",
            "C. Flow",
            "D. Functions"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-compare-logic-apps-ms-flow-webjobs",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 1",
        "questionIs": "Your company has an Azure subscription.\nYou need to deploy a number of Azure virtual machines to the subscription by using Azure Resource Manager (ARM) templates. The virtual machines will be included in a single availability set.\nYou need to ensure that the ARM template allows for as many virtual machines as possible to remain accessible in the event of fabric failure or maintenance.\nWhich of the following is the value that you should configure for the platformFaultDomainCount property?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. 10",
            "B. 30",
            "C. Min Value",
            "D. Max Value Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "The number of fault domains for managed availability sets varies by region - either two or three per region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/manage-availability",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #7 -- Topic 1",
        "questionIs": "Your company has an Azure subscription.\nYou need to deploy a number of Azure virtual machines to the subscription by using Azure Resource Manager (ARM) templates. The virtual machines will be included in a single availability set.\nYou need to ensure that the ARM template allows for as many virtual machines as possible to remain accessible in the event of fabric failure or maintenance.\nWhich of the following is the value that you should configure for the platformUpdateDomainCount property?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. 10",
            "B. 20 Most Voted",
            "C. 30",
            "D. 40"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Each virtual machine in your availability set is assigned an update domain and a fault domain by the underlying Azure platform. For a given availability set, five non-user-configurable update domains are assigned by default (Resource Manager deployments can then be increased to provide up to 20 update domains) to indicate groups of virtual machines and underlying physical hardware that can be rebooted at the same time.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/manage-availability",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #8 -- Topic 1",
        "questionIs": "DRAG DROP -\nYou are creating an Azure Cosmos DB account that makes use of the SQL API. Data will be added to the account every day by a web application.\nYou need to ensure that an email notification is sent when information is received from IoT devices, and that compute cost is reduced.\nYou decide to deploy a function app.\nWhich of the following should you configure the function app to use? Answer by dragging the correct options from the list to the answer area.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0000900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #9 -- Topic 1",
        "questionIs": "This question requires that you evaluate the underlined text to determine if it is correct.\nYou company has an on-premises deployment of MongoDB, and an Azure Cosmos DB account that makes use of the MongoDB API.\nYou need to devise a strategy to migrate MongoDB to the Azure Cosmos DB account.\nYou include the Data Management Gateway tool in your migration strategy.\nInstructions: Review the underlined text. If it makes the statement correct, select `No change required.` If the statement is incorrect, select the answer choice that makes the statement correct.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. No change required",
            "B. mongorestore Most Voted",
            "C. Azure Storage Explorer",
            "D. AzCopy"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/mongodb-migrate https://docs.mongodb.com/manual/reference/program/mongorestore/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #10 -- Topic 1",
        "questionIs": "You are developing an e-Commerce Web App.\nYou want to use Azure Key Vault to ensure that sign-ins to the e-Commerce Web App are secured by using Azure App Service authentication and Azure Active\nDirectory (AAD).\nWhat should you do on the e-Commerce Web App?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Run the az keyvault secret command.",
            "B. Enable Azure AD Connect.",
            "C. Enable Managed Service Identity (MSI). Most Voted",
            "D. Create an Azure AD service principal."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "A managed identity from Azure Active Directory allows your app to easily access other AAD-protected resources such as Azure Key Vault.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/overview-managed-identity https://docs.microsoft.com/en-us/samples/azure-samples/app-service-msi-keyvault-dotnet/keyvault-msi-appservice-sample/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #11 -- Topic 1",
        "questionIs": "This question requires that you evaluate the underlined text to determine if it is correct.\nYour Azure Active Directory Azure (Azure AD) tenant has an Azure subscription linked to it.\nYour developer has created a mobile application that obtains Azure AD access tokens using the OAuth 2 implicit grant type.\nThe mobile application must be registered in Azure AD.\nYou require a redirect URI from the developer for registration purposes.\nInstructions: Review the underlined text. If it makes the statement correct, select `No change is needed.` If the statement is incorrect, select the answer choice that makes the statement correct.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. No change required. Most Voted",
            "B. a secret",
            "C. a login hint",
            "D. a client ID"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "For Native Applications you need to provide a Redirect URI, which Azure AD will use to return token responses.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/v1-protocols-oauth-code",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #12 -- Topic 1",
        "questionIs": "You are creating an Azure key vault using PowerShell. Objects deleted from the key vault must be kept for a set period of 90 days.\nWhich two of the following parameters must be used in conjunction to meet the requirement? (Choose two.)",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. EnabledForDeployment",
            "B. EnablePurgeProtection Most Voted",
            "C. EnabledForTemplateDeployment",
            "D. EnableSoftDelete Most Voted"
        ],
        "answersAre": [
            "B",
            "D"
        ],
        "mostVotedAre": [
            "B",
            "D"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/powershell/module/azurerm.keyvault/new-azurermkeyvault https://docs.microsoft.com/en-us/azure/key-vault/key-vault-ovw-soft-delete",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #13 -- Topic 1",
        "questionIs": "HOTSPOT -\nYou have an Azure Active Directory (Azure AD) tenant.\nYou want to implement multi-factor authentication by making use of a conditional access policy. The conditional access policy must be applied to all users when they access the Azure portal.\nWhich three settings should you configure? To answer, select the appropriate settings in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0001500001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1:\nThe conditional access policy must be applied or assigned to Users and Groups.\nBox 2:\nThe conditional access policy must be applied when users access the Azure portal, which is a cloud app. That is: Microsoft Azure Management\nBox 3:\nAccess control must require multi-factor authentication when granting access.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/conditional-access/app-based-mfa",
        "mostVotedAre": [],
        "descriptionIs": "Box 1:\nThe conditional access policy must be applied or assigned to Users and Groups.\nBox 2:\nThe conditional access policy must be applied when users access the Azure portal, which is a cloud app. That is: Microsoft Azure Management\nBox 3:\nAccess control must require multi-factor authentication when granting access.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/conditional-access/app-based-mfa",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #14 -- Topic 1",
        "questionIs": "You manage an Azure SQL database that allows for Azure AD authentication.\nYou need to make sure that database developers can connect to the SQL database via Microsoft SQL Server Management Studio (SSMS). You also need to make sure the developers use their on-premises Active Directory account for authentication. Your strategy should allow for authentication prompts to be kept to a minimum.\nWhich of the following should you implement?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure AD token.",
            "B. Azure Multi-Factor authentication.",
            "C. Active Directory integrated authentication. Most Voted",
            "D. OATH software tokens."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Azure AD can be the initial Azure AD managed domain. Azure AD can also be an on-premises Active Directory Domain Services that is federated with the Azure\nAD.\nUsing an Azure AD identity to connect using SSMS or SSDT\nThe following procedures show you how to connect to a SQL database with an Azure AD identity using SQL Server Management Studio or SQL Server Database\nTools.\nActive Directory integrated authentication\nUse this method if you are logged in to Windows using your Azure Active Directory credentials from a federated domain.\n1. Start Management Studio or Data Tools and in the Connect to Server (or Connect to Database Engine) dialog box, in the Authentication box, select Active\nDirectory - Integrated. No password is needed or can be entered because your existing credentials will be presented for the connection.\n\n2. Select the Options button, and on the Connection Properties page, in the Connect to database box, type the name of the user database you want to connect to.\n(The AD domain name or tenant ID\u05d2\u20ac option is only supported for Universal with MFA connection options, otherwise it is greyed out.)",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0001900001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #15 -- Topic 1",
        "questionIs": "You are developing an application to transfer data between on-premises file servers and Azure Blob storage. The application stores keys, secrets, and certificates in Azure Key Vault and makes use of the Azure Key Vault APIs.\nYou want to configure the application to allow recovery of an accidental deletion of the key vault or key vault objects for 90 days after deletion.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Run the Add-AzKeyVaultKey cmdlet.",
            "B. Run the az keyvault update --enable-soft-delete true --enable-purge-protection true CLI. Most Voted",
            "C. Implement virtual network service endpoints for Azure Key Vault.",
            "D. Run the az keyvault update --enable-soft-delete false CLI."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "When soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion.\nPurge protection is an optional Key Vault behavior and is not enabled by default. Purge protection can only be enabled once soft-delete is enabled.\nWhen purge protection is on, a vault or an object in the deleted state cannot be purged until the retention period has passed. Soft-deleted vaults and objects can still be recovered, ensuring that the retention policy will be followed.\nThe default retention period is 90 days, but it is possible to set the retention policy interval to a value from 7 to 90 days through the Azure portal. Once the retention policy interval is set and saved it cannot be changed for that vault.\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/overview-soft-delete",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #16 -- Topic 1",
        "questionIs": "HOTSPOT -\nYou have developed a Web App for your company. The Web App provides services and must run in multiple regions.\nYou want to be notified whenever the Web App uses more than 85 percent of the available CPU cores over a 5 minute period. Your solution must minimize costs.\nWhich command should you use? To answer, select the appropriate settings in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0002100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/sv-se/cli/azure/monitor/metrics/alert",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/sv-se/cli/azure/monitor/metrics/alert",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #17 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment.\nYou need to ensure that the application is highly available and that the users' streaming experience is constant. You also want to configure the application to store data in a geographic location that is nearest to the user.\nSolution: You include the use of Azure Redis Cache in your design.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #18 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment.\nYou need to ensure that the application is highly available and that the users' streaming experience is constant. You also want to configure the application to store data in a geographic location that is nearest to the user.\nSolution: You include the use of an Azure Content Delivery Network (CDN) in your design.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-in/azure/cdn/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #19 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are configuring a web app that delivers streaming video to users. The application makes use of continuous integration and deployment.\nYou need to ensure that the application is highly available and that the users' streaming experience is constant. You also want to configure the application to store data in a geographic location that is nearest to the user.\nSolution: You include the use of a Storage Area Network (SAN) in your design.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #20 -- Topic 1",
        "questionIs": "You develop a Web App on a tier D1 app service plan.\nYou notice that page load times increase during periods of peak traffic.\nYou want to implement automatic scaling when CPU load is above 80 percent. Your solution must minimize costs.\nWhat should you do first?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Enable autoscaling on the Web App.",
            "B. Switch to the Premium App Service tier plan.",
            "C. Switch to the Standard App Service tier plan. Most Voted",
            "D. Switch to the Azure App Services consumption plan."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Configure the web app to the Standard App Service Tier. The Standard tier supports auto-scaling, and we should minimize the cost. We can then enable autoscaling on the web app, add a scale rule and add a Scale condition.\nReference:\nhttps://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-autoscale-get-started https://azure.microsoft.com/en-us/pricing/details/app-service/plans/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #21 -- Topic 1",
        "questionIs": "Your company's Azure subscription includes an Azure Log Analytics workspace.\nYour company has a hundred on-premises servers that run either Windows Server 2012 R2 or Windows Server 2016, and is linked to the Azure Log Analytics workspace. The Azure Log Analytics workspace is set up to gather performance counters associated with security from these linked servers.\nYou must configure alerts based on the information gathered by the Azure Log Analytics workspace.\nYou have to make sure that alert rules allow for dimensions, and that alert creation time should be kept to a minimum. Furthermore, a single alert notification must be created when the alert is created and when the alert is resolved.\nYou need to make use of the necessary signal type when creating the alert rules.\nWhich of the following is the option you should use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. The Activity log signal type.",
            "B. The Application Log signal type.",
            "C. The Metric signal type. Most Voted",
            "D. The Audit Log signal type."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Metric alerts in Azure Monitor provide a way to get notified when one of your metrics cross a threshold. Metric alerts work on a range of multi-dimensional platform metrics, custom metrics, Application Insights standard and custom metrics.\nNote: Signals are emitted by the target resource and can be of several types. Metric, Activity log, Application Insights, and Log.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/platform/alerts-metric",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #22 -- Topic 1",
        "questionIs": "You are developing a .NET Core MVC application that allows customers to research independent holiday accommodation providers.\nYou want to implement Azure Search to allow the application to search the index by using various criteria to locate documents related to accommodation.\nYou want the application to allow customers to search the index by using regular expressions.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure the SearchMode property of the SearchParameters class.",
            "B. Configure the QueryType property of the SearchParameters class. Most Voted",
            "C. Configure the Facets property of the SearchParameters class.",
            "D. Configure the Filter property of the SearchParameters class."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "The SearchParameters.QueryType Property gets or sets a value that specifies the syntax of the search query. The default is 'simple'. Use 'full' if your query uses the Lucene query syntax.\nYou can write queries against Azure Search based on the rich Lucene Query Parser syntax for specialized query forms: wildcard, fuzzy search, proximity search, regular expressions are a few examples.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.search.models.searchparameters https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.search.models.searchparameters.querytype",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #23 -- Topic 1",
        "questionIs": "You are a developer at your company.\nYou need to update the definitions for an existing Logic App.\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. the Enterprise Integration Pack (EIP)",
            "B. the Logic App Code View Most Voted",
            "C. the API Connections",
            "D. the Logic Apps Designer"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Edit JSON - Azure portal -\n1. Sign in to the Azure portal.\n2. From the left menu, choose All services. In the search box, find \"logic apps\", and then from the results, select your logic app.\n3. On your logic app's menu, under Development Tools, select Logic App Code View.\n4. The Code View editor opens and shows your logic app definition in JSON format.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-overview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-definitions",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #24 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service instance.\nSolution: You configure Basic gateway credentials for the Azure resource.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "API Management allows to secure access to the back-end service of an API using client certificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #25 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service instance.\nSolution: You configure Client cert gateway credentials for the HTTP(s) endpoint.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "The API back end is hosted in an Azure App Service instance. It is an Azure resource and not an HTTP(s) endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #26 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service instance.\nSolution: You configure Basic gateway credentials for the HTTP(s) endpoint.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "API Management allows to secure access to the back-end service of an API using client certificates. Furthermore, the API back end is hosted in an Azure App\nService instance. It is an Azure resource and not an HTTP(s) endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #27 -- Topic 1",
        "questionIs": "Note: The question is included in a number of questions that depicts the identical set-up. However, every question has a distinctive result. Establish if the solution satisfies the requirements.\nYou are developing a solution for a public facing API.\nThe API back end is hosted in an Azure App Service instance. You have implemented a RESTful service for the API back end.\nYou must configure back-end authentication for the API Management service instance.\nSolution: You configure Client cert gateway credentials for the Azure resource.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "API Management allows to secure access to the back-end service of an API using client certificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #28 -- Topic 1",
        "questionIs": "You are developing a .NET Core MVC application that allows customers to research independent holiday accommodation providers.\nYou want to implement Azure Search to allow the application to search the index by using various criteria to locate documents related to accommodation venues.\nYou want the application to list holiday accommodation venues that fall within a specific price range and are within a specified distance to an airport.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure the SearchMode property of the SearchParameters class.",
            "B. Configure the QueryType property of the SearchParameters class.",
            "C. Configure the Facets property of the SearchParameters class.",
            "D. Configure the Filter property of the SearchParameters class. Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "The Filter property gets or sets the OData $filter expression to apply to the search query.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.search.models.searchparameters https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.search.models.searchparameters.querytype",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #29 -- Topic 1",
        "questionIs": "You are a developer at your company.\nYou need to edit the workflows for an existing Logic App.\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. the Enterprise Integration Pack (EIP)",
            "B. the Logic App Code View Most Voted",
            "C. the API Connections",
            "D. the Logic Apps Designer Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B",
            "D"
        ],
        "descriptionIs": "For business-to-business (B2B) solutions and seamless communication between organizations, you can build automated scalable enterprise integration workflows by using the Enterprise Integration Pack (EIP) with Azure Logic Apps.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-overview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-definitions",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #30 -- Topic 1",
        "questionIs": "DRAG DROP -\nYou are a developer for a company that provides a bookings management service in the tourism industry. You are implementing Azure Search for the tour agencies listed in your company's solution.\nYou create the index in Azure Search. You now need to use the Azure Search .NET SDK to import the relevant data into the Azure Search service.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions from left to right and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0003200001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "1. The index needs to be populated. To do this, we will need a SearchIndexClient. There are two ways to obtain one: by constructing it, or by calling\nIndexes.GetClient on the SearchServiceClient. Here we will use the first method.\n2. Create the indexBatch with the documents\nSomething like:\nvar hotels = new Hotel[];\n{\nnew Hotel()\n{\nHotelId = \"3\",\nBaseRate = 129.99,\nDescription = \"Close to town hall and the river\"\n}\n};\n\u05d2\u20ac\u00a6\nvar batch = IndexBatch.Upload(hotels);\n3. The next step is to populate the newly-created index\nExample:\nvar batch = IndexBatch.Upload(hotels);\ntry\n{\nindexClient.Documents.Index(batch);\n}\nReference:\nhttps://docs.microsoft.com/en-us/azure/search/search-howto-dotnet-sdk",
        "mostVotedAre": [],
        "descriptionIs": "1. The index needs to be populated. To do this, we will need a SearchIndexClient. There are two ways to obtain one: by constructing it, or by calling\nIndexes.GetClient on the SearchServiceClient. Here we will use the first method.\n2. Create the indexBatch with the documents\nSomething like:\nvar hotels = new Hotel[];\n{\nnew Hotel()\n{\nHotelId = \"3\",\nBaseRate = 129.99,\nDescription = \"Close to town hall and the river\"\n}\n};\n\u05d2\u20ac\u00a6\nvar batch = IndexBatch.Upload(hotels);\n3. The next step is to populate the newly-created index\nExample:\nvar batch = IndexBatch.Upload(hotels);\ntry\n{\nindexClient.Documents.Index(batch);\n}\nReference:\nhttps://docs.microsoft.com/en-us/azure/search/search-howto-dotnet-sdk",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #31 -- Topic 1",
        "questionIs": "You are developing an application that applies a set of governance policies for internal and external services, as well as for applications.\nYou develop a stateful ASP.NET Core 2.1 web application named PolicyApp and deploy it to an Azure App Service Web App. The PolicyApp reacts to events from\nAzure Event Grid and performs policy actions based on those events.\nYou have the following requirements:\n\u2711 Authentication events must be used to monitor users when they sign in and sign out.\n\u2711 All authentication events must be processed by PolicyApp.\n\u2711 Sign outs must be processed as fast as possible.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a new Azure Event Grid subscription for all authentication events. Use the subscription to process sign-out events.",
            "B. Create a separate Azure Event Grid handler for sign-in and sign-out events.",
            "C. Create separate Azure Event Grid topics and subscriptions for sign-in and sign-out events. Most Voted",
            "D. Add a subject prefix to sign-out events. Create an Azure Event Grid subscription. Configure the subscription to use the subjectBeginsWith filter. Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "C",
            "D"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/subscription-creation-schema",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #32 -- Topic 1",
        "questionIs": "HOTSPOT -\nYou are developing a C++ application that compiles to a native application named process.exe. The application accepts images as input and returns images in one of the following image formats: GIF, PNG, or JPEG.\nYou must deploy the application as an Azure Function.\nYou need to configure the function and host json files.\nHow should you complete the json files? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0003600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: \"type\": \"http\"\nBox 2: \"customHandler\": { \"description\":{\nA custom handler is defined by configuring the host.json file with details on how to run the web server via the customHandler section.\nThe customHandler section points to a target as defined by the defaultExecutablePath.\nExample:\n\"customHandler\": {\n\"description\": {\n\"defaultExecutablePath\": \"handler.exe\"\nBox 3: \"enableForwardingHttpRequest\": false\nIncorrect:\nFor HTTP-triggered functions with no additional bindings or outputs, you may want your handler to work directly with the HTTP request and response instead of the custom handler request and response payloads. This behavior can be configured in host.json using the enableForwardingHttpRequest setting.\nAt the root of the app, the host.json file is configured to run handler.exe and enableForwardingHttpRequest is set to true.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: \"type\": \"http\"\nBox 2: \"customHandler\": { \"description\":{\nA custom handler is defined by configuring the host.json file with details on how to run the web server via the customHandler section.\nThe customHandler section points to a target as defined by the defaultExecutablePath.\nExample:\n\"customHandler\": {\n\"description\": {\n\"defaultExecutablePath\": \"handler.exe\"\nBox 3: \"enableForwardingHttpRequest\": false\nIncorrect:\nFor HTTP-triggered functions with no additional bindings or outputs, you may want your handler to work directly with the HTTP request and response instead of the custom handler request and response payloads. This behavior can be configured in host.json using the enableForwardingHttpRequest setting.\nAt the root of the app, the host.json file is configured to run handler.exe and enableForwardingHttpRequest is set to true.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-custom-handlers",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #33 -- Topic 1",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an Azure Static Web app that contains training materials for a tool company. Each tool\u2019s training material is contained in a static web page that is linked from the tool\u2019s publicly available description page.\n\nA user must be authenticated using Azure AD prior to viewing training.\n\nYou need to ensure that the user can view training material pages after authentication.\n\nHow should you complete the configuration file? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image377.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #34 -- Topic 1",
        "questionIs": "HOTSPOT\n-\n\nYou are authoring a set of nested Azure Resource Manager templates to deploy Azure resources. You author an Azure Resource Manager template named mainTemplate.json that contains the following linked templates: linkedTemplate1.json, linkedTemplate2.json.\n\nYou add parameters to a parameters template file named mainTemplate.parameters,json. You save all templates on a local device in the C:\\templates\\ folder.\n\nYou have the following requirements:\n\n\u2022 Store the templates in Azure for later deployment.\n\u2022 Enable versioning of the templates.\n\u2022 Manage access to the templates by using Azure RBAC.\n\u2022 Ensure that users have read-only access to the templates.\n\u2022 Allow users to deploy the templates.\n\nYou need to store the templates in Azure.\n\nHow should you complete the command? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image379.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #35 -- Topic 1",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a service where customers can report news events from a browser using Azure Web PubSub. The service is implemented as an Azure Function App that uses the JSON WebSocket subprotocol to receive news events.\n\nYou need to implement the bindings for the Azure Function App.\n\nHow should you configure the binding? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image381.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #36 -- Topic 1",
        "questionIs": "HOTSPOT\n-\n\nYou are building a software-as-a-service (SaaS) application that analyzes DNA data that will run on Azure virtual machines (VMs) in an availability zone. The data is stored on managed disks attached to the VM. The performance of the analysis is determined by the speed of the disk attached to the VM.\n\nYou have the following requirements:\n\n\u2022 The application must be able to quickly revert to the previous day\u2019s data if a systemic error is detected.\n\u2022 The application must minimize downtime in the case of an Azure datacenter outage.\n\nYou need to provision the managed disk for the VM to maximize performance while meeting the requirements.\n\nWhich type of Azure Managed Disk should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image383.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #37 -- Topic 1",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an application that includes two Docker containers.\n\nThe application must meet the following requirements:\n\n\u2022 The containers must not run as root.\n\u2022 The containers must be deployed to Azure Container Instances by using a YAML file.\n\u2022 The containers must share a lifecycle, resources, local network, and storage volume.\n\u2022 The storage volume must persist through container crashes.\n\u2022 The storage volume must be deployed on stop or restart of the containers.\n\nYou need to configure Azure Container Instances for the application.\n\nWhich configuration values should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image385.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are implementing a software as a service (SaaS) ASP.NET Core web service that will run as an Azure Web App. The web service will use an on-premises\nSQL Server database for storage. The web service also includes a WebJob that processes data updates. Four customers will use the web service.\n\u2711 Each instance of the WebJob processes data for a single customer and must run as a singleton instance.\n\u2711 Each deployment must be tested by using deployment slots prior to serving production data.\n\u2711 Azure costs must be minimized.\n\u2711 Azure resources must be located in an isolated network.\nYou need to configure the App Service plan for the Web App.\nHow should you configure the App Service plan? To answer, select the appropriate settings in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0008800001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Number of VM instances: 4 -\nYou are not charged extra for deployment slots.\n\nPricing tier: Isolated -\nThe App Service Environment (ASE) is a powerful feature offering of the Azure App Service that gives network isolation and improved scale capabilities. It is essentially a deployment of the Azure App Service into a subnet of a customer's Azure Virtual Network (VNet).\nReference:\nhttps://azure.microsoft.com/sv-se/blog/announcing-app-service-isolated-more-power-scale-and-ease-of-use/",
        "mostVotedAre": [],
        "descriptionIs": "Number of VM instances: 4 -\nYou are not charged extra for deployment slots.\n\nPricing tier: Isolated -\nThe App Service Environment (ASE) is a powerful feature offering of the Azure App Service that gives network isolation and improved scale capabilities. It is essentially a deployment of the Azure App Service into a subnet of a customer's Azure Virtual Network (VNet).\nReference:\nhttps://azure.microsoft.com/sv-se/blog/announcing-app-service-isolated-more-power-scale-and-ease-of-use/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are a developer for a software as a service (SaaS) company that uses an Azure Function to process orders. The Azure Function currently runs on an Azure\nFunction app that is triggered by an Azure Storage queue.\nYou are preparing to migrate the Azure Function to Kubernetes using Kubernetes-based Event Driven Autoscaling (KEDA).\nYou need to configure Kubernetes Custom Resource Definitions (CRD) for the Azure Function.\nWhich CRDs should you configure? To answer, drag the appropriate CRD types to the correct locations. Each CRD type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0009000001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Deployment -\nTo deploy Azure Functions to Kubernetes use the func kubernetes deploy command has several attributes that directly control how our app scales, once it is deployed to Kubernetes.\n\nBox 2: ScaledObject -\nWith --polling-interval, we can control the interval used by KEDA to check Azure Service Bus Queue for messages.\nExample of ScaledObject with polling interval\napiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\nname: transformer-fn\nnamespace: tt\nlabels:\ndeploymentName: transformer-fn\nspec:\nscaleTargetRef:\ndeploymentName: transformer-fn\npollingInterval: 5\nminReplicaCount: 0\nmaxReplicaCount: 100\n\nBox 3: Secret -\nStore connection strings in Kubernetes Secrets.\nExample: to create the Secret in our demo Namespace:\n# create the k8s demo namespace\nkubectl create namespace tt\n# grab connection string from Azure Service Bus\nKEDA_SCALER_CONNECTION_STRING=$(az servicebus queue authorization-rule keys list \\\n-g $RG_NAME \\\n--namespace-name $SBN_NAME \\\n--queue-name inbound \\\n-n keda-scaler \\\n--query \"primaryConnectionString\" \\\n-o tsv)\n# create the kubernetes secret\nkubectl create secret generic tt-keda-auth \\\n--from-literal KedaScaler=$KEDA_SCALER_CONNECTION_STRING \\\n--namespace tt\nReference:\nhttps://www.thinktecture.com/en/kubernetes/serverless-workloads-with-keda/",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Deployment -\nTo deploy Azure Functions to Kubernetes use the func kubernetes deploy command has several attributes that directly control how our app scales, once it is deployed to Kubernetes.\n\nBox 2: ScaledObject -\nWith --polling-interval, we can control the interval used by KEDA to check Azure Service Bus Queue for messages.\nExample of ScaledObject with polling interval\napiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\nname: transformer-fn\nnamespace: tt\nlabels:\ndeploymentName: transformer-fn\nspec:\nscaleTargetRef:\ndeploymentName: transformer-fn\npollingInterval: 5\nminReplicaCount: 0\nmaxReplicaCount: 100\n\nBox 3: Secret -\nStore connection strings in Kubernetes Secrets.\nExample: to create the Secret in our demo Namespace:\n# create the k8s demo namespace\nkubectl create namespace tt\n# grab connection string from Azure Service Bus\nKEDA_SCALER_CONNECTION_STRING=$(az servicebus queue authorization-rule keys list \\\n-g $RG_NAME \\\n--namespace-name $SBN_NAME \\\n--queue-name inbound \\\n-n keda-scaler \\\n--query \"primaryConnectionString\" \\\n-o tsv)\n# create the kubernetes secret\nkubectl create secret generic tt-keda-auth \\\n--from-literal KedaScaler=$KEDA_SCALER_CONNECTION_STRING \\\n--namespace tt\nReference:\nhttps://www.thinktecture.com/en/kubernetes/serverless-workloads-with-keda/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are creating a CLI script that creates an Azure web app and related services in Azure App Service. The web app uses the following variables:\n\nYou need to automatically deploy code from GitHub to the newly created web app.\nHow should you complete the script? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0009200001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0009300001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: az appservice plan create\nThe azure group creates command successfully returns JSON result. Now we can use resource group to create a azure app service plan\n\nBox 2: az webapp create -\nCreate a new web app..\n\nBox 3: --plan $webappname -\n..with the serviceplan we created in step 1.\n\nBox 4: az webapp deployment -\nContinuous Delivery with GitHub. Example:\naz webapp deployment source config --name firstsamplewebsite1 --resource-group websites--repo-url $gitrepo --branch master --git-token $token\nBox 5: --repo-url $gitrepo --branch master --manual-integration\nReference:\nhttps://medium.com/@satish1v/devops-your-way-to-azure-web-apps-with-azure-cli-206ed4b3e9b1",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: az appservice plan create\nThe azure group creates command successfully returns JSON result. Now we can use resource group to create a azure app service plan\n\nBox 2: az webapp create -\nCreate a new web app..\n\nBox 3: --plan $webappname -\n..with the serviceplan we created in step 1.\n\nBox 4: az webapp deployment -\nContinuous Delivery with GitHub. Example:\naz webapp deployment source config --name firstsamplewebsite1 --resource-group websites--repo-url $gitrepo --branch master --git-token $token\nBox 5: --repo-url $gitrepo --branch master --manual-integration\nReference:\nhttps://medium.com/@satish1v/devops-your-way-to-azure-web-apps-with-azure-cli-206ed4b3e9b1",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Trigger the photo processing from Blob storage events.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "You need to catch the triggered event, so move the photo processing to an Azure Function triggered from the blob upload.\nNote: Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file-oriented workflow.\nEvents are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener.\nHowever, the processing must start in less than one minute.\nNote: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.\nYou need to ensure that scripts run and resources are available before a swap operation occurs.\nSolution: Update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. No",
            "B. Yes Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Specify custom warm-up.\nSome apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.\nYou need to ensure that scripts run and resources are available before a swap operation occurs.\nSolution: Enable auto swap for the Testing slot. Deploy the app to the Testing slot.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. No Most Voted",
            "B. Yes"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Instead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.\nNote: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #7 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.\nYou need to ensure that scripts run and resources are available before a swap operation occurs.\nSolution: Disable auto swap. Update the app with a method named statuscheck to run the scripts. Re-enable auto swap and deploy the app to the Production slot.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. No Most Voted",
            "B. Yes"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Instead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.\nNote: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #8 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Convert the Azure Storage account to a BlockBlobStorage storage account.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Not necessary to convert the account, instead move photo processing to an Azure Function triggered from the blob upload..\nAzure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file- oriented workflow.\nNote: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #9 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are developing an Azure Web App. You configure TLS mutual authentication for the web app.\nYou need to validate the client certificate in the web app. To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0010000001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Accessing the client certificate from App Service.\nIf you are using ASP.NET and configure your app to use client certificate authentication, the certificate will be available through the HttpRequest.ClientCertificate property. For other application stacks, the client cert will be available in your app through a base64 encoded value in the \"X-ARR-ClientCert\" request header. Your application can create a certificate from this value and then use it for authentication and authorization purposes in your application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-mutual-auth",
        "mostVotedAre": [],
        "descriptionIs": "Accessing the client certificate from App Service.\nIf you are using ASP.NET and configure your app to use client certificate authentication, the certificate will be available through the HttpRequest.ClientCertificate property. For other application stacks, the client cert will be available in your app through a base64 encoded value in the \"X-ARR-ClientCert\" request header. Your application can create a certificate from this value and then use it for authentication and authorization purposes in your application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-web-configure-tls-mutual-auth",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #10 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are developing a Docker/Go using Azure App Service Web App for Containers. You plan to run the container in an App Service on Linux. You identify a\nDocker container image to use.\nNone of your current resource groups reside in a location that supports Linux. You must minimize the number of resource groups required.\nYou need to create the application and perform an initial deployment.\nWhich three Azure CLI commands should you use to develop the solution? To answer, move the appropriate commands from the list of commands to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0010200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "You can host native Linux applications in the cloud by using Azure Web Apps. To create a Web App for Containers, you must run Azure CLI commands that create a group, then a service plan, and finally the web app itself.\n\nStep 1: az group create -\nIn the Cloud Shell, create a resource group with the az group create command.\nStep 2: az appservice plan create\nIn the Cloud Shell, create an App Service plan in the resource group with the az appservice plan create command.\n\nStep 3: az webapp create -\nIn the Cloud Shell, create a web app in the myAppServicePlan App Service plan with the az webapp create command. Don't forget to replace with a unique app name, and <docker-ID> with your Docker ID.\nReference:\nhttps://docs.microsoft.com/mt-mt/azure/app-service/containers/quickstart-docker-go?view=sql-server-ver15",
        "mostVotedAre": [],
        "descriptionIs": "You can host native Linux applications in the cloud by using Azure Web Apps. To create a Web App for Containers, you must run Azure CLI commands that create a group, then a service plan, and finally the web app itself.\n\nStep 1: az group create -\nIn the Cloud Shell, create a resource group with the az group create command.\nStep 2: az appservice plan create\nIn the Cloud Shell, create an App Service plan in the resource group with the az appservice plan create command.\n\nStep 3: az webapp create -\nIn the Cloud Shell, create a web app in the myAppServicePlan App Service plan with the az webapp create command. Don't forget to replace with a unique app name, and <docker-ID> with your Docker ID.\nReference:\nhttps://docs.microsoft.com/mt-mt/azure/app-service/containers/quickstart-docker-go?view=sql-server-ver15",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #11 -- Topic 2",
        "questionIs": "DRAG DROP -\nFourth Coffee has an ASP.NET Core web app that runs in Docker. The app is mapped to the www.fourthcoffee.com domain.\nFourth Coffee is migrating this application to Azure.\nYou need to provision an App Service Web App to host this docker image and map the custom domain to the App Service web app.\nA resource group named FourthCoffeePublicWebResourceGroup has been created in the WestUS region that contains an App Service Plan named\nAppServiceLinuxDockerPlan.\nWhich order should the CLI commands be used to develop the solution? To answer, move all of the Azure CLI commands from the list of commands to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0010500001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: #bin/bash -\nThe appName is used when the webapp-name is created in step 2.\n\nStep 2: az webapp create -\nCreate a web app. In the Cloud Shell, create a web app in the myAppServicePlan App Service plan with the az webapp create command.\nStep 3: az webapp config container set\nIn Create a web app, you specified an image on Docker Hub in the az webapp create command. This is good enough for a public image. To use a private image, you need to configure your Docker account ID and password in your Azure web app.\nStep 4: az webapp config hostname add\nThe webapp-name is used when the webapp is created in step 2.\nIn the Cloud Shell, follow the az webapp create command with az webapp config container set.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-docker-image https://docs.microsoft.com/en-us/azure/app-service/tutorial-custom-container?pivots=container-linux https://docs.microsoft.com/en-us/azure/app-service/scripts/cli-configure-custom-domain",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: #bin/bash -\nThe appName is used when the webapp-name is created in step 2.\n\nStep 2: az webapp create -\nCreate a web app. In the Cloud Shell, create a web app in the myAppServicePlan App Service plan with the az webapp create command.\nStep 3: az webapp config container set\nIn Create a web app, you specified an image on Docker Hub in the az webapp create command. This is good enough for a public image. To use a private image, you need to configure your Docker account ID and password in your Azure web app.\nStep 4: az webapp config hostname add\nThe webapp-name is used when the webapp is created in step 2.\nIn the Cloud Shell, follow the az webapp create command with az webapp config container set.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/containers/tutorial-custom-docker-image https://docs.microsoft.com/en-us/azure/app-service/tutorial-custom-container?pivots=container-linux https://docs.microsoft.com/en-us/azure/app-service/scripts/cli-configure-custom-domain",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #12 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are developing a serverless Java application on Azure. You create a new Azure Key Vault to work with secrets from a new Azure Functions application.\nThe application must meet the following requirements:\n\u2711 Reference the Azure Key Vault without requiring any changes to the Java code.\n\u2711 Dynamically add and remove instances of the Azure Functions host based on the number of incoming application events.\n\u2711 Ensure that instances are perpetually warm to avoid any cold starts.\n\u2711 Connect to a VNet.\n\u2711 Authentication to the Azure Key Vault instance must be removed if the Azure Function application is deleted.\nYou need to grant the Azure Functions application access to the Azure Key Vault.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0010800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Create the Azure Functions app with a Consumption plan type.\nUse the Consumption plan for serverless.\nStep 2: Create a system-assigned managed identity for the application.\nCreate a system-assigned managed identity for your application.\nKey Vault references currently only support system-assigned managed identities. User-assigned identities cannot be used.\nStep 3: Create an access policy in Key Vault for the application identity.\nCreate an access policy in Key Vault for the application identity you created earlier. Enable the \"Get\" secret permission on this policy. Do not configure the\n\"authorized application\" or applicationId settings, as this is not compatible with a managed identity.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Create the Azure Functions app with a Consumption plan type.\nUse the Consumption plan for serverless.\nStep 2: Create a system-assigned managed identity for the application.\nCreate a system-assigned managed identity for your application.\nKey Vault references currently only support system-assigned managed identities. User-assigned identities cannot be used.\nStep 3: Create an access policy in Key Vault for the application identity.\nCreate an access policy in Key Vault for the application identity you created earlier. Enable the \"Get\" secret permission on this policy. Do not configure the\n\"authorized application\" or applicationId settings, as this is not compatible with a managed identity.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #13 -- Topic 2",
        "questionIs": "You develop a website. You plan to host the website in Azure. You expect the website to experience high traffic volumes after it is published.\nYou must ensure that the website remains available and responsive while minimizing cost.\nYou need to deploy the website.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Deploy the website to a virtual machine. Configure the virtual machine to automatically scale when the CPU load is high.",
            "B. Deploy the website to an App Service that uses the Shared service tier. Configure the App Service plan to automatically scale when the CPU load is high.",
            "C. Deploy the website to a virtual machine. Configure a Scale Set to increase the virtual machine instance count when the CPU load is high.",
            "D. Deploy the website to an App Service that uses the Standard service tier. Configure the App Service plan to automatically scale when the CPU load is high. Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Windows Azure Web Sites (WAWS) offers 3 modes: Standard, Free, and Shared.\nStandard mode carries an enterprise-grade SLA (Service Level Agreement) of 99.9% monthly, even for sites with just one instance.\nStandard mode runs on dedicated instances, making it different from the other ways to buy Windows Azure Web Sites.\nIncorrect Answers:\nB: Shared and Free modes do not offer the scaling flexibility of Standard, and they have some important limits.\nShared mode, just as the name states, also uses shared Compute resources, and also has a CPU limit. So, while neither Free nor Shared is likely to be the best choice for your production environment due to these limits.",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #14 -- Topic 2",
        "questionIs": "HOTSPOT -\nA company is developing a Java web app. The web app code is hosted in a GitHub repository located at https://github.com/Contoso/webapp.\nThe web app must be evaluated before it is moved to production. You must deploy the initial code release to a deployment slot named staging.\nYou need to create the web app and deploy the code.\nHow should you complete the commands? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0011100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: group -\n# Create a resource group.\naz group create --location westeurope --name myResourceGroup\n\nBox 2: appservice plan -\n# Create an App Service plan in STANDARD tier (minimum required by deployment slots). az appservice plan create --name $webappname --resource-group myResourceGroup --sku S1\n\nBox 3: webapp -\n# Create a web app.\naz webapp create --name $webappname --resource-group myResourceGroup \\\n--plan $webappname\n\nBox 4: webapp deployment slot -\n#Create a deployment slot with the name \"staging\".\naz webapp deployment slot create --name $webappname --resource-group myResourceGroup \\\n--slot staging\n\nBox 5: webapp deployment source -\n# Deploy sample code to \"staging\" slot from GitHub.\naz webapp deployment source config --name $webappname --resource-group myResourceGroup \\\n--slot staging --repo-url $gitrepo --branch master --manual-integration\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/scripts/cli-deploy-staging-environment",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: group -\n# Create a resource group.\naz group create --location westeurope --name myResourceGroup\n\nBox 2: appservice plan -\n# Create an App Service plan in STANDARD tier (minimum required by deployment slots). az appservice plan create --name $webappname --resource-group myResourceGroup --sku S1\n\nBox 3: webapp -\n# Create a web app.\naz webapp create --name $webappname --resource-group myResourceGroup \\\n--plan $webappname\n\nBox 4: webapp deployment slot -\n#Create a deployment slot with the name \"staging\".\naz webapp deployment slot create --name $webappname --resource-group myResourceGroup \\\n--slot staging\n\nBox 5: webapp deployment source -\n# Deploy sample code to \"staging\" slot from GitHub.\naz webapp deployment source config --name $webappname --resource-group myResourceGroup \\\n--slot staging --repo-url $gitrepo --branch master --manual-integration\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/scripts/cli-deploy-staging-environment",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #15 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou have a web service that is used to pay for food deliveries. The web service uses Azure Cosmos DB as the data store.\nYou plan to add a new feature that allows users to set a tip amount. The new feature requires that a property named tip on the document in Cosmos DB must be present and contain a numeric value.\nThere are many existing websites and mobile apps that use the web service that will not be updated to set the tip property for some time.\nHow should you complete the trigger?\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0011400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #16 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the blob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Use the Durable Function async pattern to process the blob data.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.\nNote: Large, long-running functions can cause unexpected timeout issues. General best practices include:\nWhenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #17 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the blob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "Large, long-running functions can cause unexpected timeout issues. General best practices include:\nWhenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #18 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the blob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Configure the app to use an App Service hosting plan and enable the Always On setting.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.\nNote: Large, long-running functions can cause unexpected timeout issues. General best practices include:\nWhenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #19 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Move photo processing to an Azure Function triggered from the blob upload.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "Azure Storage events allow applications to react to events. Common Blob storage event scenarios include image or video processing, search indexing, or any file- oriented workflow.\nEvents are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener.\nNote: Only storage accounts of kind StorageV2 (general purpose v2) and BlobStorage support event integration. Storage (general purpose v1) does not support integration with Event Grid.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #20 -- Topic 2",
        "questionIs": "You are developing an application that uses Azure Blob storage.\nThe application must read the transaction logs of all the changes that occur to the blobs and the blob metadata in the storage account for auditing purposes. The changes must be in the order in which they occurred, include only create, update, delete, and copy operations and be retained for compliance reasons.\nYou need to process the transaction logs asynchronously.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Process all Azure Blob storage events by using Azure Event Grid with a subscriber Azure Function app.",
            "B. Enable the change feed on the storage account and process all changes for available events. Most Voted",
            "C. Process all Azure Storage Analytics logs for successful blob events.",
            "D. Use the Azure Monitor HTTP Data Collector API and scan the request body for successful blob events."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Change feed support in Azure Blob Storage\nThe purpose of the change feed is to provide transaction logs of all the changes that occur to the blobs and the blob metadata in your storage account. The change feed provides ordered, guaranteed, durable, immutable, read-only log of these changes. Client applications can read these logs at any time, either in streaming or in batch mode. The change feed enables you to build efficient and scalable solutions that process change events that occur in your Blob Storage account at a low cost.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #21 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou plan to create a Docker image that runs an ASP.NET Core application named ContosoApp. You have a setup script named setupScript.ps1 and a series of application files including ContosoApp.dll.\nYou need to create a Dockerfile document that meets the following requirements:\n\u2711 Call setupScripts.ps1 when the container is built.\n\u2711 Run ContosoApp.dll when the container starts.\nThe Dockerfile document must be created in the same folder where ContosoApp.dll and setupScript.ps1 are stored.\nWhich five commands should you use to develop the solution? To answer, move the appropriate commands from the list of commands to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0012100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: CMD [..]\nCmd starts a new instance of the command interpreter, Cmd.exe.\nSyntax: CMD <string>\nSpecifies the command you want to carry out.\nBox 2: FROM microsoft/aspnetcore-build:latest\n\nBox 3: WORKDIR /apps/ContosoApp -\nBxo 4: COPY ./ .\nBox 5: RUN powershell ./setupScript.ps1",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: CMD [..]\nCmd starts a new instance of the command interpreter, Cmd.exe.\nSyntax: CMD <string>\nSpecifies the command you want to carry out.\nBox 2: FROM microsoft/aspnetcore-build:latest\n\nBox 3: WORKDIR /apps/ContosoApp -\nBxo 4: COPY ./ .\nBox 5: RUN powershell ./setupScript.ps1",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #22 -- Topic 2",
        "questionIs": "You are developing an Azure Function App that processes images that are uploaded to an Azure Blob container.\nImages must be processed as quickly as possible after they are uploaded, and the solution must minimize latency. You create code to process images when the\nFunction App is triggered.\nYou need to configure the Function App.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Use an App Service plan. Configure the Function App to use an Azure Blob Storage input trigger.",
            "B. Use a Consumption plan. Configure the Function App to use an Azure Blob Storage trigger.",
            "C. Use a Consumption plan. Configure the Function App to use a Timer trigger.",
            "D. Use an App Service plan. Configure the Function App to use an Azure Blob Storage trigger. Most Voted",
            "E. Use a Consumption plan. Configure the Function App to use an Azure Blob Storage input trigger."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "The Blob storage trigger starts a function when a new or updated blob is detected. The blob contents are provided as input to the function.\nThe Consumption plan limits a function app on one virtual machine (VM) to 1.5 GB of memory.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #23 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are configuring a new development environment for a Java application.\nThe environment requires a Virtual Machine Scale Set (VMSS), several storage accounts, and networking components.\nThe VMSS must not be created until the storage accounts have been successfully created and an associated load balancer and virtual network is configured.\nHow should you complete the Azure Resource Manager template? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0012400001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: copyIndex -\nNotice that the name of each resource includes the copyIndex() function, which returns the current iteration in the loop. copyIndex() is zero-based.\n\nBox 2: copy -\nBy adding the copy element to the resources section of your template, you can dynamically set the number of resources to deploy.\n\nBox 3: dependsOn -\nExample:\n\"type\": \"Microsoft.Compute/virtualMachineScaleSets\",\n\"apiVersion\": \"2020-06-01\",\n\"name\": \"[variables('namingInfix')]\",\n\"location\": \"[parameters('location')]\",\n\"sku\": {\n\"name\": \"[parameters('vmSku')]\",\n\"tier\": \"Standard\",\n\"capacity\": \"[parameters('instanceCount')]\"\n},\n\"dependsOn\": [\n\"[resourceId('Microsoft.Network/loadBalancers', variables('loadBalancerName'))]\",\n\"[resourceId('Microsoft.Network/virtualNetworks', variables('virtualNetworkName'))]\"\n],\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/copy-resources https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/quick-create-template-windows",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: copyIndex -\nNotice that the name of each resource includes the copyIndex() function, which returns the current iteration in the loop. copyIndex() is zero-based.\n\nBox 2: copy -\nBy adding the copy element to the resources section of your template, you can dynamically set the number of resources to deploy.\n\nBox 3: dependsOn -\nExample:\n\"type\": \"Microsoft.Compute/virtualMachineScaleSets\",\n\"apiVersion\": \"2020-06-01\",\n\"name\": \"[variables('namingInfix')]\",\n\"location\": \"[parameters('location')]\",\n\"sku\": {\n\"name\": \"[parameters('vmSku')]\",\n\"tier\": \"Standard\",\n\"capacity\": \"[parameters('instanceCount')]\"\n},\n\"dependsOn\": [\n\"[resourceId('Microsoft.Network/loadBalancers', variables('loadBalancerName'))]\",\n\"[resourceId('Microsoft.Network/virtualNetworks', variables('virtualNetworkName'))]\"\n],\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/copy-resources https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/quick-create-template-windows",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #24 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are developing an Azure Function App by using Visual Studio. The app will process orders input by an Azure Web App. The web app places the order information into Azure Queue Storage.\nYou need to review the Azure Function App code shown below.\n\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0012700001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0012700002.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: No -\nExpirationTime - The time that the message expires.\nInsertionTime - The time that the message was added to the queue.\n\nBox 2: Yes -\nmaxDequeueCount - The number of times to try processing a message before moving it to the poison queue. Default value is 5.\n\nBox 3: Yes -\nWhen there are multiple queue messages waiting, the queue trigger retrieves a batch of messages and invokes function instances concurrently to process them.\nBy default, the batch size is 16. When the number being processed gets down to 8, the runtime gets another batch and starts processing those messages. So the maximum number of concurrent messages being processed per function on one virtual machine (VM) is 24.\n\nBox 4: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: No -\nExpirationTime - The time that the message expires.\nInsertionTime - The time that the message was added to the queue.\n\nBox 2: Yes -\nmaxDequeueCount - The number of times to try processing a message before moving it to the poison queue. Default value is 5.\n\nBox 3: Yes -\nWhen there are multiple queue messages waiting, the queue trigger retrieves a batch of messages and invokes function instances concurrently to process them.\nBy default, the batch size is 16. When the number being processed gets down to 8, the runtime gets another batch and starts processing those messages. So the maximum number of concurrent messages being processed per function on one virtual machine (VM) is 24.\n\nBox 4: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-queue",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #25 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are developing a solution for a hospital to support the following use cases:\n\u2711 The most recent patient status details must be retrieved even if multiple users in different locations have updated the patient record.\n\u2711 Patient health monitoring data retrieved must be the current version or the prior version.\n\u2711 After a patient is discharged and all charges have been assessed, the patient billing record contains the final charges.\nYou provision a Cosmos DB NoSQL database and set the default consistency level for the database account to Strong. You set the value for Indexing Mode to\nConsistent.\nYou need to minimize latency and any impact to the availability of the solution. You must override the default consistency level at the query level to meet the required consistency guarantees for the scenarios.\nWhich consistency levels should you implement? To answer, drag the appropriate consistency levels to the correct requirements. Each consistency level may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0012900004.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Strong -\nStrong: Strong consistency offers a linearizability guarantee. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.\n\nBox 2: Bounded staleness -\nBounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most \"K\" versions (that is\n\"updates\") of an item or by \"t\" time interval. When you choose bounded staleness, the \"staleness\" can be configured in two ways:\nThe number of versions (K) of the item\nThe time interval (t) by which the reads might lag behind the writes\n\nBox 3: Eventual -\nEventual: There's no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge.\nIncorrect Answers:\nConsistent prefix: Updates that are returned contain some prefix of all the updates, with no gaps. Consistent prefix guarantees that reads never see out-of-order writes.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Strong -\nStrong: Strong consistency offers a linearizability guarantee. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.\n\nBox 2: Bounded staleness -\nBounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most \"K\" versions (that is\n\"updates\") of an item or by \"t\" time interval. When you choose bounded staleness, the \"staleness\" can be configured in two ways:\nThe number of versions (K) of the item\nThe time interval (t) by which the reads might lag behind the writes\n\nBox 3: Eventual -\nEventual: There's no ordering guarantee for reads. In the absence of any further writes, the replicas eventually converge.\nIncorrect Answers:\nConsistent prefix: Updates that are returned contain some prefix of all the updates, with no gaps. Consistent prefix guarantees that reads never see out-of-order writes.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #26 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are configuring a development environment for your team. You deploy the latest Visual Studio image from the Azure Marketplace to your Azure subscription.\nThe development environment requires several software development kits (SDKs) and third-party components to support application development across the organization. You install and customize the deployed virtual machine (VM) for your development team. The customized VM must be saved to allow provisioning of a new team member development environment.\nYou need to save the customized VM for future provisioning.\nWhich tools or services should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0013100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Powershell -\nCreating an image directly from the VM ensures that the image includes all of the disks associated with the VM, including the OS disk and any data disks.\nBefore you begin, make sure that you have the latest version of the Azure PowerShell module.\nYou use Sysprep to generalize the virtual machine, then use Azure PowerShell to create the image.\n\nBox 2: Azure Blob Storage -\nYou can store images in Azure Blob Storage.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource#create-an-image-of-a-vm-using-powershell",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Powershell -\nCreating an image directly from the VM ensures that the image includes all of the disks associated with the VM, including the OS disk and any data disks.\nBefore you begin, make sure that you have the latest version of the Azure PowerShell module.\nYou use Sysprep to generalize the virtual machine, then use Azure PowerShell to create the image.\n\nBox 2: Azure Blob Storage -\nYou can store images in Azure Blob Storage.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource#create-an-image-of-a-vm-using-powershell",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #27 -- Topic 2",
        "questionIs": "You are preparing to deploy a website to an Azure Web App from a GitHub repository. The website includes static content generated by a script.\nYou plan to use the Azure Web App continuous deployment feature.\nYou need to run the static generation script before the website starts serving traffic.\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Add the path to the static content generation tool to WEBSITE_RUN_FROM_PACKAGE setting in the host.json file.",
            "B. Add a PreBuild target in the websites csproj project file that runs the static content generation script. Most Voted",
            "C. Create a file named run.cmd in the folder /run that calls a script which generates the static content and deploys the website.",
            "D. Create a file named .deployment in the root of the repository that calls a script which generates the static content and deploys the website. Most Voted"
        ],
        "answersAre": [
            "A",
            "D"
        ],
        "mostVotedAre": [
            "B",
            "D"
        ],
        "descriptionIs": "A: In Azure, you can run your functions directly from a deployment package file in your function app. The other option is to deploy your files in the d:\\home\\site\n\\wwwroot directory of your function app (see A above).\nTo enable your function app to run from a package, you just add a WEBSITE_RUN_FROM_PACKAGE setting to your function app settings.\nNote: The host.json metadata file contains global configuration options that affect all functions for a function app.\nD: To customize your deployment, include a .deployment file in the repository root.\nYou just need to add a file to the root of your repository with the name .deployment and the content:\n[config]\ncommand = YOUR COMMAND TO RUN FOR DEPLOYMENT\nthis command can be just running a script (batch file) that has all that is required for your deployment, like copying files from the repository to the web root directory for example.\nReference:\nhttps://github.com/projectkudu/kudu/wiki/Custom-Deployment-Script https://docs.microsoft.com/bs-latn-ba/azure/azure-functions/run-functions-from-deployment-package",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #28 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are developing an application to use Azure Blob storage. You have configured Azure Blob storage to include change feeds.\nA copy of your storage account must be created in another region. Data must be copied from the current storage account to the new storage account directly between the storage servers.\nYou need to create a copy of the storage account in another region and copy the data.\nIn which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0013400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "To move a storage account, create a copy of your storage account in another region. Then, move your data to that account by using AzCopy, or another tool of your choice.\nThe steps are:\n\u2711 Export a template.\n\u2711 Modify the template by adding the target region and storage account name.\n\u2711 Deploy the template to create the new storage account.\n\u2711 Configure the new storage account.\n\u2711 Move data to the new storage account.\n\u2711 Delete the resources in the source region.\nNote: You must enable the change feed on your storage account to begin capturing and recording changes. You can enable and disable changes by using Azure\nResource Manager templates on Portal or Powershell.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-move https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed",
        "mostVotedAre": [],
        "descriptionIs": "To move a storage account, create a copy of your storage account in another region. Then, move your data to that account by using AzCopy, or another tool of your choice.\nThe steps are:\n\u2711 Export a template.\n\u2711 Modify the template by adding the target region and storage account name.\n\u2711 Deploy the template to create the new storage account.\n\u2711 Configure the new storage account.\n\u2711 Move data to the new storage account.\n\u2711 Delete the resources in the source region.\nNote: You must enable the change feed on your storage account to begin capturing and recording changes. You can enable and disable changes by using Azure\nResource Manager templates on Portal or Powershell.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-move https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #29 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are preparing to deploy an Azure virtual machine (VM)-based application.\nThe VMs that run the application have the following requirements:\n\u2711 When a VM is provisioned the firewall must be automatically configured before it can access Azure resources.\n\u2711 Supporting services must be installed by using an Azure PowerShell script that is stored in Azure Storage.\nYou need to ensure that the requirements are met.\nWhich features should you use? To answer, drag the appropriate features to the correct requirements. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0013600001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-command",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/automation/automation-hybrid-runbook-worker https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-command",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #30 -- Topic 2",
        "questionIs": "HOTSPOT -\nA company is developing a Node.js web app. The web app code is hosted in a GitHub repository located at https://github.com/TailSpinToys/webapp.\nThe web app must be reviewed before it is moved to production. You must deploy the initial code release to a deployment slot named review.\nYou need to create the web app and deploy the code.\nHow should you complete the commands? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0013700001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: New-AzResourceGroup -\nThe New-AzResourceGroup cmdlet creates an Azure resource group.\n\nBox 2: New-AzAppServicePlan -\nThe New-AzAppServicePlan cmdlet creates an Azure App Service plan in a given location\n\nBox 3: New-AzWebApp -\nThe New-AzWebApp cmdlet creates an Azure Web App in a given a resource group\n\nBox 4: New-AzWebAppSlot -\nThe New-AzWebAppSlot cmdlet creates an Azure Web App slot.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/az.resources/new-azresourcegroup?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azappserviceplan?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azwebapp?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azwebappslot?view=azps-2.3.2",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: New-AzResourceGroup -\nThe New-AzResourceGroup cmdlet creates an Azure resource group.\n\nBox 2: New-AzAppServicePlan -\nThe New-AzAppServicePlan cmdlet creates an Azure App Service plan in a given location\n\nBox 3: New-AzWebApp -\nThe New-AzWebApp cmdlet creates an Azure Web App in a given a resource group\n\nBox 4: New-AzWebAppSlot -\nThe New-AzWebAppSlot cmdlet creates an Azure Web App slot.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/az.resources/new-azresourcegroup?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azappserviceplan?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azwebapp?view=azps-2.3.2 https://docs.microsoft.com/en-us/powershell/module/az.websites/new-azwebappslot?view=azps-2.3.2",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #31 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are developing an application that needs access to an Azure virtual machine (VM).\nThe access lifecycle for the application must be associated with the VM service instance.\nYou need to enable managed identity for the VM.\nHow should you complete the PowerShell segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0013900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: -IdentityType -\nEnable system-assigned managed identity on an existing Azure VM:\nTo enable a system-assigned managed identity, use the -IdentityType switch on the Update-AzVM cmdlet (see below).\n\nBox 2: $SystemAssigned -\n$vm = Get-AzVM -ResourceGroupName myResourceGroup -Name myVM\nUpdate-AzVM -ResourceGroupName myResourceGroup -VM $vm -IdentityType SystemAssigned\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/qs-configure-powershell-windows-vm",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: -IdentityType -\nEnable system-assigned managed identity on an existing Azure VM:\nTo enable a system-assigned managed identity, use the -IdentityType switch on the Update-AzVM cmdlet (see below).\n\nBox 2: $SystemAssigned -\n$vm = Get-AzVM -ResourceGroupName myResourceGroup -Name myVM\nUpdate-AzVM -ResourceGroupName myResourceGroup -VM $vm -IdentityType SystemAssigned\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/qs-configure-powershell-windows-vm",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #32 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Create an Azure Function app that uses the Consumption hosting model and that is triggered from the blob upload.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "In the Consumption hosting plan, resources are added dynamically as required by your functions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #33 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop and deploy an Azure App Service API app to a Windows-hosted deployment slot named Development. You create additional deployment slots named Testing and Production. You enable auto swap on the Production deployment slot.\nYou need to ensure that scripts run and resources are available before a swap operation occurs.\nSolution: Update the app with a method named statuscheck to run the scripts. Update the app settings for the app. Set the\nWEBSITE_SWAP_WARMUP_PING_PATH and WEBSITE_SWAP_WARMUP_PING_STATUSES with a path to the new method and appropriate response codes.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. No",
            "B. Yes Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "These are valid warm-up behavior options, but are not helpful in fixing swap problems.\nInstead update the web.config file to include the applicationInitialization configuration element. Specify custom initialization actions to run the scripts.\nNote: Some apps might require custom warm-up actions before the swap. The applicationInitialization configuration element in web.config lets you specify custom initialization actions. The swap operation waits for this custom warm-up to finish before swapping with the target slot. Here's a sample web.config fragment.\n<system.webServer>\n<applicationInitialization>\n<add initializationPage=\"/\" hostName=\"[app hostname]\" />\n<add initializationPage=\"/Home/About\" hostName=\"[app hostname]\" />\n</applicationInitialization>\n</system.webServer>\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/deploy-staging-slots#troubleshoot-swaps",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #34 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou create the following PowerShell script:\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0014200001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0014300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: No -\nThe AzScheduledQueryRuleSource is Heartbeat, not CPU.\n\nBox 2: Yes -\nThe AzScheduledQueryRuleSource is Heartbeat!\nNote: New-AzScheduledQueryRuleTriggerCondition creates an object of type Trigger Condition. This object is to be passed to the command that creates Alerting\nAction object.\n\nBox 3: No -\nThe schedule is 60 minutes, not two hours.\n-FrequencyInMinutes: The alert frequency.\n-TimeWindowInMinutes: The alert time window\nThe New-AzAscheduledQueryRuleSchedule command creates an object of type Schedule. This object is to be passed to the command that creates Log Alert\nRule.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/az.monitor/new-azscheduledqueryrule https://docs.microsoft.com/en-us/powershell/module/az.monitor/new-azscheduledqueryruletriggercondition",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: No -\nThe AzScheduledQueryRuleSource is Heartbeat, not CPU.\n\nBox 2: Yes -\nThe AzScheduledQueryRuleSource is Heartbeat!\nNote: New-AzScheduledQueryRuleTriggerCondition creates an object of type Trigger Condition. This object is to be passed to the command that creates Alerting\nAction object.\n\nBox 3: No -\nThe schedule is 60 minutes, not two hours.\n-FrequencyInMinutes: The alert frequency.\n-TimeWindowInMinutes: The alert time window\nThe New-AzAscheduledQueryRuleSchedule command creates an object of type Schedule. This object is to be passed to the command that creates Log Alert\nRule.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/az.monitor/new-azscheduledqueryrule https://docs.microsoft.com/en-us/powershell/module/az.monitor/new-azscheduledqueryruletriggercondition",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #35 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are developing an Azure Function app.\nThe app must meet the following requirements:\n\u2711 Enable developers to write the functions by using the Rust language.\n\u2711 Declaratively connect to an Azure Blob Storage account.\nYou need to implement the app.\nWhich Azure Function app features should you use? To answer, drag the appropriate features to the correct requirements. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0014500001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Custom handler -\nCustom handlers can be used to create functions in any language or runtime by running an HTTP server process, for example Go or Rust.\n\nBox 2: Trigger -\nFunctions are invoked by a trigger and can have exactly one. In addition to invoking the function, certain triggers also serve as bindings. You may also define multiple bindings in addition to the trigger. Bindings provide a declarative way to connect data to your code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-other https://docs.microsoft.com/en-us/dotnet/architecture/serverless/azure-functions",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Custom handler -\nCustom handlers can be used to create functions in any language or runtime by running an HTTP server process, for example Go or Rust.\n\nBox 2: Trigger -\nFunctions are invoked by a trigger and can have exactly one. In addition to invoking the function, certain triggers also serve as bindings. You may also define multiple bindings in addition to the trigger. Bindings provide a declarative way to connect data to your code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-other https://docs.microsoft.com/en-us/dotnet/architecture/serverless/azure-functions",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #36 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are developing an ASP.NET Core web application. You plan to deploy the application to Azure Web App for Containers.\nThe application needs to store runtime diagnostic data that must be persisted across application restarts. You have the following code:\n\nYou need to configure the application settings so that diagnostic data is stored as required.\nHow should you configure the web app's settings? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0014600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0014700001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: If WEBSITES_ENABLE_APP_SERVICE_STORAGE\nIf WEBSITES_ENABLE_APP_SERVICE_STORAGE setting is unspecified or set to true, the /home/ directory will be shared across scale instances, and files written will persist across restarts\n\nBox 2: /home -\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-faq",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: If WEBSITES_ENABLE_APP_SERVICE_STORAGE\nIf WEBSITES_ENABLE_APP_SERVICE_STORAGE setting is unspecified or set to true, the /home/ directory will be shared across scale instances, and files written will persist across restarts\n\nBox 2: /home -\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/containers/app-service-linux-faq",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #37 -- Topic 2",
        "questionIs": "You are developing a web app that is protected by Azure Web Application Firewall (WAF). All traffic to the web app is routed through an Azure Application\nGateway instance that is used by multiple web apps. The web app address is contoso.azurewebsites.net.\nAll traffic must be secured with SSL. The Azure Application Gateway instance is used by multiple web apps.\nYou need to configure the Azure Application Gateway for the web app.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. In the Azure Application Gateway's HTTP setting, enable the Use for App service setting. Most Voted Most Voted",
            "B. Convert the web app to run in an Azure App service environment (ASE).",
            "C. Add an authentication certificate for contoso.azurewebsites.net to the Azure Application Gateway. Most Voted",
            "D. In the Azure Application Gateway's HTTP setting, set the value of the Override backend path option to contoso22.azurewebsites.net. Most Voted"
        ],
        "answersAre": [
            "A",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "C",
            "D"
        ],
        "descriptionIs": "D: The ability to specify a host override is defined in the HTTP settings and can be applied to any back-end pool during rule creation.\nThe ability to derive the host name from the IP or FQDN of the back-end pool members. HTTP settings also provide an option to dynamically pick the host name from a back-end pool member's FQDN if configured with the option to derive host name from an individual back-end pool member.\nA (not C): SSL termination and end to end SSL with multi-tenant services.\nIn case of end to end SSL, trusted Azure services such as Azure App service web apps do not require whitelisting the backends in the application gateway.\nTherefore, there is no need to add any authentication certificates.\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/application-gateway/application-gateway-web-app-overview",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0014900001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #38 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop a software as a service (SaaS) offering to manage photographs. Users upload photos to a web service which then stores the photos in Azure\nStorage Blob storage. The storage account type is General-purpose V2.\nWhen photos are uploaded, they must be processed to produce and save a mobile-friendly version of the image. The process to produce a mobile-friendly version of the image must start in less than one minute.\nYou need to design the process that starts the photo processing.\nSolution: Use the Azure Blob Storage change feed to trigger photo processing.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "The change feed is a log of changes that are organized into hourly segments but appended to and updated every few minutes. These segments are created only when there are blob change events that occur in that hour.\nInstead catch the triggered event, so move the photo processing to an Azure Function triggered from the blob upload.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #39 -- Topic 2",
        "questionIs": "You are developing a web application that runs as an Azure Web App. The web application stores data in Azure SQL Database and stores files in an Azure\nStorage account. The web application makes HTTP requests to external services as part of normal operations.\nThe web application is instrumented with Application Insights. The external services are OpenTelemetry compliant.\nYou need to ensure that the customer ID of the signed in user is associated with all operations throughout the overall system.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Add the customer ID for the signed in user to the CorrelationContext in the web application Most Voted",
            "B. On the current SpanContext, set the TraceId to the customer ID for the signed in user",
            "C. Set the header Ocp-Apim-Trace to the customer ID for the signed in user",
            "D. Create a new SpanContext with the TraceFlags value set to the customer ID for the signed in user"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/correlation",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #40 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are developing an Azure Function App. You develop code by using a language that is not supported by the Azure Function App host. The code language supports HTTP primitives.\nYou must deploy the code to a production Azure Function App environment.\nYou need to configure the app for deployment.\nWhich configuration values should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0015200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Docker container -\nA custom handler can be deployed to every Azure Functions hosting option. If your handler requires operating system or platform dependencies (such as a language runtime), you may need to use a custom container. You can create and deploy your code to Azure Functions as a custom Docker container.\n\nBox 2: PowerShell core -\nWhen creating a function app in Azure for custom handlers, we recommend you select .NET Core as the stack. A \"Custom\" stack for custom handlers will be added in the future.\nPowerShell Core (PSC) is based on the new .NET Core runtime.\n\nBox 3: 7.0 -\nOn Windows: The Azure Az PowerShell module is also supported for use with PowerShell 5.1 on Windows.\nOn Linux: PowerShell 7.0.6 LTS, PowerShell 7.1.3, or higher is the recommended version of PowerShell for use with the Azure Az PowerShell module on all platforms.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image https://docs.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.1.0",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Docker container -\nA custom handler can be deployed to every Azure Functions hosting option. If your handler requires operating system or platform dependencies (such as a language runtime), you may need to use a custom container. You can create and deploy your code to Azure Functions as a custom Docker container.\n\nBox 2: PowerShell core -\nWhen creating a function app in Azure for custom handlers, we recommend you select .NET Core as the stack. A \"Custom\" stack for custom handlers will be added in the future.\nPowerShell Core (PSC) is based on the new .NET Core runtime.\n\nBox 3: 7.0 -\nOn Windows: The Azure Az PowerShell module is also supported for use with PowerShell 5.1 on Windows.\nOn Linux: PowerShell 7.0.6 LTS, PowerShell 7.1.3, or higher is the recommended version of PowerShell for use with the Azure Az PowerShell module on all platforms.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-function-linux-custom-image https://docs.microsoft.com/en-us/powershell/azure/install-az-ps?view=azps-7.1.0",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #41 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou provision virtual machines (VMs) as development environments.\nOne VM does not start. The VM is stuck in a Windows update process. You attach the OS disk for the affected VM to a recovery VM.\nYou need to correct the issue.\nIn which order should you perform the actions? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0015500001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Remove the update that causes the problem\n1. Take a snapshot of the OS disk of the affected VM as a backup.\n2. Attach the OS disk to a recovery VM.\n3. Once the OS disk is attached on the recovery VM, run diskmgmt.msc to open Disk Management, and ensure the attached disk is ONLINE.\n4. (Step 1) Open an elevated command prompt instance (Run as administrator). Run the following command to get the list of the update packages that are on the attached OS disk: dism /image:<Attached OS disk>:\\ /get-packages > c:\\temp\\Patch_level\n5. (Step 2) Open the C:\\temp\\Patch_level.txt file, and then read it from the bottom up. Locate the update that's in Install Pending or Uninstall Pending state.\n6. Remove the update that caused the problem:\ndism /Image:<Attached OS disk>:\\ /Remove-Package /PackageName:<PACK\n7. (Step 4) Detach the OS disk and recreate the VM. Then check whether the issue is resolved.\nReference:\nhttps://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-stuck-updating-boot-error",
        "mostVotedAre": [],
        "descriptionIs": "Remove the update that causes the problem\n1. Take a snapshot of the OS disk of the affected VM as a backup.\n2. Attach the OS disk to a recovery VM.\n3. Once the OS disk is attached on the recovery VM, run diskmgmt.msc to open Disk Management, and ensure the attached disk is ONLINE.\n4. (Step 1) Open an elevated command prompt instance (Run as administrator). Run the following command to get the list of the update packages that are on the attached OS disk: dism /image:<Attached OS disk>:\\ /get-packages > c:\\temp\\Patch_level\n5. (Step 2) Open the C:\\temp\\Patch_level.txt file, and then read it from the bottom up. Locate the update that's in Install Pending or Uninstall Pending state.\n6. Remove the update that caused the problem:\ndism /Image:<Attached OS disk>:\\ /Remove-Package /PackageName:<PACK\n7. (Step 4) Detach the OS disk and recreate the VM. Then check whether the issue is resolved.\nReference:\nhttps://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/troubleshoot-stuck-updating-boot-error",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #42 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop an HTTP triggered Azure Function app to process Azure Storage blob data. The app is triggered using an output binding on the blob.\nThe app continues to time out after four minutes. The app must process the blob data.\nYou need to ensure the app does not time out and processes the blob data.\nSolution: Update the functionTimeout property of the host.json project file to 10 minutes.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Instead pass the HTTP trigger payload into an Azure Service Bus queue to be processed by a queue trigger function and return an immediate HTTP success response.\nNote: Large, long-running functions can cause unexpected timeout issues. General best practices include:\nWhenever possible, refactor large functions into smaller function sets that work together and return responses fast. For example, a webhook or HTTP trigger function might require an acknowledgment response within a certain time limit; it's common for webhooks to require an immediate response. You can pass the\nHTTP trigger payload into a queue to be processed by a queue trigger function. This approach lets you defer the actual work and return an immediate response.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #43 -- Topic 2",
        "questionIs": "HOTSPOT -\nYou are developing an Azure Durable Function based application that processes a list of input values. The application is monitored using a console application that retrieves JSON data from an Azure Function diagnostic endpoint.\nDuring processing a single instance of invalid input does not cause the function to fail. Invalid input must be available to the monitoring application.\nYou need to implement the Azure Durable Function and the monitoring console application.\nHow should you complete the code segments? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0015800001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: await context.CallEntityAsync(input[errindex],\"error\")\nOrchestration signals and calls an entity\nOrchestrator functions can access entities by using APIs on the orchestration trigger binding.\nExample:\n[FunctionName(\"CounterOrchestration\")]\npublic static async Task Run(\n[OrchestrationTrigger] IDurableOrchestrationContext context)\n{\nvar entityId = new EntityId(nameof(Counter), \"myCounter\");\n// Two-way call to the entity which returns a value - awaits the response int currentValue = await context.CallEntityAsync<int>(entityId, \"Get\");\n\nBox 2: Failed -\nDuring processing a single instance of invalid input does not cause the function to fail.\nNote: RuntimeStatus: One of the following values:\nFailed: The instance failed with an error.\nCompleted: The instance has completed normally.\nTerminated: The instance was stopped abruptly.\nPending: The instance has been scheduled but has not yet started running.\nRunning: The instance has started running.\nContinuedAsNew: The instance has restarted itself with a new history. This state is a transient state.\n\nBox 3: Input -\nInvalid input must be available to the monitoring application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-entities https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: await context.CallEntityAsync(input[errindex],\"error\")\nOrchestration signals and calls an entity\nOrchestrator functions can access entities by using APIs on the orchestration trigger binding.\nExample:\n[FunctionName(\"CounterOrchestration\")]\npublic static async Task Run(\n[OrchestrationTrigger] IDurableOrchestrationContext context)\n{\nvar entityId = new EntityId(nameof(Counter), \"myCounter\");\n// Two-way call to the entity which returns a value - awaits the response int currentValue = await context.CallEntityAsync<int>(entityId, \"Get\");\n\nBox 2: Failed -\nDuring processing a single instance of invalid input does not cause the function to fail.\nNote: RuntimeStatus: One of the following values:\nFailed: The instance failed with an error.\nCompleted: The instance has completed normally.\nTerminated: The instance was stopped abruptly.\nPending: The instance has been scheduled but has not yet started running.\nRunning: The instance has started running.\nContinuedAsNew: The instance has restarted itself with a new history. This state is a transient state.\n\nBox 3: Input -\nInvalid input must be available to the monitoring application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-entities https://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-instance-management",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #44 -- Topic 2",
        "questionIs": "You are developing an Azure Durable Function to manage an online ordering process.\nThe process must call an external API to gather product discount information.\nYou need to implement the Azure Durable Function.\nWhich Azure Durable Function types should you use? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Orchestrator Most Voted",
            "B. Entity",
            "C. Client",
            "D. Activity Most Voted"
        ],
        "answersAre": [
            "A",
            "B"
        ],
        "mostVotedAre": [
            "A",
            "D"
        ],
        "descriptionIs": "The Durable Functions extension exposes a set of built-in HTTP APIs that can be used to perform management tasks on orchestrations, entities, and task hubs.\nThese HTTP APIs are extensibility webhooks that are authorized by the Azure Functions host but handled directly by the Durable Functions extension.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-http-api",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #45 -- Topic 2",
        "questionIs": "DRAG DROP -\nYou are authoring a set of nested Azure Resource Manager templates to deploy multiple Azure resources.\nThe templates must be tested before deployment and must follow recommended practices.\nYou need to validate and test the templates before deployment.\nWhich tools should you use? To answer, drag the appropriate tools to the correct requirements. Each tool may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0016200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Resource Manager test toolkit\n\nUse ARM template test toolkit -\nThe Azure Resource Manager template (ARM template) test toolkit checks whether your template uses recommended practices. When your template isn't compliant with recommended practices, it returns a list of warnings with the suggested changes. By using the test toolkit, you can learn how to avoid common problems in template development.\n\nBox 2: What-if operation -\nARM template deployment what-if operation\nBefore deploying an Azure Resource Manager template (ARM template), you can preview the changes that will happen. Azure Resource Manager provides the what-if operation to let you see how resources will change if you deploy the template. The what-if operation doesn't make any changes to existing resources.\nInstead, it predicts the changes if the specified template is deployed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/test-toolkit https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-what-if",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Resource Manager test toolkit\n\nUse ARM template test toolkit -\nThe Azure Resource Manager template (ARM template) test toolkit checks whether your template uses recommended practices. When your template isn't compliant with recommended practices, it returns a list of warnings with the suggested changes. By using the test toolkit, you can learn how to avoid common problems in template development.\n\nBox 2: What-if operation -\nARM template deployment what-if operation\nBefore deploying an Azure Resource Manager template (ARM template), you can preview the changes that will happen. Azure Resource Manager provides the what-if operation to let you see how resources will change if you deploy the template. The what-if operation doesn't make any changes to existing resources.\nInstead, it predicts the changes if the specified template is deployed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/test-toolkit https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/deploy-what-if",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #46 -- Topic 2",
        "questionIs": "You develop Azure Durable Functions to manage vehicle loans.\n\nThe loan process includes multiple actions that must be run in a specified order. One of the actions includes a customer credit check process, which may require multiple days to process.\n\nYou need to implement Azure Durable Functions for the loan process.\n\nWhich Azure Durable Functions type should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. orchestrator Most Voted",
            "B. client",
            "C. entity",
            "D. activity"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #47 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an Azure Function app.\n\nAll functions in the app meet the following requirements:\n\n\u2022 Run until either a successful run or until 10 run attempts occur.\n\u2022 Ensure that there are at least 20 seconds between attempts for up to 15 minutes.\n\nYou need to configure the host.json file.\n\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image387.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #48 -- Topic 2",
        "questionIs": "You develop Azure Web Apps for a commercial diving company. Regulations require that all divers fill out a health questionnaire every 15 days after each diving job starts.\n\nYou need to configure the Azure Web Apps so that the instance count scales up when divers are filling out the questionnaire and scales down after they are complete.\n\nYou need to configure autoscaling.\n\nWhat are two possible auto scaling configurations to achieve this goal? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Recurrence profile Most Voted",
            "B. CPU usage-based autoscaling Most Voted",
            "C. Fixed date profile Most Voted",
            "D. Predictive autoscaling Most Voted"
        ],
        "answersAre": [
            "C",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "B",
            "C",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #49 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an online game that allows players to vote for their favorite photo that illustrates a word. The game is built by using Azure Functions and uses durable entities to track the vote count.\n\nThe voting window is 30 seconds. You must minimize latency.\n\nYou need to implement the Azure Function for voting.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image422.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #50 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou have an App Service plan named asp1 based on the Free pricing tier.\n\nYou plan to use asp1 to implement an Azure Function app with a queue trigger. Your solution must minimize cost.\n\nYou need to identify the configuration options that will meet the requirements.\n\nWhich value should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image443.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #51 -- Topic 2",
        "questionIs": "DRAG DROP\n-\n\nYou are developing several microservices to run on Azure Container Apps.\n\nThe microservices must allow HTTPS access by using a custom domain.\n\nYou need to configure the custom domain in Azure Container Apps.\n\nIn which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image445.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #52 -- Topic 2",
        "questionIs": "You are developing several microservices to run on Azure Container Apps. External HTTP ingress traffic has been enabled for the microservices.\n\nThe microservices must be deployed to the same virtual network and write logs to the same Log Analytics workspace.\n\nYou need to deploy the microservices.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Enable single revision mode.",
            "B. Use a separate environment for each container.",
            "C. Use a private container registry image and single image for all containers.",
            "D. Use a single environment for all containers. Most Voted",
            "E. Enable multiple revision mode."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #53 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou are developing several microservices to run on Azure Container Apps. External HTTP ingress traffic has been enabled for the microservices.\n\nA deployed microservice must be updated to allow users to test new features. You have the following requirements:\n\n\u2022 Enable and maintain a single URL for the updated microservice to provide to test users.\n\u2022 Update the microservice that corresponds to the current microservice version.\n\nYou need to configure Azure Container Apps.\n\nWhich features should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image463.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #54 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou plan to develop an Azure Functions app with an HTTP trigger.\n\nThe app must support the following requirements:\n\n\u2022 Event-driven scaling\n\u2022 Ability to use custom Linux images for function execution\n\nYou need to identify the app\u2019s hosting plan and the maximum amount of time that the app function can take to respond to incoming requests.\n\nWhich configuration setting values should you use? To answer, select the appropriate values in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image478.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #55 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou develop a Python application for image rendering. The application uses GPU resources to optimize rendering processes.\n\nYou have the following requirements:\n\n\u2022 The application must be deployed to a Linux container.\n\u2022 The container must be stopped when the image rendering is complete.\n\u2022 The solution must minimize cost.\n\nYou need to deploy the application to Azure.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image480.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #56 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nYou plan to develop an Azure Functions app with an Azure Blob Storage trigger. The app will be used infrequently, with a limited duration of individual executions.\n\nThe app must meet the following requirements:\n\n\u2022 Event-driven scaling\n\u2022 Support for deployment slots\n\u2022 Minimize costs\n\nYou need to identify the hosting plan and the maximum duration when executing the app.\n\nWhich configuration setting values should you use? To answer, select the appropriate values in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image482.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #57 -- Topic 2",
        "questionIs": "You are developing an ASP.NET Core app hosted in Azure App Service.\n\nThe app requires custom claims to be returned from Microsoft Entra ID for user authorization. The claims must be removed when the app registration is removed.\n\nYou need to include the custom claims in the user access token.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Require the https://graph.microsoft.com/.default scope during authentication.",
            "B. Configure the app to use the OAuth 2.0 authorization code flow.",
            "C. Implement custom middleware to retrieve role information from Azure AD.",
            "D. Add the groups to the groupMembershipClaims attribute in the app manifest.",
            "E. Add the roles to the appRoles attribute in the app manifest. Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "E"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #58 -- Topic 2",
        "questionIs": "You are developing a microservice to run on Azure Container Apps for a company. External HTTP ingress traffic has been enabled.\n\nThe company requires that updates to the microservice must not cause downtime.\n\nYou need to deploy an update to the microservices.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Enable single revision mode. Most Voted",
            "B. Use multiple environments for each container.",
            "C. Use a private container registry and single image for all containers.",
            "D. Use a single environment for all containers.",
            "E. Enable multiple revision mode."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #59 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\nA company uses Azure Container Apps. A container app named App1 resides in a resource group named RG1.\n\nThe company requires testing of updates to App1.\n\nYou enable multiple revision modes on App1.\n\nYou need to ensure traffic is routed to each revision of App1.\n\nHow should you complete the code segment?\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image500.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #60 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou deploy an Azure Container Apps app and disable ingress on the container app.\n\nUsers report that they are unable to access the container app. You investigate and observe that the app has scaled to 0 instances.\n\nYou need to resolve the issue with the container app.\n\nSolution: Enable ingress, create an HTTP scale rule, and apply the rule to the container app.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #61 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou deploy an Azure Container Apps app and disable ingress on the container app.\n\nUsers report that they are unable to access the container app. You investigate and observe that the app has scaled to 0 instances.\n\nYou need to resolve the issue with the container app.\n\nSolution: Enable ingress, create a custom scale rule, and apply the rule to the container app.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #62 -- Topic 2",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou deploy an Azure Container Apps app and disable ingress on the container app.\n\nUsers report that they are unable to access the container app. You investigate and observe that the app has scaled to 0 instances.\n\nYou need to resolve the issue with the container app.\n\nSolution: Enable ingress and configure the minimum replicas to 1 for the container app.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #63 -- Topic 2",
        "questionIs": "HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment\n-\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website\n-\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms\n-\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors\n-\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms\n-\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors\n-\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff\n-\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity\n-\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues\n-\n\n\nCorporate website\n-\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors\n-\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to configure App Service to support the corporate website migration.\n\nWhich configuration should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image502.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing a solution that uses the Azure Storage Client library for .NET. You have the following code: (Line numbers are included for reference only.)\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0020500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0020600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Yes -\nAcquireLeaseAsync does not specify leaseTime.\nleaseTime is a TimeSpan representing the span of time for which to acquire the lease, which will be rounded down to seconds. If null, an infinite lease will be acquired. If not null, this must be 15 to 60 seconds.\n\nBox 2: No -\nThe GetBlockBlobReference method just gets a reference to a block blob in this container.\n\nBox 3: Yes -\nThe BreakLeaseAsync method initiates an asynchronous operation that breaks the current lease on this container.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.acquireleaseasync https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.getblockblobreference https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.breakleaseasync",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Yes -\nAcquireLeaseAsync does not specify leaseTime.\nleaseTime is a TimeSpan representing the span of time for which to acquire the lease, which will be rounded down to seconds. If null, an infinite lease will be acquired. If not null, this must be 15 to 60 seconds.\n\nBox 2: No -\nThe GetBlockBlobReference method just gets a reference to a block blob in this container.\n\nBox 3: Yes -\nThe BreakLeaseAsync method initiates an asynchronous operation that breaks the current lease on this container.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.acquireleaseasync https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.getblockblobreference https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.storage.blob.cloudblobcontainer.breakleaseasync",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 3",
        "questionIs": "You are building a website that uses Azure Blob storage for data storage. You configure Azure Blob storage lifecycle to move all blobs to the archive tier after 30 days.\nCustomers have requested a service-level agreement (SLA) for viewing data older than 30 days.\nYou need to document the minimum SLA for data recovery.\nWhich SLA should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. at least two days",
            "B. between one and 15 hours Most Voted",
            "C. at least one day",
            "D. between zero and 60 minutes"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "The archive access tier has the lowest storage cost. But it has higher data retrieval costs compared to the hot and cool tiers. Data in the archive tier can take several hours to retrieve depending on the priority of the rehydration. For small objects, a high priority rehydrate may retrieve the object from archive in under 1 hour.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?tabs=azure-portal",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing a ticket reservation system for an airline.\nThe storage solution for the application must meet the following requirements:\n\u2711 Ensure at least 99.99% availability and provide low latency.\n\u2711 Accept reservations even when localized network outages or other unforeseen failures occur.\n\u2711 Process reservations in the exact sequence as reservations are submitted to minimize overbooking or selling the same seat to multiple travelers.\n\u2711 Allow simultaneous and out-of-order reservations with a maximum five-second tolerance window.\nYou provision a resource group named airlineResourceGroup in the Azure South-Central US region.\nYou need to provision a SQL API Cosmos DB account to support the app.\nHow should you complete the Azure CLI commands? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0020900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: BoundedStaleness -\nBounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most \"K\" versions (that is,\n\"updates\") of an item or by \"T\" time interval. In other words, when you choose bounded staleness, the \"staleness\" can be configured in two ways:\nThe number of versions (K) of the item\nThe time interval (T) by which the reads might lag behind the writes\nIncorrect Answers:\n\nStrong -\nStrong consistency offers a linearizability guarantee. Linearizability refers to serving requests concurrently. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.\nBox 2: --enable-automatic-failover true\\\nFor multi-region Cosmos accounts that are configured with a single-write region, enable automatic-failover by using Azure CLI or Azure portal. After you enable automatic failover, whenever there is a regional disaster, Cosmos DB will automatically failover your account.\nQuestion: Accept reservations event when localized network outages or other unforeseen failures occur.\nBox 3: --locations'southcentralus=0 eastus=1 westus=2\nNeed multi-region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/cosmos-db/manage-with-cli.md",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: BoundedStaleness -\nBounded staleness: The reads are guaranteed to honor the consistent-prefix guarantee. The reads might lag behind writes by at most \"K\" versions (that is,\n\"updates\") of an item or by \"T\" time interval. In other words, when you choose bounded staleness, the \"staleness\" can be configured in two ways:\nThe number of versions (K) of the item\nThe time interval (T) by which the reads might lag behind the writes\nIncorrect Answers:\n\nStrong -\nStrong consistency offers a linearizability guarantee. Linearizability refers to serving requests concurrently. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.\nBox 2: --enable-automatic-failover true\\\nFor multi-region Cosmos accounts that are configured with a single-write region, enable automatic-failover by using Azure CLI or Azure portal. After you enable automatic failover, whenever there is a regional disaster, Cosmos DB will automatically failover your account.\nQuestion: Accept reservations event when localized network outages or other unforeseen failures occur.\nBox 3: --locations'southcentralus=0 eastus=1 westus=2\nNeed multi-region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/cosmos-db/manage-with-cli.md",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are preparing to deploy a Python website to an Azure Web App using a container. The solution will use multiple containers in the same container group. The\nDockerfile that builds the container is as follows:\n\nYou build a container by using the following command. The Azure Container Registry instance named images is a private registry.\n\nThe user name and password for the registry is admin.\nThe Web App must always run the same version of the website regardless of future builds.\nYou need to create an Azure Web App to run the website.\nHow should you complete the commands? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0021300001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0021300002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0021400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: --SKU B1 --hyper-v -\n--hyper-v\nHost web app on Windows container.\nBox 2: --deployment-source-url images.azurecr.io/website:v1.0.0\n--deployment-source-url -u\nGit repository URL to link with manual integration.\nThe Web App must always run the same version of the website regardless of future builds.\nIncorrect:\n--deployment-container-image-name -i\nLinux only. Container image name from Docker Hub, e.g. publisher/image-name:tag.\nBox 3: az webapp config container set -url https://images.azurecr.io -u admin -p admin az webapp config container set\nSet a web app container's settings.\nParemeter: --docker-registry-server-url -r\nThe container registry server url.\nThe Azure Container Registry instance named images is a private registry.\nExample:\naz webapp config container set --docker-registry-server-url https://{azure-container-registry-name}.azurecr.io\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/appservice/plan",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: --SKU B1 --hyper-v -\n--hyper-v\nHost web app on Windows container.\nBox 2: --deployment-source-url images.azurecr.io/website:v1.0.0\n--deployment-source-url -u\nGit repository URL to link with manual integration.\nThe Web App must always run the same version of the website regardless of future builds.\nIncorrect:\n--deployment-container-image-name -i\nLinux only. Container image name from Docker Hub, e.g. publisher/image-name:tag.\nBox 3: az webapp config container set -url https://images.azurecr.io -u admin -p admin az webapp config container set\nSet a web app container's settings.\nParemeter: --docker-registry-server-url -r\nThe container registry server url.\nThe Azure Container Registry instance named images is a private registry.\nExample:\naz webapp config container set --docker-registry-server-url https://{azure-container-registry-name}.azurecr.io\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/appservice/plan",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing a back-end Azure App Service that scales based on the number of messages contained in a Service Bus queue.\nA rule already exists to scale up the App Service when the average queue length of unprocessed and valid queue messages is greater than 1000.\nYou need to add a new rule that will continuously scale down the App Service as long as the scale up condition is not met.\nHow should you configure the Scale rule? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0021700001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Service bus queue -\nYou are developing a back-end Azure App Service that scales based on the number of messages contained in a Service Bus queue.\n\nBox 2: ActiveMessage Count -\nActiveMessageCount: Messages in the queue or subscription that are in the active state and ready for delivery.\n\nBox 3: Count -\n\nBox 4: Less than or equal to -\nYou need to add a new rule that will continuously scale down the App Service as long as the scale up condition is not met.\nBox 5: Decrease count by",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Service bus queue -\nYou are developing a back-end Azure App Service that scales based on the number of messages contained in a Service Bus queue.\n\nBox 2: ActiveMessage Count -\nActiveMessageCount: Messages in the queue or subscription that are in the active state and ready for delivery.\n\nBox 3: Count -\n\nBox 4: Less than or equal to -\nYou need to add a new rule that will continuously scale down the App Service as long as the scale up condition is not met.\nBox 5: Decrease count by",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou have an application that uses Azure Blob storage.\nYou need to update the metadata of the blobs.\nWhich three methods should you use to develop the solution? To answer, move the appropriate methods from the list of methods to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0022100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Metadata.Add example:\n// Add metadata to the dictionary by calling the Add method\nmetadata.Add(\"docType\", \"textDocuments\");\nSetMetadataAsync example:\n// Set the blob's metadata.\nawait blob.SetMetadataAsync(metadata);\n// Set the blob's properties.\nawait blob.SetPropertiesAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-metadata",
        "mostVotedAre": [],
        "descriptionIs": "Metadata.Add example:\n// Add metadata to the dictionary by calling the Add method\nmetadata.Add(\"docType\", \"textDocuments\");\nSetMetadataAsync example:\n// Set the blob's metadata.\nawait blob.SetMetadataAsync(metadata);\n// Set the blob's properties.\nawait blob.SetPropertiesAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-metadata",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #7 -- Topic 3",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce\n2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\nYou must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Grid. Configure the machine identifier as the partition key and enable capture.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #8 -- Topic 3",
        "questionIs": "You develop Azure solutions.\nA .NET application needs to receive a message each time an Azure virtual machine finishes processing data. The messages must NOT persist after being processed by the receiving application.\nYou need to implement the .NET object that will receive the messages.\nWhich object should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. QueueClient Most Voted",
            "B. SubscriptionClient",
            "C. TopicClient",
            "D. CloudQueueClient"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "A queue allows processing of a message by a single consumer. Need a CloudQueueClient to access the Azure VM.\nIncorrect Answers:\nB, C: In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a publish and subscribe pattern. It's useful for scaling to large numbers of recipients.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #9 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou are maintaining an existing application that uses an Azure Blob GPv1 Premium storage account. Data older than three months is rarely used.\nData newer than three months must be available immediately. Data older than a year must be saved but does not need to be available immediately.\nYou need to configure the account to support a lifecycle management rule that moves blob data to archive storage for data not modified in the last year.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0022400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Upgrade the storage account to GPv2\nObject storage data tiering between hot, cool, and archive is supported in Blob Storage and General Purpose v2 (GPv2) accounts. General Purpose v1 (GPv1) accounts don't support tiering.\nYou can easily convert your existing GPv1 or Blob Storage accounts to GPv2 accounts through the Azure portal.\nStep 2: Copy the data to be archived to a Standard GPv2 storage account and then delete the data from the original storage account\nStep 3: Change the storage account access tier from hot to cool\nNote: Hot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30 days.\nArchive - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.\nOnly the hot and cool access tiers can be set at the account level. The archive access tier can only be set at the blob level.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Upgrade the storage account to GPv2\nObject storage data tiering between hot, cool, and archive is supported in Blob Storage and General Purpose v2 (GPv2) accounts. General Purpose v1 (GPv1) accounts don't support tiering.\nYou can easily convert your existing GPv1 or Blob Storage accounts to GPv2 accounts through the Azure portal.\nStep 2: Copy the data to be archived to a Standard GPv2 storage account and then delete the data from the original storage account\nStep 3: Change the storage account access tier from hot to cool\nNote: Hot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30 days.\nArchive - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.\nOnly the hot and cool access tiers can be set at the account level. The archive access tier can only be set at the blob level.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #10 -- Topic 3",
        "questionIs": "You develop Azure solutions.\nYou must connect to a No-SQL globally-distributed database by using the .NET API.\nYou need to create an object to configure and execute requests in the database.\nWhich code segment should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. new Container(EndpointUri, PrimaryKey);",
            "B. new Database(EndpointUri, PrimaryKey);",
            "C. new CosmosClient(EndpointUri, PrimaryKey); Most Voted"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Example:\n// Create a new instance of the Cosmos Client\nthis.cosmosClient = new CosmosClient(EndpointUri, PrimaryKey)\n//ADD THIS PART TO YOUR CODE\nawait this.CreateDatabaseAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql-api-get-started",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #11 -- Topic 3",
        "questionIs": "You have an existing Azure storage account that stores large volumes of data across multiple containers.\nYou need to copy all data from the existing storage account to a new storage account. The copy process must meet the following requirements:\n\u2711 Automate data movement.\n\u2711 Minimize user input required to perform the operation.\n\u2711 Ensure that the data movement process is recoverable.\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. AzCopy Most Voted",
            "B. Azure Storage Explorer",
            "C. Azure portal",
            "D. .NET Storage Client Library"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "You can copy blobs, directories, and containers between storage accounts by using the AzCopy v10 command-line utility.\nThe copy operation is synchronous so when the command returns, that indicates that all files have been copied.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-blobs-copy",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #12 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou are developing a web service that will run on Azure virtual machines that use Azure Storage. You configure all virtual machines to use managed identities.\nYou have the following requirements:\n\u2711 Secret-based authentication mechanisms are not permitted for accessing an Azure Storage account.\n\u2711 Must use only Azure Instance Metadata Service endpoints.\nYou need to write code to retrieve an access token to access Azure Storage. To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0022700003.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Azure Instance Metadata Service endpoints \"/oauth2/token\"\nBox 1: http://169.254.169.254/metadata/identity/oauth2/token\nSample request using the Azure Instance Metadata Service (IMDS) endpoint (recommended):\nGET 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' HTTP/1.1 Metadata: true\nBox 2: JsonConvert.DeserializeObject<Dictionary<string,string>>(payload);\nDeserialized token response; returning access code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token https://docs.microsoft.com/en-us/azure/service-fabric/how-to-managed-identity-service-fabric-app-code",
        "mostVotedAre": [],
        "descriptionIs": "Azure Instance Metadata Service endpoints \"/oauth2/token\"\nBox 1: http://169.254.169.254/metadata/identity/oauth2/token\nSample request using the Azure Instance Metadata Service (IMDS) endpoint (recommended):\nGET 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/' HTTP/1.1 Metadata: true\nBox 2: JsonConvert.DeserializeObject<Dictionary<string,string>>(payload);\nDeserialized token response; returning access code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/how-to-use-vm-token https://docs.microsoft.com/en-us/azure/service-fabric/how-to-managed-identity-service-fabric-app-code",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #13 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou are developing a new page for a website that uses Azure Cosmos DB for data storage. The feature uses documents that have the following format:\n\nYou must display data for the new page in a specific order. You create the following query for the page:\n\nYou need to configure a Cosmos DB policy to support the query.\nHow should you configure the policy? To answer, drag the appropriate JSON segments to the correct locations. Each JSON segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0022900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0022900002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0023000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: compositeIndexes -\nYou can order by multiple properties. A query that orders by multiple properties requires a composite index.\n\nBox 2: descending -\nExample: Composite index defined for (name ASC, age ASC):\nIt is optional to specify the order. If not specified, the order is ascending.\n{\n\"automatic\":true,\n\"indexingMode\":\"Consistent\",\n\"includedPaths\":[\n{\n\"path\":\"/*\"\n}\n],\n\"excludedPaths\":[],\n\"compositeIndexes\":[\n[\n{\n\"path\":\"/name\",\n},\n{\n\"path\":\"/age\",\n}\n]\n]\n}",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: compositeIndexes -\nYou can order by multiple properties. A query that orders by multiple properties requires a composite index.\n\nBox 2: descending -\nExample: Composite index defined for (name ASC, age ASC):\nIt is optional to specify the order. If not specified, the order is ascending.\n{\n\"automatic\":true,\n\"indexingMode\":\"Consistent\",\n\"includedPaths\":[\n{\n\"path\":\"/*\"\n}\n],\n\"excludedPaths\":[],\n\"compositeIndexes\":[\n[\n{\n\"path\":\"/name\",\n},\n{\n\"path\":\"/age\",\n}\n]\n]\n}",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #14 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are building a traffic monitoring system that monitors traffic along six highways. The system produces time series analysis-based reports for each highway.\nData from traffic sensors are stored in Azure Event Hub.\nTraffic data is consumed by four departments. Each department has an Azure Web App that displays the time series-based reports and contains a WebJob that processes the incoming data from Event Hub. All Web Apps run on App Service Plans with three instances.\nData throughput must be maximized. Latency must be minimized.\nYou need to implement the Azure Event Hub.\nWhich settings should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0023300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: 6 -\nThe number of partitions is specified at creation and must be between 2 and 32.\nThere are 6 highways.\n\nBox 2: Highway -\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: 6 -\nThe number of partitions is specified at creation and must be between 2 and 32.\nThere are 6 highways.\n\nBox 2: Highway -\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #15 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou are developing a microservices solution. You plan to deploy the solution to a multinode Azure Kubernetes Service (AKS) cluster.\nYou need to deploy a solution that includes the following features:\n\u2711 reverse proxy capabilities\n\u2711 configurable traffic routing\n\u2711 TLS termination with a custom certificate\nWhich components should you use? To answer, drag the appropriate components to the correct requirements. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0023500004.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Helm -\nTo create the ingress controller, use Helm to install nginx-ingress.\n\nBox 2: kubectl -\nTo find the cluster IP address of a Kubernetes pod, use the kubectl get pod command on your local machine, with the option -o wide .\n\nBox 3: Ingress Controller -\nAn ingress controller is a piece of software that provides reverse proxy, configurable traffic routing, and TLS termination for Kubernetes services. Kubernetes ingress resources are used to configure the ingress rules and routes for individual Kubernetes services.\nIncorrect Answers:\nVirtual Kubelet: Virtual Kubelet is an open-source Kubernetes kubelet implementation that masquerades as a kubelet. This allows Kubernetes nodes to be backed by Virtual Kubelet providers such as serverless cloud container platforms.\nCoreDNS: CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Like Kubernetes, the CoreDNS project is hosted by the\nCNCF.\nReference:\nhttps://docs.microsoft.com/bs-cyrl-ba/azure/aks/ingress-basic https://www.digitalocean.com/community/tutorials/how-to-inspect-kubernetes-networking",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Helm -\nTo create the ingress controller, use Helm to install nginx-ingress.\n\nBox 2: kubectl -\nTo find the cluster IP address of a Kubernetes pod, use the kubectl get pod command on your local machine, with the option -o wide .\n\nBox 3: Ingress Controller -\nAn ingress controller is a piece of software that provides reverse proxy, configurable traffic routing, and TLS termination for Kubernetes services. Kubernetes ingress resources are used to configure the ingress rules and routes for individual Kubernetes services.\nIncorrect Answers:\nVirtual Kubelet: Virtual Kubelet is an open-source Kubernetes kubelet implementation that masquerades as a kubelet. This allows Kubernetes nodes to be backed by Virtual Kubelet providers such as serverless cloud container platforms.\nCoreDNS: CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Like Kubernetes, the CoreDNS project is hosted by the\nCNCF.\nReference:\nhttps://docs.microsoft.com/bs-cyrl-ba/azure/aks/ingress-basic https://www.digitalocean.com/community/tutorials/how-to-inspect-kubernetes-networking",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #16 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou are implementing an order processing system. A point of sale application publishes orders to topics in an Azure Service Bus queue. The Label property for the topic includes the following data:\n\nThe system has the following requirements for subscriptions:\n\nYou need to implement filtering and maximize throughput while evaluating filters.\nWhich filter types should you implement? To answer, drag the appropriate filter types to the correct subscriptions. Each filter type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0023700001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0023700002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0023800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "FutureOrders: SQLFilter -\nHighPriortyOrders: CorrelationFilter\n\nCorrelationID only -\n\nInternationalOrders: SQLFilter -\nCountry NOT USA requires an SQL Filter\n\nHighQuantityOrders: SQLFilter -\nNeed to use relational operators so an SQL Filter is needed.\n\nAllOrders: No Filter -\nSQL Filter: SQL Filters - A SqlFilter holds a SQL-like conditional expression that is evaluated in the broker against the arriving messages' user-defined properties and system properties. All system properties must be prefixed with sys. in the conditional expression. The SQL-language subset for filter conditions tests for the existence of properties (EXISTS), as well as for null-values (IS NULL), logical NOT/AND/OR, relational operators, simple numeric arithmetic, and simple text pattern matching with LIKE.\nCorrelation Filters - A CorrelationFilter holds a set of conditions that are matched against one or more of an arriving message's user and system properties. A common use is to match against the CorrelationId property, but the application can also choose to match against ContentType, Label, MessageId, ReplyTo,\nReplyToSessionId, SessionId, To, and any user-defined properties. A match exists when an arriving message's value for a property is equal to the value specified in the correlation filter. For string expressions, the comparison is case-sensitive. When specifying multiple match properties, the filter combines them as a logical\nAND condition, meaning for the filter to match, all conditions must match.\nBoolean filters - The TrueFilter and FalseFilter either cause all arriving messages (true) or none of the arriving messages (false) to be selected for the subscription.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters",
        "mostVotedAre": [],
        "descriptionIs": "FutureOrders: SQLFilter -\nHighPriortyOrders: CorrelationFilter\n\nCorrelationID only -\n\nInternationalOrders: SQLFilter -\nCountry NOT USA requires an SQL Filter\n\nHighQuantityOrders: SQLFilter -\nNeed to use relational operators so an SQL Filter is needed.\n\nAllOrders: No Filter -\nSQL Filter: SQL Filters - A SqlFilter holds a SQL-like conditional expression that is evaluated in the broker against the arriving messages' user-defined properties and system properties. All system properties must be prefixed with sys. in the conditional expression. The SQL-language subset for filter conditions tests for the existence of properties (EXISTS), as well as for null-values (IS NULL), logical NOT/AND/OR, relational operators, simple numeric arithmetic, and simple text pattern matching with LIKE.\nCorrelation Filters - A CorrelationFilter holds a set of conditions that are matched against one or more of an arriving message's user and system properties. A common use is to match against the CorrelationId property, but the application can also choose to match against ContentType, Label, MessageId, ReplyTo,\nReplyToSessionId, SessionId, To, and any user-defined properties. A match exists when an arriving message's value for a property is equal to the value specified in the correlation filter. For string expressions, the comparison is case-sensitive. When specifying multiple match properties, the filter combines them as a logical\nAND condition, meaning for the filter to match, all conditions must match.\nBoolean filters - The TrueFilter and FalseFilter either cause all arriving messages (true) or none of the arriving messages (false) to be selected for the subscription.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/topic-filters",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #17 -- Topic 3",
        "questionIs": "DRAG DROP -\nYour company has several websites that use a company logo image. You use Azure Content Delivery Network (CDN) to store the static image.\nYou need to determine the correct process of how the CDN and the Point of Presence (POP) server will distribute the image and list the items in the correct order.\nIn which order do the actions occur? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0024100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: A user requests the image..\nA user requests a file (also called an asset) by using a URL with a special domain name, such as <endpoint name>.azureedge.net. This name can be an endpoint hostname or a custom domain. The DNS routes the request to the best performing POP location, which is usually the POP that is geographically closest to the user.\nStep 2: If no edge servers in the POP have the..\nIf no edge servers in the POP have the file in their cache, the POP requests the file from the origin server. The origin server can be an Azure Web App, Azure\nCloud Service, Azure Storage account, or any publicly accessible web server.\nStep 3: The origin server returns the..\nThe origin server returns the file to an edge server in the POP.\nAn edge server in the POP caches the file and returns the file to the original requestor (Alice). The file remains cached on the edge server in the POP until the time-to-live (TTL) specified by its HTTP headers expires. If the origin server didn't specify a TTL, the default TTL is seven days.\nStep 4: Subsequent requests for..\nAdditional users can then request the same file by using the same URL that the original user used, and can also be directed to the same POP.\nIf the TTL for the file hasn't expired, the POP edge server returns the file directly from the cache. This process results in a faster, more responsive user experience.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-overview",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: A user requests the image..\nA user requests a file (also called an asset) by using a URL with a special domain name, such as <endpoint name>.azureedge.net. This name can be an endpoint hostname or a custom domain. The DNS routes the request to the best performing POP location, which is usually the POP that is geographically closest to the user.\nStep 2: If no edge servers in the POP have the..\nIf no edge servers in the POP have the file in their cache, the POP requests the file from the origin server. The origin server can be an Azure Web App, Azure\nCloud Service, Azure Storage account, or any publicly accessible web server.\nStep 3: The origin server returns the..\nThe origin server returns the file to an edge server in the POP.\nAn edge server in the POP caches the file and returns the file to the original requestor (Alice). The file remains cached on the edge server in the POP until the time-to-live (TTL) specified by its HTTP headers expires. If the origin server didn't specify a TTL, the default TTL is seven days.\nStep 4: Subsequent requests for..\nAdditional users can then request the same file by using the same URL that the original user used, and can also be directed to the same POP.\nIf the TTL for the file hasn't expired, the POP edge server returns the file directly from the cache. This process results in a faster, more responsive user experience.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #18 -- Topic 3",
        "questionIs": "You are developing an Azure Cosmos DB solution by using the Azure Cosmos DB SQL API. The data includes millions of documents. Each document may contain hundreds of properties.\nThe properties of the documents do not contain distinct values for partitioning. Azure Cosmos DB must scale individual containers in the database to meet the performance needs of the application by spreading the workload evenly across all partitions over time.\nYou need to select a partition key.\nWhich two partition keys can you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. a single property value that does not appear frequently in the documents",
            "B. a value containing the collection name",
            "C. a single property value that appears frequently in the documents",
            "D. a concatenation of multiple property values with a random suffix appended Most Voted",
            "E. a hash suffix appended to a property value Most Voted"
        ],
        "answersAre": [
            "D",
            "E"
        ],
        "mostVotedAre": [
            "D",
            "E"
        ],
        "descriptionIs": "You can form a partition key by concatenating multiple property values into a single artificial partitionKey property. These keys are referred to as synthetic keys.\nAnother possible strategy to distribute the workload more evenly is to append a random number at the end of the partition key value. When you distribute items in this way, you can perform parallel write operations across partitions.\nNote: It's the best practice to have a partition key with many distinct values, such as hundreds or thousands. The goal is to distribute your data and workload evenly across the items associated with these partition key values. If such a property doesn't exist in your data, you can construct a synthetic partition key.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/synthetic-partition-keys",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #19 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing an Azure-hosted e-commerce web application. The application will use Azure Cosmos DB to store sales orders. You are using the latest SDK to manage the sales orders in the database.\nYou create a new Azure Cosmos DB instance. You include a valid endpoint and valid authorization key to an appSettings.json file in the code project.\nYou are evaluating the following application code: (Line number are included for reference only.)\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0024400001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0024500001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Yes -\nThe createDatabaseIfNotExistsAsync method checks if a database exists, and if it doesn't, create it.\nThe Database.CreateContainerAsync method creates a container as an asynchronous operation in the Azure Cosmos service.\n\nBox 2: Yes -\nThe CosmosContainer.CreateItemAsync method creates an item as an asynchronous operation in the Azure Cosmos service.\n\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.database.createcontainerasync https://docs.microsoft.com/en-us/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Yes -\nThe createDatabaseIfNotExistsAsync method checks if a database exists, and if it doesn't, create it.\nThe Database.CreateContainerAsync method creates a container as an asynchronous operation in the Azure Cosmos service.\n\nBox 2: Yes -\nThe CosmosContainer.CreateItemAsync method creates an item as an asynchronous operation in the Azure Cosmos service.\n\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.cosmosclient.createdatabaseifnotexistsasync https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.cosmos.database.createcontainerasync https://docs.microsoft.com/en-us/dotnet/api/azure.cosmos.cosmoscontainer.createitemasync",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #20 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou develop an Azure solution that uses Cosmos DB.\nThe current Cosmos DB container must be replicated and must use a partition key that is optimized for queries.\nYou need to implement a change feed processor solution.\nWhich change feed processor components should you use? To answer, drag the appropriate components to the correct requirements. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view the content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0024700001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: The monitored container -\nThe monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.\n\nBox 2: The lease container -\nThe lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.\nBox 3: The host: A host is an application instance that uses the change feed processor to listen for changes. Multiple instances with the same lease configuration can run in parallel, but each instance should have a different instance name.\n\nBox 4: The delegate -\nThe delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-processor",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: The monitored container -\nThe monitored container has the data from which the change feed is generated. Any inserts and updates to the monitored container are reflected in the change feed of the container.\n\nBox 2: The lease container -\nThe lease container acts as a state storage and coordinates processing the change feed across multiple workers. The lease container can be stored in the same account as the monitored container or in a separate account.\nBox 3: The host: A host is an application instance that uses the change feed processor to listen for changes. Multiple instances with the same lease configuration can run in parallel, but each instance should have a different instance name.\n\nBox 4: The delegate -\nThe delegate is the code that defines what you, the developer, want to do with each batch of changes that the change feed processor reads.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/change-feed-processor",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #21 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing a web application that will use Azure Storage. Older data will be less frequently used than more recent data.\nYou need to configure data storage for the application. You have the following requirements:\n\u2711 Retain copies of data for five years.\n\u2711 Minimize costs associated with storing data that is over one year old.\n\u2711 Implement Zone Redundant Storage for application data.\nWhat should you do? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0024800004.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/azure/storage/blobs/toc.json",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/azure/storage/blobs/toc.json",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #22 -- Topic 3",
        "questionIs": "HOTSPOT -\nA company develops a series of mobile games. All games use a single leaderboard service.\nYou have the following requirements:\n\u2711 Code must be scalable and allow for growth.\n\u2711 Each record must consist of a playerId, gameId, score, and time played.\n\u2711 When users reach a new high score, the system will save the new score using the SaveScore function below.\nEach game is assigned an Id based on the series title.\n\nYou plan to store customer information in Azure Cosmos DB. The following data already exists in the database:\n\nYou develop the following code to save scores in the data store. (Line numbers are included for reference only.)\n\nYou develop the following code to query the database. (Line numbers are included for reference only.)\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0024900005.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0025000001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0025000002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0025000003.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0025100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Yes -\nCreate a table.\nA CloudTableClient object lets you get reference objects for tables and entities. The following code creates a CloudTableClient object and uses it to create a new\nCloudTable object, which represents a table\n// Retrieve storage account from connection-string.\nCloudStorageAccount storageAccount =\nCloudStorageAccount.parse(storageConnectionString);\n// Create the table client.\nCloudTableClient tableClient = storageAccount.createCloudTableClient();\n// Create the table if it doesn't exist.\nString tableName = \"people\";\nCloudTable cloudTable = tableClient.getTableReference(tableName); cloudTable.createIfNotExists();\n\nBox 2: No -\nNew records are inserted with TableOperation.insert. Old records are not updated.\nTo update old records TableOperation.insertOrReplace should be used instead.\n\nBox 3: No -\n\nBox 4: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/table-storage-how-to-use-java",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Yes -\nCreate a table.\nA CloudTableClient object lets you get reference objects for tables and entities. The following code creates a CloudTableClient object and uses it to create a new\nCloudTable object, which represents a table\n// Retrieve storage account from connection-string.\nCloudStorageAccount storageAccount =\nCloudStorageAccount.parse(storageConnectionString);\n// Create the table client.\nCloudTableClient tableClient = storageAccount.createCloudTableClient();\n// Create the table if it doesn't exist.\nString tableName = \"people\";\nCloudTable cloudTable = tableClient.getTableReference(tableName); cloudTable.createIfNotExists();\n\nBox 2: No -\nNew records are inserted with TableOperation.insert. Old records are not updated.\nTo update old records TableOperation.insertOrReplace should be used instead.\n\nBox 3: No -\n\nBox 4: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/table-storage-how-to-use-java",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #23 -- Topic 3",
        "questionIs": "You develop and deploy a web application to Azure App Service. The application accesses data stored in an Azure Storage account. The account contains several containers with several blobs with large amounts of data. You deploy all Azure resources to a single region.\nYou need to move the Azure Storage account to the new region. You must copy all data to the new region.\nWhat should you do first?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Export the Azure Storage account Azure Resource Manager template Most Voted",
            "B. Initiate a storage account failover",
            "C. Configure object replication for all blobs",
            "D. Use the AzCopy command line tool",
            "E. Create a new Azure Storage account in the current region",
            "F. Create a new subscription in the current region"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "To move a storage account, create a copy of your storage account in another region. Then, move your data to that account by using AzCopy, or another tool of your choice and finally, delete the resources in the source region.\nTo get started, export, and then modify a Resource Manager template.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-move?tabs=azure-portal",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #24 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing an application to collect the following telemetry data for delivery drivers: first name, last name, package count, item id, and current location coordinates. The app will store the data in Azure Cosmos DB.\nYou need to configure Azure Cosmos DB to query the data.\nWhich values should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0025400001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Core (SQL)\nCore(SQL) API stores data in document format. It offers the best end-to-end experience as we have full control over the interface, service, and the SDK client libraries. SQL API supports analytics and offers performance isolation between operational and analytical workloads.\n\nBox 2: item id -\nitem id is a unique identifier and is suitable for the partition key.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/choose-api\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Core (SQL)\nCore(SQL) API stores data in document format. It offers the best end-to-end experience as we have full control over the interface, service, and the SDK client libraries. SQL API supports analytics and offers performance isolation between operational and analytical workloads.\n\nBox 2: item id -\nitem id is a unique identifier and is suitable for the partition key.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/choose-api\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #25 -- Topic 3",
        "questionIs": "DRAG DROP -\nYou are implementing an Azure solution that uses Azure Cosmos DB and the latest Azure Cosmos DB SDK. You add a change feed processor to a new container instance.\nYou attempt to read a batch of 100 documents. The process fails when reading one of the documents. The solution must monitor the progress of the change feed processor instance on the new container as the change feed is read. You must prevent the change feed processor from retrying the entire batch when one document cannot be read.\nYou need to implement the change feed processor to read the documents.\nWhich features should you use? To answer, drag the appropriate features to the cored requirements. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each cored selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0025600001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Change feed estimator -\nYou can use the change feed estimator to monitor the progress of your change feed processor instances as they read the change feed or use the life cycle notifications to detect underlying failures.\n\nBox 2: Dead-letter queue -\nTo prevent your change feed processor from getting \"stuck\" continuously retrying the same batch of changes, you should add logic in your delegate code to write documents, upon exception, to a dead-letter queue. This design ensures that you can keep track of unprocessed changes while still being able to continue to process future changes. The dead-letter queue might be another Cosmos container. The exact data store does not matter, simply that the unprocessed changes are persisted.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/change-feed-processor",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Change feed estimator -\nYou can use the change feed estimator to monitor the progress of your change feed processor instances as they read the change feed or use the life cycle notifications to detect underlying failures.\n\nBox 2: Dead-letter queue -\nTo prevent your change feed processor from getting \"stuck\" continuously retrying the same batch of changes, you should add logic in your delegate code to write documents, upon exception, to a dead-letter queue. This design ensures that you can keep track of unprocessed changes while still being able to continue to process future changes. The dead-letter queue might be another Cosmos container. The exact data store does not matter, simply that the unprocessed changes are persisted.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/change-feed-processor",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #26 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing an application that uses a premium block blob storage account. The application will process a large volume of transactions daily. You enable\nBlob storage versioning.\nYou are optimizing costs by automating Azure Blob Storage access tiers. You apply the following policy rules to the storage account. (Line numbers are included for reference only.)\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0025800001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0025900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: No -\nWould be true if daysAfterModificationGreaterThan was used, but here daysAfterCreationGreaterThan\n\nBox 2: No -\nWould need to use the daysAfterLastAccessTimeGreaterThan predicate.\n\nBox 3: Yes -\n\nBox 4: Yes -\nWith the lifecycle management policy, you can:\nTransition blobs from cool to hot immediately when they are accessed, to optimize for performance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: No -\nWould be true if daysAfterModificationGreaterThan was used, but here daysAfterCreationGreaterThan\n\nBox 2: No -\nWould need to use the daysAfterLastAccessTimeGreaterThan predicate.\n\nBox 3: Yes -\n\nBox 4: Yes -\nWith the lifecycle management policy, you can:\nTransition blobs from cool to hot immediately when they are accessed, to optimize for performance.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #27 -- Topic 3",
        "questionIs": "An organization deploys Azure Cosmos DB.\nYou need to ensure that the index is updated as items are created, updated, or deleted.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Set the indexing mode to Lazy.",
            "B. Set the value of the automatic property of the indexing policy to False.",
            "C. Set the value of the EnableScanInQuery option to True.",
            "D. Set the indexing mode to Consistent. Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Azure Cosmos DB supports two indexing modes:\nConsistent: The index is updated synchronously as you create, update or delete items. This means that the consistency of your read queries will be the consistency configured for the account.\nNone: Indexing is disabled on the container.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/index-policy",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #28 -- Topic 3",
        "questionIs": "You are developing a .Net web application that stores data in Azure Cosmos DB. The application must use the Core API and allow millions of reads and writes.\nThe Azure Cosmos DB account has been created with multiple write regions enabled. The application has been deployed to the East US2 and Central US regions.\nYou need to update the application to support multi-region writes.\nWhat are two possible ways to achieve this goal? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Update the ConnectionPolicy class for the Cosmos client and populate the PreferredLocations property based on the geo-proximity of the application. Most Voted",
            "B. Update Azure Cosmos DB to use the Strong consistency level. Add indexed properties to the container to indicate region.",
            "C. Update the ConnectionPolicy class for the Cosmos client and set the UseMultipleWriteLocations property to true. Most Voted",
            "D. Create and deploy a custom conflict resolution policy.",
            "E. Update Azure Cosmos DB to use the Session consistency level. Send the SessionToken property value from the FeedResponse object of the write action to the end-user by using a cookie."
        ],
        "answersAre": [
            "C",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "C"
        ],
        "descriptionIs": "C: The UseMultipleWriteLocations of the ConnectionPolicy class gets or sets the flag to enable writes on any locations (regions) for geo-replicated database accounts in the Azure Cosmos DB service.\nNote: Once an account has been created with multiple write regions enabled, you must make two changes in your application to the ConnectionPolicy for the\nCosmos client to enable the multi-region writes in Azure Cosmos DB. Within the ConnectionPolicy, set UseMultipleWriteLocations to true and pass the name of the region where the application is deployed to ApplicationRegion. This will populate the PreferredLocations property based on the geo-proximity from location passed in. If a new region is later added to the account, the application does not have to be updated or redeployed, it will automatically detect the closer region and will auto-home on to it should a regional event occur.\nAzure core API application \" ConnectionPolicy class\" cosmos db multiple write regions enabled\nD: With multi-region writes, when multiple clients write to the same item, conflicts may occur. When a conflict occurs, you can resolve the conflict by using different conflict resolution policies.\nNote: Conflict resolution policy can only be specified at container creation time and cannot be modified after container creation.\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.documents.client.connectionpolicy https://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-multi-master https://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-manage-conflicts",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #29 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing a solution to store documents in Azure Blob storage. Customers upload documents to multiple containers. Documents consist of PDF, CSV,\nMicrosoft Office format and plain text files.\nThe solution must process millions of documents across hundreds of containers. The solution must meet the following requirements:\n\u2711 Documents must be categorized by a customer identifier as they are uploaded to the storage account.\n\u2711 Allow filtering by the customer identifier.\n\u2711 Allow searching of information contained within a document\n\u2711 Minimize costs.\nYou create and configure a standard general-purpose v2 storage account to support the solution.\nYou need to implement the solution.\nWhat should you implement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0026300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Blob index tags -\nAs datasets get larger, finding a specific object in a sea of data can be difficult. Blob index tags provide data management and discovery capabilities by using key- value index tag attributes. You can categorize and find objects within a single container or across all containers in your storage account. As data requirements change, objects can be dynamically categorized by updating their index tags. Objects can remain in-place with their current container organization.\n\nBox 2: Azure Cognitive Search -\nOnly index tags are automatically indexed and made searchable by the native Blob Storage service. Metadata can't be natively indexed or searched. You must use a separate service such as Azure Search.\nAzure Cognitive Search is the only cloud search service with built-in AI capabilities that enrich all types of information to help you identify and explore relevant content at scale. Use cognitive skills for vision, language, and speech, or use custom machine learning models to uncover insights from all types of content.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-manage-find-blobs https://azure.microsoft.com/en-us/services/search/",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Blob index tags -\nAs datasets get larger, finding a specific object in a sea of data can be difficult. Blob index tags provide data management and discovery capabilities by using key- value index tag attributes. You can categorize and find objects within a single container or across all containers in your storage account. As data requirements change, objects can be dynamically categorized by updating their index tags. Objects can remain in-place with their current container organization.\n\nBox 2: Azure Cognitive Search -\nOnly index tags are automatically indexed and made searchable by the native Blob Storage service. Metadata can't be natively indexed or searched. You must use a separate service such as Azure Search.\nAzure Cognitive Search is the only cloud search service with built-in AI capabilities that enrich all types of information to help you identify and explore relevant content at scale. Use cognitive skills for vision, language, and speech, or use custom machine learning models to uncover insights from all types of content.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-manage-find-blobs https://azure.microsoft.com/en-us/services/search/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #30 -- Topic 3",
        "questionIs": "HOTSPOT -\nYou are developing a web application by using the Azure SDK. The web application accesses data in a zone-redundant BlockBlobStorage storage account.\nThe application must determine whether the data has changed since the application last read the data. Update operations must use the latest data changes when writing data to the storage account.\nYou need to implement the update operations.\nWhich values should you use? To answer, select the appropriate option in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0026600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Last Modified -\nThe Last-Modified response HTTP header contains a date and time when the origin server believes the resource was last modified. It is used as a validator to determine if the resource is the same as the previously stored one. Less accurate than an ETag header, it is a fallback mechanism.\n\nBox 2: If-Modified-Since -\nConditional Header If-Modified-Since:\nA DateTime value. Specify this header to perform the operation only if the resource has been modified since the specified time.\nIncorrect:\n\nNot ETag/If-Match -\nConditional Header If-Match:\nAn ETag value. Specify this header to perform the operation only if the resource's ETag matches the value specified. For versions 2011-08-18 and newer, the\nETag can be specified in quotes.\nReference:\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Last-Modified https://docs.microsoft.com/en-us/rest/api/storageservices/specifying-conditional-headers-for-blob-service-operations",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Last Modified -\nThe Last-Modified response HTTP header contains a date and time when the origin server believes the resource was last modified. It is used as a validator to determine if the resource is the same as the previously stored one. Less accurate than an ETag header, it is a fallback mechanism.\n\nBox 2: If-Modified-Since -\nConditional Header If-Modified-Since:\nA DateTime value. Specify this header to perform the operation only if the resource has been modified since the specified time.\nIncorrect:\n\nNot ETag/If-Match -\nConditional Header If-Match:\nAn ETag value. Specify this header to perform the operation only if the resource's ETag matches the value specified. For versions 2011-08-18 and newer, the\nETag can be specified in quotes.\nReference:\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Last-Modified https://docs.microsoft.com/en-us/rest/api/storageservices/specifying-conditional-headers-for-blob-service-operations",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #31 -- Topic 3",
        "questionIs": "HOTSPOT -\nAn organization deploys a blob storage account. Users take multiple snapshots of the blob storage account over time.\nYou need to delete all snapshots of the blob storage account. You must not delete the blob storage account itself.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0026800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: DeleteSnapshotsOption -\nSample code in powershell:\n//dont forget to add the include snapshots :)\nawait batchClient.DeleteBlobsAsync(listofURIforBlobs,\nAzure.Storage.Blobs.Models.DeleteSnapshotsOption.IncludeSnapshots);\nSample code in .Net:\n// Create a batch with three deletes\nBlobBatchClient batchClient = service.GetBlobBatchClient();\nBlobBatch batch = batchClient.CreateBatch();\nbatch.DeleteBlob(foo.Uri, DeleteSnapshotsOption.IncludeSnapshots); batch.DeleteBlob(bar.Uri, DeleteSnapshotsOption.OnlySnapshots); batch.DeleteBlob(baz.Uri);\n// Submit the batch\nbatchClient.SubmitBatch(batch);\n\nBox 2: OnlySnapshots -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/overview/azure/storage.blobs.batch-readme https://stackoverflow.com/questions/39471212/programmatically-delete-azure-blob-storage-objects-in-bulks",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: DeleteSnapshotsOption -\nSample code in powershell:\n//dont forget to add the include snapshots :)\nawait batchClient.DeleteBlobsAsync(listofURIforBlobs,\nAzure.Storage.Blobs.Models.DeleteSnapshotsOption.IncludeSnapshots);\nSample code in .Net:\n// Create a batch with three deletes\nBlobBatchClient batchClient = service.GetBlobBatchClient();\nBlobBatch batch = batchClient.CreateBatch();\nbatch.DeleteBlob(foo.Uri, DeleteSnapshotsOption.IncludeSnapshots); batch.DeleteBlob(bar.Uri, DeleteSnapshotsOption.OnlySnapshots); batch.DeleteBlob(baz.Uri);\n// Submit the batch\nbatchClient.SubmitBatch(batch);\n\nBox 2: OnlySnapshots -\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/overview/azure/storage.blobs.batch-readme https://stackoverflow.com/questions/39471212/programmatically-delete-azure-blob-storage-objects-in-bulks",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #32 -- Topic 3",
        "questionIs": "HOTSPOT -\nAn organization deploys a blob storage account. Users take multiple snapshots of the blob storage account over time.\nYou need to delete all snapshots of the blob storage account. You must not delete the blob storage account itself.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0027000001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: delete_snapshots -\n# Delete only the snapshot (blob itself is retained)\nblob_client.delete_blob(delete_snapshots=\"only\")\n\nBox 2: only -\nReference:\nhttps://github.com/Azure/azure-sdk-for-python/blob/main/sdk/storage/azure-storage-blob/samples/blob_samples_common.py",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: delete_snapshots -\n# Delete only the snapshot (blob itself is retained)\nblob_client.delete_blob(delete_snapshots=\"only\")\n\nBox 2: only -\nReference:\nhttps://github.com/Azure/azure-sdk-for-python/blob/main/sdk/storage/azure-storage-blob/samples/blob_samples_common.py",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #33 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an application that monitors data added to an Azure Blob storage account.\n\nYou need to process each change made to the storage account.\n\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image389.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #34 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou develop an application that sells AI generated images based on user input. You recently started a marketing campaign that displays unique ads every second day.\n\nSales data is stored in Azure Cosmos DB with the date of each sale being stored in a property named \u2018whenFinished\u2019.\n\nThe marketing department requires a view that shows the number of sales for each unique ad.\nYou need to implement the query for the view.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image391.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #35 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou implement an Azure solution to include Azure Cosmos DB, the latest Azure Cosmos DB SDK, and the Core (SQL) API. You also implement a change feed processor on a new container instance by using the Azure Functions trigger for Azure Cosmos DB.\n\nA large batch of documents continues to fail when reading one of the documents in the batch. The same batch of documents is continuously retried by the triggered function and a new batch of documents must be read.\n\nYou need to implement the change feed processor to read the documents.\n\nWhich feature should you implement? To answer, select the appropriate features in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image393.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #36 -- Topic 3",
        "questionIs": "You are developing an application to store business-critical data in Azure Blob storage.\n\nThe application must meet the following requirements:\n\n\u2022 Data must not be modified or deleted for a user-specified interval.\n\u2022 Data must be protected from overwrites and deletes.\n\u2022 Data must be written once and allowed to be read many times.\n\nYou need to protect the data in the Azure Blob storage account.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure a time-based retention policy for the storage account. Most Voted",
            "B. Create an account shared-access signature (SAS).",
            "C. Enable the blob change feed for the storage account.",
            "D. Enable version-level immutability support for the storage account. Most Voted",
            "E. Enable point-in-time restore for containers in the storage account.",
            "F. Create a service shared-access signature (SAS)."
        ],
        "answersAre": [
            "A",
            "F"
        ],
        "mostVotedAre": [
            "A",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #37 -- Topic 3",
        "questionIs": "You are updating an application that stores data on Azure and uses Azure Cosmos DB for storage. The application stores data in multiple documents associated with a single username.\n\nThe application requires the ability to update multiple documents for a username in a single ACID operation.\n\nYou need to configure Azure Cosmos DB.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a collection sharded on username to store documents.",
            "B. Configure Azure Cosmos DB to use the Gremlin API.",
            "C. Create an unsharded collection to store documents. Most Voted",
            "D. Configure Azure Cosmos DB to use the MongoDB API. Most Voted"
        ],
        "answersAre": [
            "C",
            "D"
        ],
        "mostVotedAre": [
            "C",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #38 -- Topic 3",
        "questionIs": "You develop Azure solutions.\n\nYou must connect to a No-SQL globally-distributed database by using the .NET API.\n\nYou need to create an object to configure and execute requests in the database.\n\nWhich code segment should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. database_name = 'MyDatabase'\ndatabase = client.create_database_if_not_exists(id=database_name)",
            "B. client = CosmosClient(endpoint, key) Most Voted",
            "C. container_name = 'MyContainer'\ncontainer = database.create_container_if_not_exists(\nid=container_name, partition_key=PartitionKey(path=\"/lastName\"), offer_throughput=400 )"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #39 -- Topic 3",
        "questionIs": "You develop a web application that provides access to legal documents that are stored on Azure Blob Storage with version-level immutability policies. Documents are protected with both time-based policies and legal hold policies. All time-based retention policies have the AllowProtectedAppendWrites property enabled.\n\nYou have a requirement to prevent the user from attempting to perform operations that would fail only when a legal hold is in effect and when all other policies are expired.\n\nYou need to meet the requirement.\n\nWhich two operations should you prevent? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. adding data to documents",
            "B. deleting documents",
            "C. creating documents",
            "D. overwriting existing documents"
        ],
        "answersAre": [
            "B",
            "D"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #40 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou provisioned an Azure Cosmos DB for NoSQL account named account1 with the default consistency level.\n\nYou plan to configure the consistency level on a per request basis. The level needs to be set for consistent prefix for read and write operations to account1.\n\nYou need to identify the resulting consistency level for read and write operations.\n\nWhich levels should you configure? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image424.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #41 -- Topic 3",
        "questionIs": "DRAG DROP\n-\n\nYou are developing an application to store millions of images in Azure blob storage. The images are uploaded to an Azure blob storage container named companyimages contained in an Azure blob storage account named companymedia. The stored images are uploaded with multiple blob index tags across multiple blobs in the container.\n\nYou must find all blobs whose tags match a search expression in the container. The search expression must evaluate an index tag named status with a value of final.\n\nYou need to construct the GET method request URI.\n\nHow should you complete the URI? To answer, drag the appropriate parameters to the correct request URI targets. Each parameter may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image426.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #42 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou develop two Python scripts to process data.\n\nThe Python scripts must be deployed to two, separate Linux containers running in an Azure Container Instance container group. The containers must access external data by using the Server Message Block (SMB) protocol. Containers in the container group must run only once.\n\nYou need to configure the Azure Container Instance.\n\nWhich configuration value should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image447.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #43 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a static website hosted on Azure Blob Storage. You create a storage account and enable static website hosting.\n\nThe website must support the following requirements:\n\n\u2022 Custom domain name\n\u2022 Custom header values for all responses\n\u2022 Custom SSL certificate\n\nYou need to implement the static website.\n\nWhat should you configure? To answer, select the appropriate values in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image449.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #44 -- Topic 3",
        "questionIs": "You are developing an inventory tracking solution. The solution includes an Azure Function app containing multiple functions triggered by Azure Cosmos DB. You plan to deploy the solution to multiple Azure regions.\n\nThe solution must meet the following requirements:\n\n\u2022 Item results from Azure Cosmos DS must return the most recent committed version of an item.\n\u2022 Items written to Azure Cosmos DB must provide ordering guarantees.\n\nYou need to configure the consistency level for the Azure Cosmos DB deployments.\n\nWhich consistency level should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. consistent prefix",
            "B. eventual",
            "C. bounded staleness",
            "D. strong Most Voted",
            "E. session"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #45 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an application that runs in several customer Azure Kubernetes Service clusters. Within each cluster, a pod runs that collects performance data to be analyzed later. A large amount of data is collected so saving latency must be minimized.\n\nThe performance data must be stored so that pod restarts do not impact the stored data. Write latency should be minimized.\n\nYou need to configure blob storage.\n\nHow should you complete the YAML configuration? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image451.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #46 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\n\nCurrent environment\n-\n\n\nCorporate website\n-\n\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations\n-\n\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\n\u2022 Secure the website by using SSL.\n\u2022 Minimize costs for data storage and hosting.\n\u2022 Implement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\n\u2022 Distribute the website content globally for local use.\n\u2022 Implement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\n\u2022 The website must have 99.95 percent uptime.\n\n\nRetail store locations\n-\n\n\u2022 Azure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\n\u2022 Audit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\n\nDelivery services\n-\n\n\u2022 Store service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\n\u2022 Store delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\n\nInventory services\n-\n\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to\ninclude read-only access to the data.\n\n\nSecurity\n-\n\n\u2022 All Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\n\u2022 Authentication and authorization must use Azure AD and services must use managed identities where possible.\n\n\nIssues\n-\n\n\nRetail Store Locations\n-\n\n\u2022 You must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\n\u2022 Azure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\n\n\nYou need to implement the delivery service telemetry data.\n\nHow should you configure the solution? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image461.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #47 -- Topic 3",
        "questionIs": "You create and publish a new Azure App Service web app.\n\nUser authentication and authorization must use Azure Active Directory (Azure AD).\n\nYou need to configure authentication and authorization.\n\nWhat should you do first?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Add an identity provider. Most Voted",
            "B. Map an existing custom DNS name.",
            "C. Create and configure a new app setting.",
            "D. Add a private certificate.",
            "E. Create and configure a managed identity."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #48 -- Topic 3",
        "questionIs": "DRAG DROP\n-\n\nYou have an Azure Cosmos DB for NoSQL account.\n\nYou plan to develop two apps named App1 and App2 that will use the change feed functionality to track changes to containers. App1 will use the pull model and App2 will use the push model.\n\nYou need to choose the method to track the most recently processed change in App1 and App2.\n\nWhich component should you use? To answer, drag the appropriate components to the correct apps. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image465.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #49 -- Topic 3",
        "questionIs": "You have a Linux container-based console application that uploads image files from customer sites all over the world. A back-end system that runs on Azure virtual machines processes the images by using the Azure Blobs API.\n\nYou are not permitted to make changes to the application.\n\nSome customer sites only have phone-based internet connections.\n\nYou need to configure the console application to access the images.\n\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure BlobFuse Most Voted",
            "B. Azure Disks",
            "C. Azure Storage Network File System (NFS) 3.0 support",
            "D. Azure Files"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #50 -- Topic 3",
        "questionIs": "DRAG DROP\n-\n\nYou are developing several microservices named serviceA, serviceB, and serviceC. You deploy the microservices to a new Azure Container Apps environment.\n\nYou have the following requirements:\n\n\u2022 The microservices must persist data to storage.\n\u2022 serviceA must persist data only visible to the current container and the storage must be restricted to the amount of disk space available in the container.\n\u2022 serviceB must persist data for the lifetime of the replica and allow multiple containers in the replica to mount the same storage location.\n\u2022 serviceC must persist data beyond the lifetime of the replica while allowing multiple containers to access the storage and enable per object permissions.\n\nYou need to configure storage for each microservice.\n\nWhich storage type should you use? To answer, drag the appropriate storage types to the correct microservices. Each storage type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image484.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #51 -- Topic 3",
        "questionIs": "DRAG DROP\n-\n\nYou are developing a web service that will run on Azure virtual machines that use Azure Storage. You configure all virtual machines to use managed identities.\n\nYou have the following requirements:\n\n\u2022 Secret-based authentication mechanisms are not permitted for accessing an Azure Storage account.\n\u2022 Must use only Azure Instance Metadata Service endpoints.\n\nYou need to write code to retrieve an access token to access Azure Storage. To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image504.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #52 -- Topic 3",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an Azure Function app.\n\nThe Azure Function app must enable a WebHook to read an image from Azure Blob Storage and create a new Azure Cosmos DB document.\n\nYou need to implement the Azure Function app.\n\nWhich configuration should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image506.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #53 -- Topic 3",
        "questionIs": "You create an Azure Cosmos DB for NoSQL database.\n\nYou plan to use the Azure Cosmos DB .NET SDK v3 API for NoSQL to upload the following files:\n\n\n\nYou receive the following error message when uploading the files: \u201c413 Entity too large\u201d.\n\nYou need to determine which files you can upload to the Azure Cosmos DB for NoSQL database.\n\nWhich files can you upload?",
        "questionImages": [
            "https://img.examtopics.com/az-204/image508.png"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. File1, File2, File3, File4, and File5",
            "B. File1 and File2 only Most Voted",
            "C. File1, File2, and File3 only",
            "D. File1, File2, File3, and File4 only",
            "E. File1 only"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 4",
        "questionIs": "You are developing a Java application that uses Cassandra to store key and value data. You plan to use a new Azure Cosmos DB resource and the Cassandra\nAPI in the application. You create an Azure Active Directory (Azure AD) group named Cosmos DB Creators to enable provisioning of Azure Cosmos accounts, databases, and containers.\nThe Azure AD group must not be able to access the keys that are required to access the data.\nYou need to restrict access to the Azure AD group.\nWhich role-based access control should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. DocumentDB Accounts Contributor",
            "B. Cosmos Backup Operator",
            "C. Cosmos DB Operator Most Voted",
            "D. Cosmos DB Account Reader"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Azure Cosmos DB now provides a new RBAC role, Cosmos DB Operator. This new role lets you provision Azure Cosmos accounts, databases, and containers, but can't access the keys that are required to access the data. This role is intended for use in scenarios where the ability to grant access to Azure Active Directory service principals to manage deployment operations for Cosmos DB is needed, including the account, database, and containers.\nReference:\nhttps://azure.microsoft.com/en-us/updates/azure-cosmos-db-operator-role-for-role-based-access-control-rbac-is-now-available/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level.\nYou need to configure authorization.\nSolution: Configure the Azure Web App for the website to allow only authenticated requests and require Azure AD log on.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Instead in the Azure AD application's manifest, set value of the groupMembershipClaims option to All.\nReference:\nhttps://blogs.msdn.microsoft.com/waws/2017/03/13/azure-app-service-authentication-aad-groups/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level.\nYou need to configure authorization.\nSolution:\n\u2711 Create a new Azure AD application. In the application's manifest, set value of the groupMembershipClaims option to All.\n\u2711 In the website, use the value of the groups claim from the JWT for the user to determine permissions.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "To configure Manifest to include Group Claims in Auth Token\n1. Go to Azure Active Directory to configure the Manifest. Click on Azure Active Directory, and go to App registrations to find your application:\n2. Click on your application (or search for it if you have a lot of apps) and edit the Manifest by clicking on it.\n3. Locate the \u05d2\u20acgroupMembershipClaims\u05d2\u20ac setting. Set its value to either \u05d2\u20acSecurityGroup\u05d2\u20ac or \u05d2\u20acAll\u05d2\u20ac. To help you decide which:\n\u2711 \u05d2\u20acSecurityGroup\u05d2\u20ac - groups claim will contain the identifiers of all security groups of which the user is a member.\n\u2711 \u05d2\u20acAll\u05d2\u20ac - groups claim will contain the identifiers of all security groups and all distribution lists of which the user is a member\nNow your application will include group claims in your manifest and you can use this fact in your code.\nReference:\nhttps://blogs.msdn.microsoft.com/waws/2017/03/13/azure-app-service-authentication-aad-groups/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level.\nYou need to configure authorization.\nSolution:\n\u2711 Create a new Azure AD application. In the application's manifest, define application roles that match the required permission levels for the application.\n\u2711 Assign the appropriate Azure AD group to each role. In the website, use the value of the roles claim from the JWT for the user to determine permissions.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "To configure Manifest to include Group Claims in Auth Token\n1. Go to Azure Active Directory to configure the Manifest. Click on Azure Active Directory, and go to App registrations to find your application:\n2. Click on your application (or search for it if you have a lot of apps) and edit the Manifest by clicking on it.\n3. Locate the \u05d2\u20acgroupMembershipClaims\u05d2\u20ac setting. Set its value to either \u05d2\u20acSecurityGroup\u05d2\u20ac or \u05d2\u20acAll\u05d2\u20ac. To help you decide which:\n\u2711 \u05d2\u20acSecurityGroup\u05d2\u20ac - groups claim will contain the identifiers of all security groups of which the user is a member.\n\u2711 \u05d2\u20acAll\u05d2\u20ac - groups claim will contain the identifiers of all security groups and all distribution lists of which the user is a member\nNow your application will include group claims in your manifest and you can use this fact in your code.\nReference:\nhttps://blogs.msdn.microsoft.com/waws/2017/03/13/azure-app-service-authentication-aad-groups/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 4",
        "questionIs": "DRAG DROP -\nYou are developing an application to securely transfer data between on-premises file systems and Azure Blob storage. The application stores keys, secrets, and certificates in Azure Key Vault. The application uses the Azure Key Vault APIs.\nThe application must allow recovery of an accidental deletion of the key vault or key vault objects. Key vault objects must be retained for 90 days after deletion.\nYou need to protect the key vault and key vault objects.\nWhich Azure Key Vault feature should you use? To answer, drag the appropriate features to the correct actions. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0033400001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Soft delete -\nWhen soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion.\n\nBox 2: Purge protection -\nPurge protection is an optional Key Vault behavior and is not enabled by default. Purge protection can only be enabled once soft-delete is enabled.\nWhen purge protection is on, a vault or an object in the deleted state cannot be purged until the retention period has passed. Soft-deleted vaults and objects can still be recovered, ensuring that the retention policy will be followed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/soft-delete-overview",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Soft delete -\nWhen soft-delete is enabled, resources marked as deleted resources are retained for a specified period (90 days by default). The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion.\n\nBox 2: Purge protection -\nPurge protection is an optional Key Vault behavior and is not enabled by default. Purge protection can only be enabled once soft-delete is enabled.\nWhen purge protection is on, a vault or an object in the deleted state cannot be purged until the retention period has passed. Soft-deleted vaults and objects can still be recovered, ensuring that the retention policy will be followed.\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/soft-delete-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 4",
        "questionIs": "You provide an Azure API Management managed web service to clients. The back-end web service implements HTTP Strict Transport Security (HSTS).\nEvery request to the backend service must include a valid HTTP authorization header.\nYou need to configure the Azure API Management instance with an authentication policy.\nWhich two policies can you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Basic Authentication Most Voted Most Voted",
            "B. Digest Authentication",
            "C. Certificate Authentication Most Voted",
            "D. OAuth Client Credential Grant Most Voted"
        ],
        "answersAre": [
            "C",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "C",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #7 -- Topic 4",
        "questionIs": "DRAG DROP -\nYou are developing an ASP.NET Core website that can be used to manage photographs which are stored in Azure Blob Storage containers.\nUsers of the website authenticate by using their Azure Active Directory (Azure AD) credentials.\nYou implement role-based access control (RBAC) role permissions on the containers that store photographs. You assign users to RBAC roles.\nYou need to configure the website's Azure AD Application so that user's permissions can be used with the Azure Blob containers.\nHow should you configure the application? To answer, drag the appropriate setting to the correct location. Each setting can be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0033600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: user_impersonation -\n\nBox 2: delegated -\nExample:\n1. Select the API permissions section\n2. Click the Add a permission button and then:\nEnsure that the My APIs tab is selected\n3. In the list of APIs, select the API TodoListService-aspnetcore.\n4. In the Delegated permissions section, ensure that the right permissions are checked: user_impersonation.\n5. Select the Add permissions button.\n\nBox 3: delegated -\n\nExample -\n1. Select the API permissions section\n2. Click the Add a permission button and then,\nEnsure that the Microsoft APIs tab is selected\n3. In the Commonly used Microsoft APIs section, click on Microsoft Graph\n4. In the Delegated permissions section, ensure that the right permissions are checked: User.Read. Use the search box if necessary.\n5. Select the Add permissions button\nReference:\nhttps://docs.microsoft.com/en-us/samples/azure-samples/active-directory-dotnet-webapp-webapi-openidconnect-aspnetcore/calling-a-web-api-in-an-aspnet-core- web-application-using-azure-ad/",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: user_impersonation -\n\nBox 2: delegated -\nExample:\n1. Select the API permissions section\n2. Click the Add a permission button and then:\nEnsure that the My APIs tab is selected\n3. In the list of APIs, select the API TodoListService-aspnetcore.\n4. In the Delegated permissions section, ensure that the right permissions are checked: user_impersonation.\n5. Select the Add permissions button.\n\nBox 3: delegated -\n\nExample -\n1. Select the API permissions section\n2. Click the Add a permission button and then,\nEnsure that the Microsoft APIs tab is selected\n3. In the Commonly used Microsoft APIs section, click on Microsoft Graph\n4. In the Delegated permissions section, ensure that the right permissions are checked: User.Read. Use the search box if necessary.\n5. Select the Add permissions button\nReference:\nhttps://docs.microsoft.com/en-us/samples/azure-samples/active-directory-dotnet-webapp-webapi-openidconnect-aspnetcore/calling-a-web-api-in-an-aspnet-core- web-application-using-azure-ad/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #8 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are developing an ASP.NET Core app that includes feature flags which are managed by Azure App Configuration. You create an Azure App Configuration store named AppFeatureFlagStore that contains a feature flag named Export.\nYou need to update the app to meet the following requirements:\n\u2711 Use the Export feature in the app without requiring a restart of the app.\n\u2711 Validate users before users are allowed access to secure resources.\n\u2711 Permit users to access secure resources.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0033800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: UseAuthentication -\nNeed to validate users before users are allowed access to secure resources.\nUseAuthentication adds the AuthenticationMiddleware to the specified IApplicationBuilder, which enables authentication capabilities.\n\nBox 2: UseAuthorization -\nNeed to permit users to access secure resources.\nUseAuthorization adds the AuthorizationMiddleware to the specified IApplicationBuilder, which enables authorization capabilities.\n\nBox 3: UseStaticFiles -\nNeed to use the Export feature in the app without requiring a restart of the app.\nUseStaticFiles enables static file serving for the current request path\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.iapplicationbuilder?view=aspnetcore-5.0",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: UseAuthentication -\nNeed to validate users before users are allowed access to secure resources.\nUseAuthentication adds the AuthenticationMiddleware to the specified IApplicationBuilder, which enables authentication capabilities.\n\nBox 2: UseAuthorization -\nNeed to permit users to access secure resources.\nUseAuthorization adds the AuthorizationMiddleware to the specified IApplicationBuilder, which enables authorization capabilities.\n\nBox 3: UseStaticFiles -\nNeed to use the Export feature in the app without requiring a restart of the app.\nUseStaticFiles enables static file serving for the current request path\nReference:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.iapplicationbuilder?view=aspnetcore-5.0",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #9 -- Topic 4",
        "questionIs": "You have an application that includes an Azure Web app and several Azure Function apps. Application secrets including connection strings and certificates are stored in Azure Key Vault.\nSecrets must not be stored in the application or application runtime environment. Changes to Azure Active Directory (Azure AD) must be minimized.\nYou need to design the approach to loading application secrets.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a single user-assigned Managed Identity with permission to access Key Vault and configure each App Service to use that Managed Identity. Most Voted",
            "B. Create a single Azure AD Service Principal with permission to access Key Vault and use a client secret from within the App Services to access Key Vault.",
            "C. Create a system assigned Managed Identity in each App Service with permission to access Key Vault.",
            "D. Create an Azure AD Service Principal with Permissions to access Key Vault for each App Service and use a certificate from within the App Services to access Key Vault."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Use Key Vault references for App Service and Azure Functions.\nKey Vault references currently only support system-assigned managed identities. User-assigned identities cannot be used.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #10 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a medical records document management website. The website is used to store scanned copies of patient intake forms.\nIf the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised.\nYou need to store the intake forms according to the requirements.\nSolution:\n1. Create an Azure Key Vault key named skey.\n2. Encrypt the intake forms using the public key portion of skey.\n3. Store the encrypted data in Azure Blob storage.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #11 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a medical records document management website. The website is used to store scanned copies of patient intake forms.\nIf the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised.\nYou need to store the intake forms according to the requirements.\nSolution:\n1. Create an Azure Cosmos DB database with Storage Service Encryption enabled.\n2. Store the intake forms in the Azure Cosmos DB database.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Instead use an Azure Key vault and public key encryption. Store the encrypted from in Azure Storage Blob storage.",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #12 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a medical records document management website. The website is used to store scanned copies of patient intake forms.\nIf the stored intake forms are downloaded from storage by a third party, the contents of the forms must not be compromised.\nYou need to store the intake forms according to the requirements.\nSolution: Store the intake forms as Azure Key Vault secrets.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Instead use an Azure Key vault and public key encryption. Store the encrypted from in Azure Storage Blob storage.",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #13 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou plan to deploy a new application to a Linux virtual machine (VM) that is hosted in Azure.\nThe entire VM must be secured at rest by using industry-standard encryption technology to address organizational security and compliance requirements.\nYou need to configure Azure Disk Encryption for the VM.\nHow should you complete the Azure CLI commands? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0034400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: keyvault -\nCreate an Azure Key Vault with az keyvault create and enable the Key Vault for use with disk encryption. Specify a unique Key Vault name for keyvault_name as follows: keyvault_name=myvaultname$RANDOM az keyvault create \\\n--name $keyvault_name \\\n--resource-group $resourcegroup \\\n--location eastus \\\n--enabled-for-disk-encryption True\n\nBox 2: keyvault key -\nThe Azure platform needs to be granted access to request the cryptographic keys when the VM boots to decrypt the virtual disks. Create a cryptographic key in your Key Vault with az keyvault key create. The following example creates a key named myKey: az keyvault key create \\\n--vault-name $keyvault_name \\\n--name myKey \\\n--protection software\n\nBox 3: vm -\nCreate a VM with az vm create. Only certain marketplace images support disk encryption. The following example creates a VM named myVM using an Ubuntu\n16.04 LTS image:\naz vm create \\\n--resource-group $resourcegroup \\\n--name myVM \\\n--image Canonical:UbuntuServer:16.04-LTS:latest \\\n--admin-username azureuser \\\n--generate-ssh-keys \\\n\nBox 4: vm encryption -\nEncrypt your VM with az vm encryption enable:\naz vm encryption enable \\\n--resource-group $resourcegroup \\\n--name myVM \\\n--disk-encryption-keyvault $keyvault_name \\\n--key-encryption-key myKey \\\n--volume-type all\nNote: seems to an error in the question. Should have enable instead of create.\n\nBox 5: all -\nEncrypt both data and operating system.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/linux/disk-encryption-cli-quickstart",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: keyvault -\nCreate an Azure Key Vault with az keyvault create and enable the Key Vault for use with disk encryption. Specify a unique Key Vault name for keyvault_name as follows: keyvault_name=myvaultname$RANDOM az keyvault create \\\n--name $keyvault_name \\\n--resource-group $resourcegroup \\\n--location eastus \\\n--enabled-for-disk-encryption True\n\nBox 2: keyvault key -\nThe Azure platform needs to be granted access to request the cryptographic keys when the VM boots to decrypt the virtual disks. Create a cryptographic key in your Key Vault with az keyvault key create. The following example creates a key named myKey: az keyvault key create \\\n--vault-name $keyvault_name \\\n--name myKey \\\n--protection software\n\nBox 3: vm -\nCreate a VM with az vm create. Only certain marketplace images support disk encryption. The following example creates a VM named myVM using an Ubuntu\n16.04 LTS image:\naz vm create \\\n--resource-group $resourcegroup \\\n--name myVM \\\n--image Canonical:UbuntuServer:16.04-LTS:latest \\\n--admin-username azureuser \\\n--generate-ssh-keys \\\n\nBox 4: vm encryption -\nEncrypt your VM with az vm encryption enable:\naz vm encryption enable \\\n--resource-group $resourcegroup \\\n--name myVM \\\n--disk-encryption-keyvault $keyvault_name \\\n--key-encryption-key myKey \\\n--volume-type all\nNote: seems to an error in the question. Should have enable instead of create.\n\nBox 5: all -\nEncrypt both data and operating system.\nReference:\nhttps://docs.microsoft.com/en-us/azure/virtual-machines/linux/disk-encryption-cli-quickstart",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #14 -- Topic 4",
        "questionIs": "Your company is developing an Azure API hosted in Azure.\nYou need to implement authentication for the Azure API to access other Azure resources. You have the following requirements:\n\u2711 All API calls must be authenticated.\n\u2711 Callers to the API must not send credentials to the API.\nWhich authentication mechanism should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Basic",
            "B. Anonymous",
            "C. Managed identity Most Voted",
            "D. Client certificate"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Azure Active Directory Managed Service Identity (MSI) gives your code an automatically managed identity for authenticating to Azure services, so that you can keep credentials out of your code.\nNote: Use the authentication-managed-identity policy to authenticate with a backend service using the managed identity. This policy essentially uses the managed identity to obtain an access token from Azure Active Directory for accessing the specified resource. After successfully obtaining the token, the policy will set the value of the token in the Authorization header using the Bearer scheme.\nIncorrect Answers:\nA: Use the authentication-basic policy to authenticate with a backend service using Basic authentication. This policy effectively sets the HTTP Authorization header to the value corresponding to the credentials provided in the policy.\nB: Anonymous is no authentication at all.\nD: Your code needs credentials to authenticate to cloud services, but you want to limit the visibility of those credentials as much as possible. Ideally, they never appear on a developer's workstation or get checked-in to source control. Azure Key Vault can store credentials securely so they aren't in your code, but to retrieve them you need to authenticate to Azure Key Vault. To authenticate to Key Vault, you need a credential! A classic bootstrap problem.\nReference:\nhttps://azure.microsoft.com/en-us/blog/keep-credentials-out-of-code-introducing-azure-ad-managed-service-identity/ https://docs.microsoft.com/en-us/azure/api-management/api-management-authentication-policies",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #15 -- Topic 4",
        "questionIs": "DRAG DROP -\nYou are developing an application. You have an Azure user account that has access to two subscriptions.\nYou need to retrieve a storage account key secret from Azure Key Vault.\nIn which order should you arrange the PowerShell commands to develop the solution? To answer, move all commands from the list of commands to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0034900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Get-AzSubscription -\nIf you have multiple subscriptions, you might have to specify the one that was used to create your key vault. Enter the following to see the subscriptions for your account:\n\nGet-AzSubscription -\nStep 2: Set-AzContext -SubscriptionId\nTo specify the subscription that's associated with the key vault you'll be logging, enter:\nSet-AzContext -SubscriptionId <subscriptionID>\n\nStep 3: Get-AzStorageAccountKey -\nYou must get that storage account key.\nStep 4: $secretvalue = ConvertTo-SecureString <storageAccountKey> -AsPlainText -Force\nSet-AzKeyVaultSecret -VaultName <vaultName> -Name <secretName> -SecretValue $secretvalue\nAfter retrieving your secret (in this case, your storage account key), you must convert that key to a secure string, and then create a secret with that value in your key vault.\n\nStep 5: Get-AzKeyVaultSecret -\nNext, get the URI for the secret you created. You'll need this URI in a later step to call the key vault and retrieve your secret. Run the following PowerShell command and make note of the ID value, which is the secret's URI:\nGet-AzKeyVaultSecret \u05d2\u20ac\"VaultName <vaultName>\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/Azure/key-vault/key-vault-key-rotation-log-monitoring",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Get-AzSubscription -\nIf you have multiple subscriptions, you might have to specify the one that was used to create your key vault. Enter the following to see the subscriptions for your account:\n\nGet-AzSubscription -\nStep 2: Set-AzContext -SubscriptionId\nTo specify the subscription that's associated with the key vault you'll be logging, enter:\nSet-AzContext -SubscriptionId <subscriptionID>\n\nStep 3: Get-AzStorageAccountKey -\nYou must get that storage account key.\nStep 4: $secretvalue = ConvertTo-SecureString <storageAccountKey> -AsPlainText -Force\nSet-AzKeyVaultSecret -VaultName <vaultName> -Name <secretName> -SecretValue $secretvalue\nAfter retrieving your secret (in this case, your storage account key), you must convert that key to a secure string, and then create a secret with that value in your key vault.\n\nStep 5: Get-AzKeyVaultSecret -\nNext, get the URI for the secret you created. You'll need this URI in a later step to call the key vault and retrieve your secret. Run the following PowerShell command and make note of the ID value, which is the secret's URI:\nGet-AzKeyVaultSecret \u05d2\u20ac\"VaultName <vaultName>\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/Azure/key-vault/key-vault-key-rotation-log-monitoring",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #16 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop Azure solutions.\nYou must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager.\nYou need to obtain an Azure Resource Manager access token.\nSolution: Use an X.509 certificate to authenticate the VM with Azure Resource Manager.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Instead run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-arm",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #17 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop Azure solutions.\nYou must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager.\nYou need to obtain an Azure Resource Manager access token.\nSolution: Use the Reader role-based access control (RBAC) role to authenticate the VM with Azure Resource Manager.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Instead run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-arm",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #18 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are building a website that is used to review restaurants. The website will use an Azure CDN to improve performance and add functionality to requests.\nYou build and deploy a mobile app for Apple iPhones. Whenever a user accesses the website from an iPhone, the user must be redirected to the app store.\nYou need to implement an Azure CDN rule that ensures that iPhone users are redirected to the app store.\nHow should you complete the Azure Resource Manager template? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0035300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: iOS -\nAzure AD Conditional Access supports the following device platforms:\n\u2711 Android\n\u2711 iOS\n\u2711 Windows Phone\n\u2711 Windows\nmacOS\n\nBox 2: DeliveryRuleIsDeviceConditionParameters\nThe DeliveryRuleIsDeviceCondition defines the IsDevice condition for the delivery rule. parameters defines the parameters for the condition.\n\nBox 3: HTTP_USER_AGENT -\nIncorrect Answers:\n\u2711 The Pragma HTTP/1.0 general header is an implementation-specific header that may have various effects along the request-response chain. It is used for backwards compatibility with HTTP/1.0 caches.\n\u2711 \"X-Powered-By\" is a common non-standard HTTP response header (most headers prefixed with an 'X-' are non-standard).\nBox 4: DeliveryRuleRequestHeaderConditionParameters\nDeliveryRuleRequestHeaderCondition defines the RequestHeader condition for the delivery rule. parameters defines the parameters for the condition.\n\nBox 5: iOS -\nThe Require approved client app requirement only supports the iOS and Android for device platform condition.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/conditional-access/concept-conditional-access-conditions https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/concept-conditional-access-granthttps://www.examtopics.com/assets/media/exam-media/04273/0035400006.png",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: iOS -\nAzure AD Conditional Access supports the following device platforms:\n\u2711 Android\n\u2711 iOS\n\u2711 Windows Phone\n\u2711 Windows\nmacOS\n\nBox 2: DeliveryRuleIsDeviceConditionParameters\nThe DeliveryRuleIsDeviceCondition defines the IsDevice condition for the delivery rule. parameters defines the parameters for the condition.\n\nBox 3: HTTP_USER_AGENT -\nIncorrect Answers:\n\u2711 The Pragma HTTP/1.0 general header is an implementation-specific header that may have various effects along the request-response chain. It is used for backwards compatibility with HTTP/1.0 caches.\n\u2711 \"X-Powered-By\" is a common non-standard HTTP response header (most headers prefixed with an 'X-' are non-standard).\nBox 4: DeliveryRuleRequestHeaderConditionParameters\nDeliveryRuleRequestHeaderCondition defines the RequestHeader condition for the delivery rule. parameters defines the parameters for the condition.\n\nBox 5: iOS -\nThe Require approved client app requirement only supports the iOS and Android for device platform condition.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/conditional-access/concept-conditional-access-conditions https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/concept-conditional-access-grant",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0035400006.png"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #19 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing a website that will run as an Azure Web App. Users will authenticate by using their Azure Active Directory (Azure AD) credentials.\nYou plan to assign users one of the following permission levels for the website: admin, normal, and reader. A user's Azure AD group membership must be used to determine the permission level.\nYou need to configure authorization.\nSolution:\n\u2711 Configure and use Integrated Windows Authentication in the website.\n\u2711 In the website, query Microsoft Graph API to load the groups to which the user is a member.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Microsoft Graph is a RESTful web API that enables you to access Microsoft Cloud service resources.\nInstead in the Azure AD application's manifest, set value of the groupMembershipClaims option to All. In the website, use the value of the groups claim from the\nJWT for the user to determine permissions.\nReference:\nhttps://blogs.msdn.microsoft.com/waws/2017/03/13/azure-app-service-authentication-aad-groups/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #20 -- Topic 4",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou develop Azure solutions.\nYou must grant a virtual machine (VM) access to specific resource groups in Azure Resource Manager.\nYou need to obtain an Azure Resource Manager access token.\nSolution: Run the Invoke-RestMethod cmdlet to make a request to the local managed identity for Azure resources endpoint.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Get an access token using the VM's system-assigned managed identity and use it to call Azure Resource Manager\nYou will need to use PowerShell in this portion.\n1. In the portal, navigate to Virtual Machines and go to your Windows virtual machine and in the Overview, click Connect.\n2. Enter in your Username and Password for which you added when you created the Windows VM.\n3. Now that you have created a Remote Desktop Connection with the virtual machine, open PowerShell in the remote session.\n4. Using the Invoke-WebRequest cmdlet, make a request to the local managed identity for Azure resources endpoint to get an access token for Azure Resource\nManager.\nExample:\n$response = Invoke-WebRequest -Uri 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https:// management.azure.com/' -Method GET -Headers @{Metadata=\"true\"}\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-arm",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #21 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are building a website to access project data related to teams within your organization. The website does not allow anonymous access. Authentication is performed using an Azure Active Directory (Azure AD) app named internal.\nThe website has the following authentication requirements:\n\u2711 Azure AD users must be able to login to the website.\n\u2711 Personalization of the website must be based on membership in Active Directory groups.\nYou need to configure the application's manifest to meet the authentication requirements.\nHow should you configure the manifest? To answer, select the appropriate configuration in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0035800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: groupMembershipClaims -\nScenario: Personalization of the website must be based on membership in Active Directory groups.\nGroup claims can also be configured in the Optional Claims section of the Application Manifest.\nEnable group membership claims by changing the groupMembershipClaim\nThe valid values are:\n\"All\"\n\"SecurityGroup\"\n\"DistributionList\"\n\"DirectoryRole\"\n\nBox 2: oauth2Permissions -\nScenario: Azure AD users must be able to login to the website. oauth2Permissions specifies the collection of OAuth 2.0 permission scopes that the web API (resource) app exposes to client apps. These permission scopes may be granted to client apps during consent.\nIncorrect Answers:\noauth2AllowImplicitFlow. oauth2AllowImplicitFlow specifies whether this web app can request OAuth2.0 implicit flow access tokens. The default is false. This flag is used for browser-based apps, like Javascript single-page apps.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/hybrid/how-to-connect-fed-group-claims",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: groupMembershipClaims -\nScenario: Personalization of the website must be based on membership in Active Directory groups.\nGroup claims can also be configured in the Optional Claims section of the Application Manifest.\nEnable group membership claims by changing the groupMembershipClaim\nThe valid values are:\n\"All\"\n\"SecurityGroup\"\n\"DistributionList\"\n\"DirectoryRole\"\n\nBox 2: oauth2Permissions -\nScenario: Azure AD users must be able to login to the website. oauth2Permissions specifies the collection of OAuth 2.0 permission scopes that the web API (resource) app exposes to client apps. These permission scopes may be granted to client apps during consent.\nIncorrect Answers:\noauth2AllowImplicitFlow. oauth2AllowImplicitFlow specifies whether this web app can request OAuth2.0 implicit flow access tokens. The default is false. This flag is used for browser-based apps, like Javascript single-page apps.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/hybrid/how-to-connect-fed-group-claims",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #22 -- Topic 4",
        "questionIs": "You develop an app that allows users to upload photos and videos to Azure storage. The app uses a storage REST API call to upload the media to a blob storage account named Account1. You have blob storage containers named Container1 and Container2.\nUploading of videos occurs on an irregular basis.\nYou need to copy specific blobs from Container1 to Container2 when a new video is uploaded.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Copy blobs to Container2 by using the Put Blob operation of the Blob Service REST API",
            "B. Create an Event Grid topic that uses the Start-AzureStorageBlobCopy cmdlet Most Voted",
            "C. Use AzCopy with the Snapshot switch to copy blobs to Container2",
            "D. Download the blob to a virtual machine and then upload the blob to Container2"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "The Start-AzureStorageBlobCopy cmdlet starts to copy a blob.\n\nExample 1: Copy a named blob -\nC:\\PS>Start-AzureStorageBlobCopy -SrcBlob \"ContosoPlanning2015\" -DestContainer \"ContosoArchives\" -SrcContainer \"ContosoUploads\"\nThis command starts the copy operation of the blob named ContosoPlanning2015 from the container named ContosoUploads to the container named\nContosoArchives.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/azure.storage/start-azurestorageblobcopy?view=azurermps-6.13.0",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #23 -- Topic 4",
        "questionIs": "You are developing an ASP.NET Core website that uses Azure FrontDoor. The website is used to build custom weather data sets for researchers. Data sets are downloaded by users as Comma Separated Value (CSV) files. The data is refreshed every 10 hours.\nSpecific files must be purged from the FrontDoor cache based upon Response Header values.\nYou need to purge individual assets from the Front Door cache.\nWhich type of cache purge should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. single path Most Voted",
            "B. wildcard",
            "C. root domain"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "These formats are supported in the lists of paths to purge:\n\u2711 Single path purge: Purge individual assets by specifying the full path of the asset (without the protocol and domain), with the file extension, for example, /\n[1]\n\u2711 Wildcard purge: Asterisk (*) may be used as a wildcard. Purge all folders, subfolders, and files under an endpoint with /* in the path or purge all subfolders and files under a specific folder by specifying the folder followed by /*, for example, /pictures/*.\n\u2711 Root domain purge: Purge the root of the endpoint with \"/\" in the path.\nReference:\nhttps://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #24 -- Topic 4",
        "questionIs": "Your company is developing an Azure API.\nYou need to implement authentication for the Azure API. You have the following requirements:\nAll API calls must be secure.\n\n\u2711 Callers to the API must not send credentials to the API.\nWhich authentication mechanism should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0036100004.png"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Basic",
            "B. Anonymous",
            "C. Managed identity Most Voted",
            "D. Client certificate"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Use the authentication-managed-identity policy to authenticate with a backend service using the managed identity of the API Management service. This policy essentially uses the managed identity to obtain an access token from Azure Active Directory for accessing the specified resource. After successfully obtaining the token, the policy will set the value of the token in the Authorization header using the Bearer scheme.\nReference:\nhttps://docs.microsoft.com/bs-cyrl-ba/azure/api-management/api-management-authentication-policies",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #25 -- Topic 4",
        "questionIs": "You are a developer for a SaaS company that offers many web services.\nAll web services for the company must meet the following requirements:\n\u2711 Use API Management to access the services\n\u2711 Use OpenID Connect for authentication\n\u2711 Prevent anonymous usage\nA recent security audit found that several web services can be called without any authentication.\nWhich API Management policy should you implement?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. jsonp",
            "B. authentication-certificate",
            "C. check-header",
            "D. validate-jwt Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Add the validate-jwt policy to validate the OAuth token for every incoming request.\nIncorrect Answers:\nA: The jsonp policy adds JSON with padding (JSONP) support to an operation or an API to allow cross-domain calls from JavaScript browser-based clients.\nJSONP is a method used in JavaScript programs to request data from a server in a different domain. JSONP bypasses the limitation enforced by most web browsers where access to web pages must be in the same domain.\nJSONP - Adds JSON with padding (JSONP) support to an operation or an API to allow cross-domain calls from JavaScript browser-based clients.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-protect-backend-with-aad",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #26 -- Topic 4",
        "questionIs": "DRAG DROP -\nContoso, Ltd. provides an API to customers by using Azure API Management (APIM). The API authorizes users with a JWT token.\nYou must implement response caching for the APIM gateway. The caching mechanism must detect the user ID of the client that accesses data for a given location and cache the response for that user ID.\nYou need to add the following policies to the policies file:\n\u2711 a set-variable policy to store the detected user identity\n\u2711 a cache-lookup-value policy\n\u2711 a cache-store-value policy\n\u2711 a find-and-replace policy to update the response body with the user profile information\nTo which policy section should you add the policies? To answer, drag the appropriate sections to the correct policies. Each section may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0036400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Inbound.\nA set-variable policy to store the detected user identity.\nExample:\n<policies>\n<inbound>\n<!-- How you determine user identity is application dependent -->\n<set-variable\nname=\"enduserid\"\nvalue=\"@(context.Request.Headers.GetValueOrDefault(\"Authorization\",\"\").Split(' ')[1].AsJwt()?.Subject)\" />\n\nBox 2: Inbound -\n\nA cache-lookup-value policy -\nExample:\n<inbound>\n<base />\n<cache-lookup vary-by-developer=\"true | false\" vary-by-developer-groups=\"true | false\" downstream-caching-type=\"none | private | public\" must- revalidate=\"true | false\">\n<vary-by-query-parameter>parameter name</vary-by-query-parameter> <!-- optional, can repeated several times -->\n</cache-lookup>\n</inbound>\n\nBox 3: Outbound -\nA cache-store-value policy.\nExample:\n<outbound>\n<base />\n<cache-store duration=\"3600\" />\n</outbound>\n\nBox 4: Outbound -\nA find-and-replace policy to update the response body with the user profile information.\nExample:\n<outbound>\n<!-- Update response body with user profile-->\n<find-and-replace\nfrom='\"$userprofile$\"'\nto=\"@((string)context.Variables[\"userprofile\"])\" />\n<base />\n</outbound>\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-policies https://docs.microsoft.com/en-us/azure/api-management/api-management-sample-cache-by-key",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Inbound.\nA set-variable policy to store the detected user identity.\nExample:\n<policies>\n<inbound>\n<!-- How you determine user identity is application dependent -->\n<set-variable\nname=\"enduserid\"\nvalue=\"@(context.Request.Headers.GetValueOrDefault(\"Authorization\",\"\").Split(' ')[1].AsJwt()?.Subject)\" />\n\nBox 2: Inbound -\n\nA cache-lookup-value policy -\nExample:\n<inbound>\n<base />\n<cache-lookup vary-by-developer=\"true | false\" vary-by-developer-groups=\"true | false\" downstream-caching-type=\"none | private | public\" must- revalidate=\"true | false\">\n<vary-by-query-parameter>parameter name</vary-by-query-parameter> <!-- optional, can repeated several times -->\n</cache-lookup>\n</inbound>\n\nBox 3: Outbound -\nA cache-store-value policy.\nExample:\n<outbound>\n<base />\n<cache-store duration=\"3600\" />\n</outbound>\n\nBox 4: Outbound -\nA find-and-replace policy to update the response body with the user profile information.\nExample:\n<outbound>\n<!-- Update response body with user profile-->\n<find-and-replace\nfrom='\"$userprofile$\"'\nto=\"@((string)context.Variables[\"userprofile\"])\" />\n<base />\n</outbound>\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-policies https://docs.microsoft.com/en-us/azure/api-management/api-management-sample-cache-by-key",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #27 -- Topic 4",
        "questionIs": "DRAG DROP -\nYou are developing an Azure solution.\nYou need to develop code to access a secret stored in Azure Key Vault.\nHow should you complete the code segment? To answer, drag the appropriate code segments to the correct location. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0036700001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: SecretClient -\n\nBox 2: DefaultAzureCredential -\nIn below example, the name of your key vault is expanded to the key vault URI, in the format \"https://<your-key-vault-name>.vault.azure.net\". This example is using 'DefaultAzureCredential()' class from Azure Identity Library, which allows to use the same code across different environments with different options to provide identity. string keyVaultName = Environment.GetEnvironmentVariable(\"KEY_VAULT_NAME\"); var kvUri = \"https://\" + keyVaultName + \".vault.azure.net\"; var client = new SecretClient(new Uri(kvUri), new DefaultAzureCredential());\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-net",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: SecretClient -\n\nBox 2: DefaultAzureCredential -\nIn below example, the name of your key vault is expanded to the key vault URI, in the format \"https://<your-key-vault-name>.vault.azure.net\". This example is using 'DefaultAzureCredential()' class from Azure Identity Library, which allows to use the same code across different environments with different options to provide identity. string keyVaultName = Environment.GetEnvironmentVariable(\"KEY_VAULT_NAME\"); var kvUri = \"https://\" + keyVaultName + \".vault.azure.net\"; var client = new SecretClient(new Uri(kvUri), new DefaultAzureCredential());\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/secrets/quick-create-net",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #28 -- Topic 4",
        "questionIs": "You are developing an Azure App Service REST API.\nThe API must be called by an Azure App Service web app. The API must retrieve and update user profile information stored in Azure Active Directory (Azure AD).\nYou need to configure the API to make the updates.\nWhich two tools should you use? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Microsoft Graph API Most Voted",
            "B. Microsoft Authentication Library (MSAL) Most Voted",
            "C. Azure API Management",
            "D. Microsoft Azure Security Center",
            "E. Microsoft Azure Key Vault SDK"
        ],
        "answersAre": [
            "A",
            "C"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "A: You can use the Azure AD REST APIs in Microsoft Graph to create unique workflows between Azure AD resources and third-party services.\nEnterprise developers use Microsoft Graph to integrate Azure AD identity management and other services to automate administrative workflows, such as employee onboarding (and termination), profile maintenance, license deployment, and more.\nC: API Management (APIM) is a way to create consistent and modern API gateways for existing back-end services.\nAPI Management helps organizations publish APIs to external, partner, and internal developers to unlock the potential of their data and services.\nReference:\nhttps://docs.microsoft.com/en-us/graph/azuread-identity-access-management-concept-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #29 -- Topic 4",
        "questionIs": "You develop a REST API. You implement a user delegation SAS token to communicate with Azure Blob storage.\nThe token is compromised.\nYou need to revoke the token.\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Revoke the delegation key. Most Voted",
            "B. Delete the stored access policy.",
            "C. Regenerate the account key.",
            "D. Remove the role assignment for the security principle. Most Voted"
        ],
        "answersAre": [
            "A",
            "B"
        ],
        "mostVotedAre": [
            "A",
            "D"
        ],
        "descriptionIs": "A: Revoke a user delegation SAS -\nTo revoke a user delegation SAS from the Azure CLI, call the az storage account revoke-delegation-keys command. This command revokes all of the user delegation keys associated with the specified storage account. Any shared access signatures associated with those keys are invalidated.\nB: To revoke a stored access policy, you can either delete it, or rename it by changing the signed identifier. Changing the signed identifier breaks the associations between any existing signatures and the stored access policy. Deleting or renaming the stored access policy immediately effects all of the shared access signatures associated with it.\nReference:\nhttps://github.com/MicrosoftDocs/azure-docs/blob/master/articles/storage/blobs/storage-blob-user-delegation-sas-create-cli.md https://docs.microsoft.com/en-us/rest/api/storageservices/define-stored-access-policy#modifying-or-revoking-a-stored-access-policy",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #30 -- Topic 4",
        "questionIs": "DRAG DROP -\nYou are developing an Azure-hosted application that must use an on-premises hardware security module (HSM) key.\nThe key must be transferred to your existing Azure Key Vault by using the Bring Your Own Key (BYOK) process.\nYou need to securely transfer the key to Azure Key Vault.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0037000001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "To perform a key transfer, a user performs following steps:\n\u2711 Generate KEK.\n\u2711 Retrieve the public key of the KEK.\n\u2711 Using HSM vendor provided BYOK tool - Import the KEK into the target HSM and exports the Target Key protected by the KEK.\n\u2711 Import the protected Target Key to Azure Key Vault.\nStep 1: Generate a Key Exchange Key (KEK).\nStep 2: Retrieve the Key Exchange Key (KEK) public key.\nStep 3: Generate a key transfer blob file by using the HSM vendor-provided tool.\nGenerate key transfer blob using HSM vendor provided BYOK tool\nStep 4: Run the az keyvault key import command\nUpload key transfer blob to import HSM-key.\nCustomer will transfer the Key Transfer Blob (\".byok\" file) to an online workstation and then run a az keyvault key import command to import this blob as a new\nHSM-backed key into Key Vault.\nTo import an RSA key use this command:\naz keyvault key import\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/keys/byok-specification",
        "mostVotedAre": [],
        "descriptionIs": "To perform a key transfer, a user performs following steps:\n\u2711 Generate KEK.\n\u2711 Retrieve the public key of the KEK.\n\u2711 Using HSM vendor provided BYOK tool - Import the KEK into the target HSM and exports the Target Key protected by the KEK.\n\u2711 Import the protected Target Key to Azure Key Vault.\nStep 1: Generate a Key Exchange Key (KEK).\nStep 2: Retrieve the Key Exchange Key (KEK) public key.\nStep 3: Generate a key transfer blob file by using the HSM vendor-provided tool.\nGenerate key transfer blob using HSM vendor provided BYOK tool\nStep 4: Run the az keyvault key import command\nUpload key transfer blob to import HSM-key.\nCustomer will transfer the Key Transfer Blob (\".byok\" file) to an online workstation and then run a az keyvault key import command to import this blob as a new\nHSM-backed key into Key Vault.\nTo import an RSA key use this command:\naz keyvault key import\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/keys/byok-specification",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #31 -- Topic 4",
        "questionIs": "You develop and deploy an Azure Logic app that calls an Azure Function app. The Azure Function app includes an OpenAPI (Swagger) definition and uses an\nAzure Blob storage account. All resources are secured by using Azure Active Directory (Azure AD).\nThe Azure Logic app must securely access the Azure Blob storage account. Azure AD resources must remain if the Azure Logic app is deleted.\nYou need to secure the Azure Logic app.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a user-assigned managed identity and assign role-based access controls. Most Voted",
            "B. Create an Azure AD custom role and assign the role to the Azure Blob storage account.",
            "C. Create an Azure Key Vault and issue a client certificate.",
            "D. Create a system-assigned managed identity and issue a client certificate.",
            "E. Create an Azure AD custom role and assign role-based access controls."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "To give a managed identity access to an Azure resource, you need to add a role to the target resource for that identity.\nNote: To easily authenticate access to other resources that are protected by Azure Active Directory (Azure AD) without having to sign in and provide credentials or secrets, your logic app can use a managed identity (formerly known as Managed Service Identity or MSI). Azure manages this identity for you and helps secure your credentials because you don't have to provide or rotate secrets.\nIf you set up your logic app to use the system-assigned identity or a manually created, user-assigned identity, the function in your logic app can also use that same identity for authentication.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity https://docs.microsoft.com/en-us/azure/api-management/api-management-howto-mutual-certificates-for-clients",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #32 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are developing an application that uses a premium block blob storage account. You are optimizing costs by automating Azure Blob Storage access tiers.\nYou apply the following policy rules to the storage account. You must determine the implications of applying the rules to the data. (Line numbers are included for reference only.)\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0037300001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0037400001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Yes -\n\n\nBox 2: Yes -\n\n\nBox 3: Yes -\n\nBox 4: Yes -https://www.examtopics.com/assets/media/exam-media/04273/0037500001.png, https://www.examtopics.com/assets/media/exam-media/04273/0037500002.png, https://www.examtopics.com/assets/media/exam-media/04273/0037600001.png",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Yes -\n\n\nBox 2: Yes -\n\n\nBox 3: Yes -\n\nBox 4: Yes -",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0037500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0037500002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0037600001.png"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #33 -- Topic 4",
        "questionIs": "You are developing a solution that will use a multi-partitioned Azure Cosmos DB database. You plan to use the latest Azure Cosmos DB SDK for development.\nThe solution must meet the following requirements:\n\u2711 Send insert and update operations to an Azure Blob storage account.\n\u2711 Process changes to all partitions immediately.\n\u2711 Allow parallelization of change processing.\nYou need to process the Azure Cosmos DB operations.\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create an Azure App Service API and implement the change feed estimator of the SDK. Scale the API by using multiple Azure App Service instances.",
            "B. Create a background job in an Azure Kubernetes Service and implement the change feed feature of the SDK.",
            "C. Create an Azure Function to use a trigger for Azure Cosmos DB. Configure the trigger to connect to the container. Most Voted",
            "D. Create an Azure Function that uses a FeedIterator object that processes the change feed by using the pull model on the container. Use a FeedRange object to parallelize the processing of the change feed across multiple functions. Most Voted"
        ],
        "answersAre": [
            "A",
            "C"
        ],
        "mostVotedAre": [
            "C",
            "D"
        ],
        "descriptionIs": "Azure Functions is the simplest option if you are just getting started using the change feed. Due to its simplicity, it is also the recommended option for most change feed use cases. When you create an Azure Functions trigger for Azure Cosmos DB, you select the container to connect, and the Azure Function gets triggered whenever there is a change in the container. Because Azure Functions uses the change feed processor behind the scenes, it automatically parallelizes change processing across your container's partitions.\nNote: You can work with change feed using the following options:\n\u2711 Using change feed with Azure Functions\n\u2711 Using change feed with change feed processor\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/read-change-feed",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #34 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou have an Azure Web app that uses Cosmos DB as a data store. You create a CosmosDB container by running the following PowerShell script:\n$resourceGroupName = \"testResourceGroup\"\n$accountName = \"testCosmosAccount\"\n$databaseName = \"testDatabase\"\n$containerName = \"testContainer\"\n$partitionKeyPath = \"/EmployeeId\"\n$autoscaleMaxThroughput = 5000\n\nNew-AzCosmosDBSqlContainer -\n-ResourceGroupName $resourceGroupName\n-AccountName $accountName\n-DatabaseName $databaseName\n-Name $containerName\n-PartitionKeyKind Hash\n-PartitionKeyPath $partitionKeyPath\n-AutoscaleMaxThroughput $autoscaleMaxThroughput\nYou create the following queries that target the container:\nSELECT * FROM c WHERE c.EmployeeId > '12345'\nSELECT * FROM c WHERE c.UserID = '12345'\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0037800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: No -\nYou set the highest, or maximum RU/s Tmax you don't want the system to exceed. The system automatically scales the throughput T such that 0.1* Tmax <= T <=\nTmax.\nIn this example we have autoscaleMaxThroughput = 5000, so the minimum throughput for the container is 500 R/Us.\n\nBox 2: No -\nFirst query: SELECT * FROM c WHERE c.EmployeeId > '12345'\nHere's a query that has a range filter on the partition key and won't be scoped to a single physical partition. In order to be an in-partition query, the query must have an equality filter that includes the partition key:\nSELECT * FROM c WHERE c.DeviceId > 'XMS-0001'\n\nBox 3: Yes -\nExample of In-partition query:\nConsider the below query with an equality filter on DeviceId. If we run this query on a container partitioned on DeviceId, this query will filter to a single physical partition.\nSELECT * FROM c WHERE c.DeviceId = 'XMS-0001'\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-choose-offer https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-query-container",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: No -\nYou set the highest, or maximum RU/s Tmax you don't want the system to exceed. The system automatically scales the throughput T such that 0.1* Tmax <= T <=\nTmax.\nIn this example we have autoscaleMaxThroughput = 5000, so the minimum throughput for the container is 500 R/Us.\n\nBox 2: No -\nFirst query: SELECT * FROM c WHERE c.EmployeeId > '12345'\nHere's a query that has a range filter on the partition key and won't be scoped to a single physical partition. In order to be an in-partition query, the query must have an equality filter that includes the partition key:\nSELECT * FROM c WHERE c.DeviceId > 'XMS-0001'\n\nBox 3: Yes -\nExample of In-partition query:\nConsider the below query with an equality filter on DeviceId. If we run this query on a container partitioned on DeviceId, this query will filter to a single physical partition.\nSELECT * FROM c WHERE c.DeviceId = 'XMS-0001'\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-choose-offer https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-query-container",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #35 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are developing a web application that makes calls to the Microsoft Graph API. You register the application in the Azure portal and upload a valid X509 certificate.\nYou create an appsettings.json file containing the certificate name, client identifier for the application, and the tenant identifier of the Azure Active Directory (Azure\nAD). You create a method named ReadCertificate to return the X509 certificate by name.\nYou need to implement code that acquires a token by using the certificate.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0038000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: ConfidentialClientApplicationBuilder\nHere's the code to instantiate the confidential client application with a client secret: app = ConfidentialClientApplicationBuilder.Create(config.ClientId)\n.WithClientSecret(config.ClientSecret)\n.WithAuthority(new Uri(config.Authority))\n.Build();\n\nBox 2: scopes -\nAfter you've constructed a confidential client application, you can acquire a token for the app by calling AcquireTokenForClient, passing the scope, and optionally forcing a refresh of the token.\nSample code: result = await app.AcquireTokenForClient(scopes)\n.ExecuteAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-daemon-app-configuration https://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-daemon-acquire-token",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: ConfidentialClientApplicationBuilder\nHere's the code to instantiate the confidential client application with a client secret: app = ConfidentialClientApplicationBuilder.Create(config.ClientId)\n.WithClientSecret(config.ClientSecret)\n.WithAuthority(new Uri(config.Authority))\n.Build();\n\nBox 2: scopes -\nAfter you've constructed a confidential client application, you can acquire a token for the app by calling AcquireTokenForClient, passing the scope, and optionally forcing a refresh of the token.\nSample code: result = await app.AcquireTokenForClient(scopes)\n.ExecuteAsync();\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-daemon-app-configuration https://docs.microsoft.com/en-us/azure/active-directory/develop/scenario-daemon-acquire-token",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #36 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou develop a containerized application. You plan to deploy the application to a new Azure Container instance by using a third-party continuous integration and continuous delivery (CI/CD) utility.\nThe deployment must be unattended and include all application assets. The third-party utility must only be able to push and pull images from the registry. The authentication must be managed by Azure Active Directory (Azure AD). The solution must use the principle of least privilege.\nYou need to ensure that the third-party utility can access the registry.\nWhich authentication options should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0038200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Service principal -\nApplications and container orchestrators can perform unattended, or \"headless,\" authentication by using an Azure Active Directory (Azure AD) service principal.\nIncorrect Answers:\n\u2711 Individual AD identity does not support unattended push/pull\n\u2711 Repository-scoped access token is not integrated with AD identity\n\u2711 Managed identity for Azure resources is used to authenticate to an Azure container registry from another Azure resource.\n\nBox 2: AcrPush -\nAcrPush provides pull/push permissions only and meets the principle of least privilege.\nIncorrect Answers:\nAcrPull only allows pull permissions it does not allow push permissions.\n\n\u2711 Owner and Contributor allow pull/push permissions but does not meet the principle of least privilege.\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli https://docs.microsoft.com/en-us/azure/container-registry/container-registry-roles?tabs=azure-clihttps://www.examtopics.com/assets/media/exam-media/04273/0038300005.png",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Service principal -\nApplications and container orchestrators can perform unattended, or \"headless,\" authentication by using an Azure Active Directory (Azure AD) service principal.\nIncorrect Answers:\n\u2711 Individual AD identity does not support unattended push/pull\n\u2711 Repository-scoped access token is not integrated with AD identity\n\u2711 Managed identity for Azure resources is used to authenticate to an Azure container registry from another Azure resource.\n\nBox 2: AcrPush -\nAcrPush provides pull/push permissions only and meets the principle of least privilege.\nIncorrect Answers:\nAcrPull only allows pull permissions it does not allow push permissions.\n\n\u2711 Owner and Contributor allow pull/push permissions but does not meet the principle of least privilege.\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli https://docs.microsoft.com/en-us/azure/container-registry/container-registry-roles?tabs=azure-cli",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0038300005.png"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #37 -- Topic 4",
        "questionIs": "You deploy an Azure App Service web app. You create an app registration for the app in Azure Active Directory (Azure AD) and Twitter.\nThe app must authenticate users and must use SSL for all communications. The app must use Twitter as the identity provider.\nYou need to validate the Azure AD request in the app code.\nWhat should you validate?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. ID token header",
            "B. ID token signature Most Voted",
            "C. HTTP response code",
            "D. Tenant ID"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app?tabs=dotnet",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #38 -- Topic 4",
        "questionIs": "A development team is creating a new REST API. The API will store data in Azure Blob storage. You plan to deploy the API to Azure App Service.\nDevelopers must access the Azure Blob storage account to develop the API for the next two months. The Azure Blob storage account must not be accessible by the developers after the two-month time period.\nYou need to grant developers access to the Azure Blob storage account.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Generate a shared access signature (SAS) for the Azure Blob storage account and provide the SAS to all developers. Most Voted",
            "B. Create and apply a new lifecycle management policy to include a last accessed date value. Apply the policy to the Azure Blob storage account.",
            "C. Provide all developers with the access key for the Azure Blob storage account. Update the API to include the Coordinated Universal Time (UTC) timestamp for the request header.",
            "D. Grant all developers access to the Azure Blob storage account by assigning role-based access control (RBAC) roles."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #39 -- Topic 4",
        "questionIs": "DRAG DROP -\nYou develop a web application.\nYou need to register the application with an active Azure Active Directory (Azure AD) tenant.\nWhich three actions should you perform in sequence? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0038600001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Register a new application using the Azure portal\n1. Sign in to the Azure portal using either a work or school account or a personal Microsoft account.\n2. If your account gives you access to more than one tenant, select your account in the upper right corner. Set your portal session to the Azure AD tenant that you want.\n3. Search for and select Azure Active Directory. Under Manage, select App registrations.\n4. Select New registration. (Step 1)\n5. In Register an application, enter a meaningful application name to display to users.\n6. Specify who can use the application. Select the Azure AD instance. (Step 2)\n7. Under Redirect URI (optional), select the type of app you're building: Web or Public client (mobile & desktop). Then enter the redirect URI, or reply URL, for your application. (Step 3)\n8. When finished, select Register.",
        "mostVotedAre": [],
        "descriptionIs": "Register a new application using the Azure portal\n1. Sign in to the Azure portal using either a work or school account or a personal Microsoft account.\n2. If your account gives you access to more than one tenant, select your account in the upper right corner. Set your portal session to the Azure AD tenant that you want.\n3. Search for and select Azure Active Directory. Under Manage, select App registrations.\n4. Select New registration. (Step 1)\n5. In Register an application, enter a meaningful application name to display to users.\n6. Specify who can use the application. Select the Azure AD instance. (Step 2)\n7. Under Redirect URI (optional), select the type of app you're building: Web or Public client (mobile & desktop). Then enter the redirect URI, or reply URL, for your application. (Step 3)\n8. When finished, select Register.",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #40 -- Topic 4",
        "questionIs": "You have a new Azure subscription. You are developing an internal website for employees to view sensitive data. The website uses Azure Active Directory (Azure\nAD) for authentication.\nYou need to implement multifactor authentication for the website.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure the website to use Azure AD B2C.",
            "B. In Azure AD, create a new conditional access policy. Most Voted",
            "C. Upgrade to Azure AD Premium. Most Voted",
            "D. In Azure AD, enable application proxy.",
            "E. In Azure AD conditional access, enable the baseline policy."
        ],
        "answersAre": [
            "B",
            "C"
        ],
        "mostVotedAre": [
            "B",
            "C"
        ],
        "descriptionIs": "B: MFA Enabled by conditional access policy. It is the most flexible means to enable two-step verification for your users. Enabling using conditional access policy only works for Azure MFA in the cloud and is a premium feature of Azure AD.\nC: Multi-Factor Authentication comes as part of the following offerings:\n\u2711 Azure Active Directory Premium licenses - Full featured use of Azure Multi-Factor Authentication Service (Cloud) or Azure Multi-Factor Authentication Server\n(On-premises).\n\u2711 Multi-Factor Authentication for Office 365\n\u2711 Azure Active Directory Global Administrators\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/authentication/howto-mfa-getstarted",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #41 -- Topic 4",
        "questionIs": "DRAG DROP -\nAn organization plans to deploy Azure storage services.\nYou need to configure shared access signature (SAS) for granting access to Azure Storage.\nWhich SAS types should you use? To answer, drag the appropriate SAS types to the correct requirements. Each SAS type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0038900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #42 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are developing an ASP.NET Core app that includes feature flags which are managed by Azure App Configuration. You create an Azure App Configuration store named AppFeatureflagStore as shown in the exhibit:\n\nYou must be able to use the feature in the app by using the following markup:\n\nYou need to update the app to use the feature flag.\nWhich values should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0039000001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0039000002.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0039100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: FeatureGate -\nYou can use the FeatureGate attribute to control whether a whole controller class or a specific action is enabled.\n\nBox 2: AddAzureAppConfiguration -\nThe extension method AddAzureAppConfiguration is used to add the Azure App Configuration Provider.\nBox 3: https://appfeatureflagstore.azconfig.io\nYou need to request the access token with resource=https://<yourstorename>.azconfig.io\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-app-configuration/use-feature-flags-dotnet-core https://csharp.christiannagel.com/2020/05/19/azureappconfiguration/ https://stackoverflow.com/questions/61899063/how-to-use-azure-app-configuration-rest-api",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: FeatureGate -\nYou can use the FeatureGate attribute to control whether a whole controller class or a specific action is enabled.\n\nBox 2: AddAzureAppConfiguration -\nThe extension method AddAzureAppConfiguration is used to add the Azure App Configuration Provider.\nBox 3: https://appfeatureflagstore.azconfig.io\nYou need to request the access token with resource=https://<yourstorename>.azconfig.io\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-app-configuration/use-feature-flags-dotnet-core https://csharp.christiannagel.com/2020/05/19/azureappconfiguration/ https://stackoverflow.com/questions/61899063/how-to-use-azure-app-configuration-rest-api",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #43 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou have a single page application (SPA) web application that manages information based on data returned by Microsoft Graph from another company's Azure\nActive Directory (Azure AD) instance.\nUsers must be able to authenticate and access Microsoft Graph by using their own company's Azure AD instance.\nYou need to configure the application manifest for the app registration.\nHow should you complete the manifest? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0039400001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: true -\nThe oauth2AllowImplicitFlow attribute Specifies whether this web app can request OAuth2.0 implicit flow access tokens. The default is false. This flag is used for browser-based apps, like JavaScript single-page apps.\nIn implicit flow, the app receives tokens directly from the Azure Active Directory (Azure AD) authorize endpoint, without any server-to-server exchange. All authentication logic and session handling is done entirely in the JavaScript client with either a page redirect or a pop-up box.\n\nBox 2: requiredResourceAccess -\nWith dynamic consent, requiredResourceAccess drives the admin consent experience and the user consent experience for users who are using static consent.\nHowever, this parameter doesn't drive the user consent experience for the general case. resourceAppId is the unique identifier for the resource that the app requires access to. This value should be equal to the appId declared on the target resource app. resourceAccess is an array that lists the OAuth2.0 permission scopes and app roles that the app requires from the specified resource. Contains the id and type values of the specified resources.\nExample:\n\"requiredResourceAccess\": [\n{\n\"resourceAppId\": \"00000002-0000-0000-c000-000000000000\",\n\"resourceAccess\": [\n{\n\"id\": \"311a71cc-e848-46a1-bdf8-97ff7156d8e6\",\n\"type\": \"Scope\"\n}\n]\n}\n],\nIncorrect Answers:\n\u2711 The legacy attribute availableToOtherTenants is no longer supported.\n\u2711 The addIns attribute defines custom behavior that a consuming service can use to call an app in specific contexts. For example, applications that can render file streams may set the addIns property for its \"FileHandler\" functionality. This parameter will let services like Microsoft 365 call the application in the context of a document the user is working on.\nExample:\n\"addIns\": [\n{\n\"id\": \"968A844F-7A47-430C-9163-07AE7C31D407\",\n\"type\":\" FileHandler\",\n\"properties\": [\n{\n\"key\": \"version\",\n\"value\": \"2\"\n}\n]\n}\n],\n\nBox 3: AzureADMyOrg -\nThe signInAudience attribute specifies what Microsoft accounts are supported for the current application. Supported values are:\n\u2711 AzureADMyOrg - Users with a Microsoft work or school account in my organization's Azure AD tenant (for example, single tenant)\n\u2711 AzureADMultipleOrgs - Users with a Microsoft work or school account in any organization's Azure AD tenant (for example, multi-tenant)\n\u2711 AzureADandPersonalMicrosoftAccount - Users with a personal Microsoft account, or a work or school account in any organization's Azure AD tenant\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-manifest https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-implicit-grant-flow",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: true -\nThe oauth2AllowImplicitFlow attribute Specifies whether this web app can request OAuth2.0 implicit flow access tokens. The default is false. This flag is used for browser-based apps, like JavaScript single-page apps.\nIn implicit flow, the app receives tokens directly from the Azure Active Directory (Azure AD) authorize endpoint, without any server-to-server exchange. All authentication logic and session handling is done entirely in the JavaScript client with either a page redirect or a pop-up box.\n\nBox 2: requiredResourceAccess -\nWith dynamic consent, requiredResourceAccess drives the admin consent experience and the user consent experience for users who are using static consent.\nHowever, this parameter doesn't drive the user consent experience for the general case. resourceAppId is the unique identifier for the resource that the app requires access to. This value should be equal to the appId declared on the target resource app. resourceAccess is an array that lists the OAuth2.0 permission scopes and app roles that the app requires from the specified resource. Contains the id and type values of the specified resources.\nExample:\n\"requiredResourceAccess\": [\n{\n\"resourceAppId\": \"00000002-0000-0000-c000-000000000000\",\n\"resourceAccess\": [\n{\n\"id\": \"311a71cc-e848-46a1-bdf8-97ff7156d8e6\",\n\"type\": \"Scope\"\n}\n]\n}\n],\nIncorrect Answers:\n\u2711 The legacy attribute availableToOtherTenants is no longer supported.\n\u2711 The addIns attribute defines custom behavior that a consuming service can use to call an app in specific contexts. For example, applications that can render file streams may set the addIns property for its \"FileHandler\" functionality. This parameter will let services like Microsoft 365 call the application in the context of a document the user is working on.\nExample:\n\"addIns\": [\n{\n\"id\": \"968A844F-7A47-430C-9163-07AE7C31D407\",\n\"type\":\" FileHandler\",\n\"properties\": [\n{\n\"key\": \"version\",\n\"value\": \"2\"\n}\n]\n}\n],\n\nBox 3: AzureADMyOrg -\nThe signInAudience attribute specifies what Microsoft accounts are supported for the current application. Supported values are:\n\u2711 AzureADMyOrg - Users with a Microsoft work or school account in my organization's Azure AD tenant (for example, single tenant)\n\u2711 AzureADMultipleOrgs - Users with a Microsoft work or school account in any organization's Azure AD tenant (for example, multi-tenant)\n\u2711 AzureADandPersonalMicrosoftAccount - Users with a personal Microsoft account, or a work or school account in any organization's Azure AD tenant\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-manifest https://docs.microsoft.com/en-us/azure/active-directory/develop/v2-oauth2-implicit-grant-flow",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #44 -- Topic 4",
        "questionIs": "You manage a data processing application that receives requests from an Azure Storage queue.\nYou need to manage access to the queue. You have the following requirements:\n\u2711 Provide other applications access to the Azure queue.\n\u2711 Ensure that you can revoke access to the queue without having to regenerate the storage account keys.\n\u2711 Specify access at the queue level and not at the storage account level.\nWhich type of shared access signature (SAS) should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Service SAS with a stored access policy Most Voted",
            "B. Account SAS",
            "C. User Delegation SAS",
            "D. Service SAS with ad hoc SAS"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "A service SAS is secured with the storage account key. A service SAS delegates access to a resource in only one of the Azure Storage services: Blob storage,\nQueue storage, Table storage, or Azure Files.\nStored access policies give you the option to revoke permissions for a service SAS without having to regenerate the storage account keys.\nIncorrect Answers:\nAccount SAS: Account SAS is specified at the account level. It is secured with the storage account key.\nUser Delegation SAS: A user delegation SAS applies to Blob storage only.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #45 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are developing an application to store and retrieve data in Azure Blob storage. The application will be hosted in an on-premises virtual machine (VM). The\nVM is connected to Azure by using a Site-to-Site VPN gateway connection. The application is secured by using Azure Active Directory (Azure AD) credentials.\nThe application must be granted access to the Azure Blob storage account with a start time, expiry time, and read permissions. The Azure Blob storage account access must use the Azure AD credentials of the application to secure data access. Data access must be able to be revoked if the client application security is breached.\nYou need to secure the application access to Azure Blob storage.\nWhich security features should you use? To answer select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0039900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Shared access signature (SAS) token\nWhen your application design requires shared access signatures for access to Blob storage, use Azure AD credentials to create a user delegation SAS when possible for superior security.\n\nBox 2: Stored access policy -\nStored access policies give you the option to revoke permissions for a service SAS without having to regenerate the storage account keys.\nA shared access signature can take one of the following two forms:\n\u2711 Service SAS with stored access policy. A stored access policy is defined on a resource container, which can be a blob container, table, queue, or file share.\nThe stored access policy can be used to manage constraints for one or more service shared access signatures. When you associate a service SAS with a stored access policy, the SAS inherits the constraints \u05d2\u20ac\" the start time, expiry time, and permissions \u05d2\u20ac\" defined for the stored access policy.\n\u2711 Ad hoc SAS.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Shared access signature (SAS) token\nWhen your application design requires shared access signatures for access to Blob storage, use Azure AD credentials to create a user delegation SAS when possible for superior security.\n\nBox 2: Stored access policy -\nStored access policies give you the option to revoke permissions for a service SAS without having to regenerate the storage account keys.\nA shared access signature can take one of the following two forms:\n\u2711 Service SAS with stored access policy. A stored access policy is defined on a resource container, which can be a blob container, table, queue, or file share.\nThe stored access policy can be used to manage constraints for one or more service shared access signatures. When you associate a service SAS with a stored access policy, the SAS inherits the constraints \u05d2\u20ac\" the start time, expiry time, and permissions \u05d2\u20ac\" defined for the stored access policy.\n\u2711 Ad hoc SAS.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #46 -- Topic 4",
        "questionIs": "You are building a web application that uses the Microsoft identity platform for user authentication.\nYou are implementing user identification for the web application.\nYou need to retrieve a claim to uniquely identify a user.\nWhich claim type should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. aud",
            "B. nonce",
            "C. oid Most Voted",
            "D. idp"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "oid -The object identifier for the user in Azure AD. This value is the immutable and non-reusable identifier of the user. Use this value, not email, as a unique identifier for users; email addresses can change. If you use the Azure AD Graph API in your app, object ID is that value used to query profile information.\nIncorrect:\nNot A: aud - Who the token was issued for. This will be the application's client ID.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/multitenant-identity/claims",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #47 -- Topic 4",
        "questionIs": "You are developing an Azure Function that calls external APIs by providing an access token for the API. The access token is stored in a secret named token in an\nAzure Key Vault named mykeyvault.\nYou need to ensure the Azure Function can access to the token. Which value should you store in the Azure Function App configuration?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. KeyVault:mykeyvault;Secret:token",
            "B. App:Settings:Secret:mykeyvault:token",
            "C. AZUREKVCONNSTR_ https://mykeyveult.vault.ezure.net/secrets/token/",
            "D. @Microsoft.KeyVault(SecretUri=https://mykeyvault.vault.azure.net/secrets/token/) Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Add Key Vault secrets reference in the Function App configuration.\nSyntax: @Microsoft.KeyVault(SecretUri={copied identifier for the username secret})\nReference:\nhttps://daniel-krzyczkowski.github.io/Integrate-Key-Vault-Secrets-With-Azure-Functions/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #48 -- Topic 4",
        "questionIs": "A company maintains multiple web and mobile applications. Each application uses custom in-house identity providers as well as social identity providers.\nYou need to implement single sign-on (SSO) for all the applications.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Use Azure Active Directory B2C (Azure AD B2C) with custom policies. Most Voted",
            "B. Use Azure Active Directory B2B (Azure AD B2B) and enable external collaboration.",
            "C. Use Azure Active Directory B2C (Azure AD B2C) with user flows.",
            "D. Use Azure Active Directory B2B (Azure AD B2B)."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "You can add Google as an identity provider for B2B guest users.\nFederation with SAML/WS-Fed identity providers for guest users.\nMake sure your organization's external collaboration settings are configured such that you're allowed to invite guests.\nNote 1: As a user who is assigned any of the limited administrator directory roles, you can use the Azure portal to invite B2B collaboration users. You can invite guest users to the directory, to a group, or to an application. After you invite a user through any of these methods, the invited user's account is added to Azure\nActive Directory (Azure AD), with a user type of Guest.\nNote 2: Direct federation in Azure Active Directory is now referred to as SAML/WS-Fed identity provider (IdP) federation.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/external-identities/google-federation https://docs.microsoft.com/en-us/azure/active-directory/external-identities/add-users-administrator",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #49 -- Topic 4",
        "questionIs": "You develop a Python application for image rendering that uses GPU resources to optimize rendering processes. You deploy the application to an Azure\nContainer Instances (ACI) Linux container.\nThe application requires a secret value to be passed when the container is started. The value must only be accessed from within the container.\nYou need to pass the secret value.\nWhat are two possible ways to achieve this goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create an environment variable Set the secureValue property to the secret value. Most Voted",
            "B. Add the secret value to the container image. Use a managed identity.",
            "C. Add the secret value to the application code Set the container startup command.",
            "D. Add the secret value to an Azure Blob storage account. Generate a SAS token.",
            "E. Mount a secret volume containing the secret value in a secrets file. Most Voted"
        ],
        "answersAre": [
            "A",
            "E"
        ],
        "mostVotedAre": [
            "A",
            "E"
        ],
        "descriptionIs": "A: Secure environment variables -\nAnother method (another than a secret volume) for providing sensitive information to containers (including Windows containers) is through the use of secure environment variables.\nE: Use a secret volume to supply sensitive information to the containers in a container group. The secret volume stores your secrets in files within the volume, accessible by the containers in the container group. By storing secrets in a secret volume, you can avoid adding sensitive data like SSH keys or database credentials to your application code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-volume-secret",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #50 -- Topic 4",
        "questionIs": "You are developing a user portal for a company.\nYou need to create a report for the portal that lists information about employees who are subject matter experts for a specific topic. You must ensure that administrators have full control and consent over the data.\nWhich technology should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Microsoft Graph data connect Most Voted",
            "B. Microsoft Graph API",
            "C. Microsoft Graph connectors"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "Data Connect grants a more granular control and consent model: you can manage data, see who is accessing it, and request specific properties of an entity. This enhances the Microsoft Graph model, which grants or denies applications access to entire entities.\nMicrosoft Graph Data Connect augments Microsoft Graph's transactional model with an intelligent way to access rich data at scale. The data covers how workers communicate, collaborate, and manage their time across all the applications and services in Microsoft 365.\nIncorrect:\nNot B: The Microsoft Graph API is a RESTful web API that enables you to access Microsoft Cloud service resources. After you register your app and get authentication tokens for a user or service, you can make requests to the Microsoft Graph API.\nA simplistic definition of a Graph API is an API that models the data in terms of nodes and edges (objects and relationships) and allows the client to interact with multiple nodes in a single request.\nNot C: Microsoft Graph connectors, your organization can index third-party data so that it appears in Microsoft Search results.\nWith Microsoft Graph connectors, your organization can index third-party data so that it appears in Microsoft Search results.\nReference:\nhttps://docs.microsoft.com/en-us/graph/data-connect-concept-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #51 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are a developer building a web site using a web app. The web site stores configuration data in Azure App Configuration.\nAccess to Azure App Configuration has been configured to use the identity of the web app for authentication. Security requirements specify that no other authentication systems must be used.\nYou need to load configuration data from Azure App Configuration.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0040600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: AddAzureAppConfiguration -\nLoad data from App Configuration, code example:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\nHost.CreateDefaultBuilder(args)\n.ConfigureWebHostDefaults(webBuilder =>\nwebBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n{\nvar settings = config.Build();\nconfig.AddAzureAppConfiguration(options =>\n{\nEtc.\nBox 2: ManagedIdentityCredential\nUse managed identities to access App Configuration\nIf you want to use a user-assigned managed identity, be sure to specify the clientId when creating the ManagedIdentityCredential. config.AddAzureAppConfiguration(options =>\n{\noptions.Connect(new Uri(settings[\"AppConfig:Endpoint\"]), new ManagedIdentityCredential(\"<your_clientId>\"))\n});\nFull code sample:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\nHost.CreateDefaultBuilder(args)\n.ConfigureWebHostDefaults(webBuilder =>\nwebBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n{\nvar settings = config.Build();\nconfig.AddAzureAppConfiguration(options =>\noptions.Connect(new Uri(settings[\"AppConfig:Endpoint\"]), new ManagedIdentityCredential()));\n})\n.UseStartup<Startup>());\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-app-configuration/howto-integrate-azure-managed-service-identity?tabs=core5x&pivots=framework- dotnet",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: AddAzureAppConfiguration -\nLoad data from App Configuration, code example:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\nHost.CreateDefaultBuilder(args)\n.ConfigureWebHostDefaults(webBuilder =>\nwebBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n{\nvar settings = config.Build();\nconfig.AddAzureAppConfiguration(options =>\n{\nEtc.\nBox 2: ManagedIdentityCredential\nUse managed identities to access App Configuration\nIf you want to use a user-assigned managed identity, be sure to specify the clientId when creating the ManagedIdentityCredential. config.AddAzureAppConfiguration(options =>\n{\noptions.Connect(new Uri(settings[\"AppConfig:Endpoint\"]), new ManagedIdentityCredential(\"<your_clientId>\"))\n});\nFull code sample:\npublic static IHostBuilder CreateHostBuilder(string[] args) =>\nHost.CreateDefaultBuilder(args)\n.ConfigureWebHostDefaults(webBuilder =>\nwebBuilder.ConfigureAppConfiguration((hostingContext, config) =>\n{\nvar settings = config.Build();\nconfig.AddAzureAppConfiguration(options =>\noptions.Connect(new Uri(settings[\"AppConfig:Endpoint\"]), new ManagedIdentityCredential()));\n})\n.UseStartup<Startup>());\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-app-configuration/howto-integrate-azure-managed-service-identity?tabs=core5x&pivots=framework- dotnet",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #52 -- Topic 4",
        "questionIs": "HOTSPOT -\nYou are building an application that stores sensitive customer data in Azure Blob storage. The data must be encrypted with a key that is unique for each customer.\nIf the encryption key has been corrupted it must not be used for encryption.\nYou need to ensure that the blob is encrypted.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0041000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: CustomerProvidedKey(key)\nThe data must be encrypted with a key that is unique for each customer.\nSample code:\nasync static Task UploadBlobWithClientKey(Uri blobUri,\nStream data,\nbyte[] key,\nstring keySha256)\n{\n// Create a new customer-provided key.\n// Key must be AES-256.\nvar cpk = new CustomerProvidedKey(key);\n\nBox 2: Encryption -\nCustomerProvidedKey.EncryptionKey Property\nSample code continued:\n// Check the key's encryption hash.\nif (cpk.EncryptionKeyHash != keySha256)\n{\nthrow new InvalidOperationException(\"The encryption key is corrupted.\");\n}\n\nBox 3: CustomerProvidedKey -\nSample code continued;\n// Specify the customer-provided key on the options for the client.\nBlobClientOptions options = new BlobClientOptions()\n{\n\nCustomerProvidedKey = cpk -\n};\n// Create the client object with options specified.\nBlobClient blobClient = new BlobClient(\nblobUri,\nnew DefaultAzureCredential(),\noptions);\nIncorrect:\n* Version - Gets the BlobClientOptions.ServiceVersion of the service API used when making requests.\nTransport - The HttpPipelineTransport to be used for this client.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-customer-provided-key",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: CustomerProvidedKey(key)\nThe data must be encrypted with a key that is unique for each customer.\nSample code:\nasync static Task UploadBlobWithClientKey(Uri blobUri,\nStream data,\nbyte[] key,\nstring keySha256)\n{\n// Create a new customer-provided key.\n// Key must be AES-256.\nvar cpk = new CustomerProvidedKey(key);\n\nBox 2: Encryption -\nCustomerProvidedKey.EncryptionKey Property\nSample code continued:\n// Check the key's encryption hash.\nif (cpk.EncryptionKeyHash != keySha256)\n{\nthrow new InvalidOperationException(\"The encryption key is corrupted.\");\n}\n\nBox 3: CustomerProvidedKey -\nSample code continued;\n// Specify the customer-provided key on the options for the client.\nBlobClientOptions options = new BlobClientOptions()\n{\n\nCustomerProvidedKey = cpk -\n};\n// Create the client object with options specified.\nBlobClient blobClient = new BlobClient(\nblobUri,\nnew DefaultAzureCredential(),\noptions);\nIncorrect:\n* Version - Gets the BlobClientOptions.ServiceVersion of the service API used when making requests.\nTransport - The HttpPipelineTransport to be used for this client.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-customer-provided-key",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #53 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a web application that uses the Microsoft Identity platform for user and resource authentication. The web application called several REST APIs.\n\nYou are implementing various authentication and authorization flows for the web application.\n\nYou need to validate the claims in the authentication token.\n\nWhich token type should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image395.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #54 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a content management application for technical manuals. The application is deployed as an Azure Static Web app.\n\nAuthenticated users can view pages under/manuals but only contributors can access the page /manuals/new.html.\n\nYou need to configure the routing for the web app.\n\nHow should you complete the configuration? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image397.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #55 -- Topic 4",
        "questionIs": "You are developing a web application that uses the Microsoft identity platform for user and resource authentication. The web application calls several REST APIs.\n\nA REST API call must read the user\u2019s calendar. The web application requires permission to send an email as the user.\n\nYou need to authorize the web application and the API.\n\nWhich parameter should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. tenant",
            "B. code_challenge",
            "C. state",
            "D. client_id",
            "E. scope Most Voted"
        ],
        "answersAre": [
            "E"
        ],
        "mostVotedAre": [
            "E"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #56 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou develop and deploy a web app to Azure App service. The web app allows users to authenticate by using social identity providers through the Azure B2C service. All user profile information is stored in Azure B2C.\n\nYou must update the web app to display common user properties from Azure B2C to include the following information:\n\n\u2022 Email address\n\u2022 Job title\n\u2022 First name\n\u2022 Last name\n\u2022 Office location\n\nYou need to implement the user properties in the web app.\n\nWhich code library and API should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image399.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #57 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou develop and deploy the following staticwebapp.config.json file to the app_location value specified in the workflow file of an Azure Static Web app:\n\n\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image401.png",
            "https://img.examtopics.com/az-204/image402.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #58 -- Topic 4",
        "questionIs": "You develop and deploy an Azure App Service web app named App1. You create a new Azure Key Vault named Vault1. You import several API keys, passwords, certificates, and cryptographic keys into Vault1.\n\nYou need to grant App1 access to Vault1 and automatically rotate credentials. Credentials must not be stored in code.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Enable App Service authentication for Appl. Assign a custom RBAC role to Vault1.",
            "B. Add a TLS/SSL binding to App1.",
            "C. Upload a self-signed client certificate to Vault1. Update App1 to use the client certificate.",
            "D. Assign a managed identity to App1. Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #59 -- Topic 4",
        "questionIs": "You are developing a Java application to be deployed in Azure. The application stores sensitive data in Azure Cosmos DB.\n\nYou need to configure Always Encrypted to encrypt the sensitive data inside the application.\n\nWhat should you do first?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a new container to include an encryption policy with the JSON properties to be encrypted.",
            "B. Create a customer-managed key (CMK) and store the key in a new Azure Key Vault instance. Most Voted",
            "C. Create a data encryption key (DEK) by using the Azure Cosmos DB SDK and store the key in Azure Cosmos DB.",
            "D. Create an Azure AD managed identity and assign the identity to a new Azure Key Vault instance."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #60 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou develop a web app that interacts with Azure Active Directory (Azure AD) groups by using Microsoft Graph.\n\nYou build a web page that shows all Azure AD groups that are not of the type 'Unified'.\n\nYou need to build the Microsoft Graph query for the page.\n\nHow should you complete the query? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image404.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #61 -- Topic 4",
        "questionIs": "DRAG DROP\n-\n\nYou are developing an Azure solution.\n\nYou need to develop code to access a secret stored in Azure Key Vault.\n\nHow should you complete the code segment? To answer, drag the appropriate code segments to the correct location. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image406.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #62 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou are a developer building a web site using a web app. The web site stores configuration data in Azure App Configuration.\n\nAccess to Azure App Configuration has been configured to use the identity of the web app for authentication. Security requirements specify that no other authentication systems must be used.\n\nYou need to load configuration data from Azure App Configuration.\n\nHow should you complete the code? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image408.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #63 -- Topic 4",
        "questionIs": "You are developing several microservices to deploy to a new Azure Kubernetes Service cluster. The microservices manage data stored in Azure Cosmos DB and Azure Blob storage. The data is secured by using customer-managed keys stored in Azure Key Vault.\n\nYou must automate key rotation for all Azure Key Vault keys and allow for manual key rotation. Keys must rotate every three months. Notifications of expiring keys must be sent before key expiry.\n\nYou need to configure key rotation and enable key expiry notifications.\n\nWhich two actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create and configure a new Azure Event Grid instance. Most Voted",
            "B. Configure Azure Key Vault alerts.",
            "C. Create and assign an Azure Key Vault access policy.",
            "D. Create and configure a key rotation policy during key creation. Most Voted"
        ],
        "answersAre": [
            "A",
            "C"
        ],
        "mostVotedAre": [
            "A",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #64 -- Topic 4",
        "questionIs": "You are developing a web application that uses the Microsoft identity platform to authenticate users and resources. The web application calls several REST APIs.\n\nThe APIs require an access token from the Microsoft identity platform.\n\nYou need to request a token.\n\nWhich three properties should you use? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Redirect URI/URL",
            "B. Application ID",
            "C. Application name",
            "D. Application secret",
            "E. Supported account type"
        ],
        "answersAre": [
            "A",
            "B",
            "D"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #65 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an application that uses Azure Storage to store customer data. The data must only be decrypted by the customer and the customer must be provided a script to rotate keys.\n\nYou need to provide a script to rotate keys to the customer.\n\nHow should you complete the command? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image453.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #66 -- Topic 4",
        "questionIs": "You are developing several Azure API Management (APIM) hosted APIs.\n\nYou must transform the APIs to hide private backend information and obscure the technology stack used to implement the backend processing.\n\nYou need to protect all APIs.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure and apply a new inbound policy scoped to a product.",
            "B. Configure and apply a new outbound policy scoped to the operation.",
            "C. Configure and apply a new outbound policy scoped to global. Most Voted",
            "D. Configure and apply a new backend policy scoped to global."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #67 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an Azure Function App named App1. You also plan to use cross-origin requests (CORS).\n\nYou have the following requirements:\n\n\u2022 App1 functions must securely access an Azure Blob Storage account.\n\u2022 Access to the Azure Blob Storage account must not require the provisioning or rotation of secrets.\n\u2022 JavaScript code running in a browser on an external host must not be allowed to interact with the function.\n\nYou need to implement App1.\n\nWhich configuration should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image486.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #68 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\nYou develop a containerized application. The application must be deployed to an existing Azure Kubernetes Service (AKS) cluster from an Azure Container Registry (ACR) instance. You use the Azure command-line interface (Azure CLI) to deploy the application image to AKS.\n\nImages must be pulled from the registry. You must be able to view all registries within the current Azure subscription. Authentication must be managed by Microsoft Entra ID and removed when the registry is deleted. The solution must use the principle of least privilege.\n\nYou need to configure authentication to the registry.\n\nWhich authentication configuration should you use? To answer, select the appropriate configuration values in the answer area,\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image509.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #69 -- Topic 4",
        "questionIs": "Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website -\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms -\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors -\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n\nCorporate website -\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms -\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors -\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff -\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity -\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues -\n\n\nCorporate website -\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors -\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to implement farmer authentication.\n\nWhich three actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Add the shared access signature (SAS) token to the app.",
            "B. Create a shared access signature (SAS) token.",
            "C. Create a user flow. Most Voted",
            "D. Add the app to the user flow. Most Voted",
            "E. Register the app in Microsoft Entra ID. Most Voted"
        ],
        "answersAre": [
            "C",
            "D",
            "E"
        ],
        "mostVotedAre": [
            "C",
            "D",
            "E"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #70 -- Topic 4",
        "questionIs": "Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website -\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms -\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors -\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n\nCorporate website -\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms -\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors -\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff -\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity -\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues -\n\n\nCorporate website -\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors -\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to secure the corporate website to meet the security requirements.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create an Azure Cache for Redis instance. Update the code to support the cache.",
            "B. Create an Azure Content Delivery Network profile and endpoint. Configure the endpoint.\n\u0421. Create an App Service instance with a standard plan. Configure the custom domain with a TLS/SSL certificate.",
            "D. Create an Azure Application Gateway with a Web Application Firewall (WAF). Configure end-to-end TLS encryption and the WAF. Most Voted"
        ],
        "answersAre": [],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #71 -- Topic 4",
        "questionIs": "HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment\n-\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website\n-\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms\n-\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors\n-\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms\n-\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors\n-\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff\n-\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity\n-\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues\n-\n\n\nCorporate website\n-\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors\n-\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to display the profile photo and email for signed-in internal staff on the website.\n\nWhich Microsoft Graph configuration should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image511.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #72 -- Topic 4",
        "questionIs": "Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website -\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms -\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors -\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n\nCorporate website -\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms -\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors -\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff -\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity -\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues -\n\n\nCorporate website -\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors -\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to configure all site configuration settings for the corporate website.\n\nWhich three actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a managed identity. Most Voted",
            "B. Update the role assignments for the Azure Key Vault.",
            "C. Create an Azure App Configuration store. Most Voted",
            "D. Update the role assignments for the Azure App Configuration store. Most Voted",
            "E. Create an Azure Key Vault."
        ],
        "answersAre": [
            "A",
            "B",
            "E"
        ],
        "mostVotedAre": [
            "A",
            "C",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #73 -- Topic 4",
        "questionIs": "You are developing an application that uses keys stored in Azure Key Vault.\n\nYou need to enforce a specific cryptographic algorithm and key size for keys stored in the vault.\n\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Secret versioning",
            "B. Azure Policy Most Voted",
            "C. Key Vault Firewall",
            "D. Access policies"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 5",
        "questionIs": "DRAG DROP -\nYou develop a web app that uses the tier D1 app service plan by using the Web Apps feature of Microsoft Azure App Service.\nSpikes in traffic have caused increases in page load times.\nYou need to ensure that the web app automatically scales when CPU load is about 85 percent and minimize costs.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0045400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Configure the web app to the Standard App Service Tier\nThe Standard tier supports auto-scaling, and we should minimize the cost.\nStep 2: Enable autoscaling on the web app\n\nFirst enable autoscale -\n\nStep 3: Add a scale rule -\n\nStep 4: Add a Scale condition -\nReference:\nhttps://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-autoscale-get-started",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Configure the web app to the Standard App Service Tier\nThe Standard tier supports auto-scaling, and we should minimize the cost.\nStep 2: Enable autoscaling on the web app\n\nFirst enable autoscale -\n\nStep 3: Add a scale rule -\n\nStep 4: Add a Scale condition -\nReference:\nhttps://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-autoscale-get-started",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 5",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output.\nYou must use a storage mechanism with the following requirements:\n\u2711 Share session state across all ASP.NET web applications.\n\u2711 Support controlled, concurrent access to the same session state data for multiple readers and a single writer.\n\u2711 Save full HTTP responses for concurrent requests.\nYou need to store the information.\nProposed Solution: Enable Application Request Routing (ARR).\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Instead deploy and configure Azure Cache for Redis. Update the web applications.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/caching#managing-concurrency-in-a-cache",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 5",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output.\nYou must use a storage mechanism with the following requirements:\n\u2711 Share session state across all ASP.NET web applications.\n\u2711 Support controlled, concurrent access to the same session state data for multiple readers and a single writer.\n\u2711 Save full HTTP responses for concurrent requests.\nYou need to store the information.\nProposed Solution: Deploy and configure an Azure Database for PostgreSQL. Update the web applications.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "Instead deploy and configure Azure Cache for Redis. Update the web applications.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/caching#managing-concurrency-in-a-cache",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 5",
        "questionIs": "HOTSPOT -\nA company is developing a gaming platform. Users can join teams to play online and see leaderboards that include player statistics. The solution includes an entity named Team.\nYou plan to implement an Azure Redis Cache instance to improve the efficiency of data operations for entities that rarely change.\nYou need to invalidate the cache when team data is changed.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0045800001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: IDatabase cache = connection.GetDatabase();\nConnection refers to a previously configured ConnectionMultiplexer.\nBox 2: cache.StringSet(\"teams\",\")\nTo specify the expiration of an item in the cache, use the TimeSpan parameter of StringSet. cache.StringSet(\"key1\", \"value1\", TimeSpan.FromMinutes(90));\nReference:\nhttps://azure.microsoft.com/sv-se/blog/lap-around-azure-redis-cache-preview/ https://docs.microsoft.com/en-us/cli/azure/webapp/config/container",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: IDatabase cache = connection.GetDatabase();\nConnection refers to a previously configured ConnectionMultiplexer.\nBox 2: cache.StringSet(\"teams\",\")\nTo specify the expiration of an item in the cache, use the TimeSpan parameter of StringSet. cache.StringSet(\"key1\", \"value1\", TimeSpan.FromMinutes(90));\nReference:\nhttps://azure.microsoft.com/sv-se/blog/lap-around-azure-redis-cache-preview/ https://docs.microsoft.com/en-us/cli/azure/webapp/config/container",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 5",
        "questionIs": "DRAG DROP -\nA company has multiple warehouses. Each warehouse contains IoT temperature devices which deliver temperature data to an Azure Service Bus queue.\nYou need to send email alerts to facility supervisors immediately if the temperature at a warehouse goes above or below specified threshold temperatures.\nWhich five actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0046000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Create a blank Logic app.\nCreate and configure a Logic App.\nStep 2: Add a logical app trigger that fires when one or more messages arrive in the queue.\nConfigure the logic app trigger.\nUnder Triggers, select When one or more messages arrive in a queue (auto-complete).\nStep 3: Add an action that reads IoT temperature data from the Service Bus queue\nStep 4: Add a condition that compares the temperature against the upper and lower thresholds.\nStep 5: Add an action that sends an email to specified personnel if the temperature is outside of those thresholds\nReference:\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-monitoring-notifications-with-azure-logic-apps",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Create a blank Logic app.\nCreate and configure a Logic App.\nStep 2: Add a logical app trigger that fires when one or more messages arrive in the queue.\nConfigure the logic app trigger.\nUnder Triggers, select When one or more messages arrive in a queue (auto-complete).\nStep 3: Add an action that reads IoT temperature data from the Service Bus queue\nStep 4: Add a condition that compares the temperature against the upper and lower thresholds.\nStep 5: Add an action that sends an email to specified personnel if the temperature is outside of those thresholds\nReference:\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-monitoring-notifications-with-azure-logic-apps",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 5",
        "questionIs": "DRAG DROP -\nYou develop an ASP.NET Core MVC application. You configure the application to track webpages and custom events.\nYou need to identify trends in application usage.\nWhich Azure Application Insights Usage Analysis features should you use? To answer, drag the appropriate features to the correct requirements. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0046200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Users -\n\nBox 2: Impact -\nOne way to think of Impact is as the ultimate tool for settling arguments with someone on your team about how slowness in some aspect of your site is affecting whether users stick around. While users may tolerate a certain amount of slowness, Impact gives you insight into how best to balance optimization and performance to maximize user conversion.\n\nBox 3: Retention -\nThe retention feature in Azure Application Insights helps you analyze how many users return to your app, and how often they perform particular tasks or achieve goals. For example, if you run a game site, you could compare the numbers of users who return to the site after losing a game with the number who return after winning. This knowledge can help you improve both your user experience and your business strategy.\n\nBox 4: User flows -\nThe User Flows tool visualizes how users navigate between the pages and features of your site. It's great for answering questions like:\n\u2711 How do users navigate away from a page on your site?\n\u2711 What do users click on a page on your site?\n\u2711 Where are the places that users churn most from your site?\n\u2711 Are there places where users repeat the same action over and over?\nIncorrect Answers:\nFunnel: If your application involves multiple stages, you need to know if most customers are progressing through the entire process, or if they are ending the process at some point. The progression through a series of steps in a web application is known as a funnel. You can use Azure Application Insights Funnels to gain insights into your users, and monitor step-by-step conversion rates.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-impact",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Users -\n\nBox 2: Impact -\nOne way to think of Impact is as the ultimate tool for settling arguments with someone on your team about how slowness in some aspect of your site is affecting whether users stick around. While users may tolerate a certain amount of slowness, Impact gives you insight into how best to balance optimization and performance to maximize user conversion.\n\nBox 3: Retention -\nThe retention feature in Azure Application Insights helps you analyze how many users return to your app, and how often they perform particular tasks or achieve goals. For example, if you run a game site, you could compare the numbers of users who return to the site after losing a game with the number who return after winning. This knowledge can help you improve both your user experience and your business strategy.\n\nBox 4: User flows -\nThe User Flows tool visualizes how users navigate between the pages and features of your site. It's great for answering questions like:\n\u2711 How do users navigate away from a page on your site?\n\u2711 What do users click on a page on your site?\n\u2711 Where are the places that users churn most from your site?\n\u2711 Are there places where users repeat the same action over and over?\nIncorrect Answers:\nFunnel: If your application involves multiple stages, you need to know if most customers are progressing through the entire process, or if they are ending the process at some point. The progression through a series of steps in a web application is known as a funnel. You can use Azure Application Insights Funnels to gain insights into your users, and monitor step-by-step conversion rates.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-impact",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #7 -- Topic 5",
        "questionIs": "You develop a gateway solution for a public facing news API. The news API back end is implemented as a RESTful service and uses an OpenAPI specification.\nYou need to ensure that you can access the news API by using an Azure API Management service instance.\nWhich Azure PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Import-AzureRmApiManagementApi -Context $ApiMgmtContext -SpecificationFormat \"Swagger\" -SpecificationPath $SwaggerPath -Path $Path Most Voted",
            "B. New-AzureRmApiManagementBackend -Context $ApiMgmtContext -Url $Url -Protocol http",
            "C. New-AzureRmApiManagement -ResourceGroupName $ResourceGroup -Name $Name \u05d2\u20ac\"Location $Location -Organization $Org -AdminEmail $AdminEmail",
            "D. New-AzureRmApiManagementBackendProxy -Url $ApiUrl"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "New-AzureRmApiManagementBackendProxy creates a new Backend Proxy Object which can be piped when creating a new Backend entity.\nExample: Create a Backend Proxy In-Memory Object\nPS C:\\>$secpassword = ConvertTo-SecureString \"PlainTextPassword\" -AsPlainText -Force\nPS C:\\>$proxyCreds = New-Object System.Management.Automation.PSCredential (\"foo\", $secpassword)\nPS C:\\>$credential = New-AzureRmApiManagementBackendProxy -Url \"http://12.168.1.1:8080\" -ProxyCredential $proxyCreds\nPS C:\\>$apimContext = New-AzureRmApiManagementContext -ResourceGroupName \"Api-Default-WestUS\" -ServiceName \"contoso\"\nPS C:\\>$backend = New-AzureRmApiManagementBackend -Context $apimContext -BackendId 123 -Url 'https://contoso.com/awesomeapi' -Protocol http -Title\n\"first backend\" -SkipCertificateChainValidation $true -Proxy $credential -Description \"backend with proxy server\"\nCreates a Backend Proxy Object and sets up Backend\nIncorrect Answers:\nA: The Import-AzureRmApiManagementApi cmdlet imports an Azure API Management API from a file or a URL in Web Application Description Language (WADL),\nWeb Services Description Language (WSDL), or Swagger format.\nB: New-AzureRmApiManagementBackend creates a new backend entity in Api Management.\nC: The New-AzureRmApiManagement cmdlet creates an API Management deployment in Azure API Management.\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/azurerm.apimanagement/new-azurermapimanagementbackendproxy?view=azurermps-6.13.0",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #8 -- Topic 5",
        "questionIs": "You are creating a hazard notification system that has a single signaling server which triggers audio and visual alarms to start and stop.\nYou implement Azure Service Bus to publish alarms. Each alarm controller uses Azure Service Bus to receive alarm signals as part of a transaction. Alarm events must be recorded for audit purposes. Each transaction record must include information about the alarm type that was activated.\nYou need to implement a reply trail auditing solution.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Assign the value of the hazard message SessionID property to the ReplyToSessionId property. Most Voted",
            "B. Assign the value of the hazard message MessageId property to the DevileryCount property.",
            "C. Assign the value of the hazard message SessionID property to the SequenceNumber property.",
            "D. Assign the value of the hazard message MessageId property to the CorrelationId property. Most Voted",
            "E. Assign the value of the hazard message SequenceNumber property to the DeliveryCount property.",
            "F. Assign the value of the hazard message MessageId property to the SequenceNumber property."
        ],
        "answersAre": [
            "A",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "D"
        ],
        "descriptionIs": "D: CorrelationId: Enables an application to specify a context for the message for the purposes of correlation; for example, reflecting the MessageId of a message that is being replied to.\nA: ReplyToSessionId: This value augments the ReplyTo information and specifies which SessionId should be set for the reply when sent to the reply entity.\nIncorrect Answers:\n\nB, E: DeliveryCount -\nNumber of deliveries that have been attempted for this message. The count is incremented when a message lock expires, or the message is explicitly abandoned by the receiver. This property is read-only.\n\nC, E: SequenceNumber -\nThe sequence number is a unique 64-bit integer assigned to a message as it is accepted and stored by the broker and functions as its true identifier. For partitioned entities, the topmost 16 bits reflect the partition identifier. Sequence numbers monotonically increase and are gapless. They roll over to 0 when the 48-\n64 bit range is exhausted. This property is read-only.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messages-payloads",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #9 -- Topic 5",
        "questionIs": "You are developing an Azure function that connects to an Azure SQL Database instance. The function is triggered by an Azure Storage queue.\nYou receive reports of numerous System.InvalidOperationExceptions with the following message:\n`Timeout expired. The timeout period elapsed prior to obtaining a connection from the pool. This may have occurred because all pooled connections were in use and max pool size was reached.`\nYou need to prevent the exception.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. In the host.json file, decrease the value of the batchSize option Most Voted",
            "B. Convert the trigger to Azure Event Hub",
            "C. Convert the Azure Function to the Premium plan",
            "D. In the function.json file, change the value of the type option to queueScaling"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "With the Premium plan the max outbound connections per instance is unbounded compared to the 600 active (1200 total) in a Consumption plan.\nNote: The number of available connections is limited partly because a function app runs in a sandbox environment. One of the restrictions that the sandbox imposes on your code is a limit on the number of outbound connections, which is currently 600 active (1,200 total) connections per instance. When you reach this limit, the functions runtime writes the following message to the logs: Host thresholds exceeded: Connections.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/manage-connections https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#service-limits",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #10 -- Topic 5",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.\nYou are developing and deploying several ASP.NET web applications to Azure App Service. You plan to save session state information and HTML output.\nYou must use a storage mechanism with the following requirements:\n\u2711 Share session state across all ASP.NET web applications.\n\u2711 Support controlled, concurrent access to the same session state data for multiple readers and a single writer.\n\u2711 Save full HTTP responses for concurrent requests.\nYou need to store the information.\nProposed Solution: Deploy and configure Azure Cache for Redis. Update the web applications.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "The session state provider for Azure Cache for Redis enables you to share session information between different instances of an ASP.NET web application.\nThe same connection can be used by multiple concurrent threads.\nRedis supports both read and write operations.\nThe output cache provider for Azure Cache for Redis enables you to save the HTTP responses generated by an ASP.NET web application.\nNote: Using the Azure portal, you can also configure the eviction policy of the cache, and control access to the cache by adding users to the roles provided. These roles, which define the operations that members can perform, include Owner, Contributor, and Reader. For example, members of the Owner role have complete control over the cache (including security) and its contents, members of the Contributor role can read and write information in the cache, and members of the\nReader role can only retrieve data from the cache.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/caching",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #11 -- Topic 5",
        "questionIs": "HOTSPOT -\nYou are debugging an application that is running on Azure Kubernetes cluster named cluster1. The cluster uses Azure Monitor for containers to monitor the cluster.\nThe application has sticky sessions enabled on the ingress controller.\nSome customers report a large number of errors in the application over the last 24 hours.\nYou need to determine on which virtual machines (VMs) the errors are occurring.\nHow should you complete the Azure Monitor query? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0046900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: ago(1d)\n\nBox 2: distinct containerID -\nBox 3: where ContainerID in (ContainerIDs)\nBox 4: summarize Count by Computer\nSummarize: aggregate groups of rows\nUse summarize to identify groups of records, according to one or more columns, and apply aggregations to them. The most common use of summarize is count, which returns the number of results in each group.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-queries https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-optimization",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: ago(1d)\n\nBox 2: distinct containerID -\nBox 3: where ContainerID in (ContainerIDs)\nBox 4: summarize Count by Computer\nSummarize: aggregate groups of rows\nUse summarize to identify groups of records, according to one or more columns, and apply aggregations to them. The most common use of summarize is count, which returns the number of results in each group.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/log-query/get-started-queries https://docs.microsoft.com/en-us/azure/azure-monitor/log-query/query-optimization",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #12 -- Topic 5",
        "questionIs": "HOTSPOT -\nYou plan to deploy a web app to App Service on Linux. You create an App Service plan. You create and push a custom Docker image that contains the web app to Azure Container Registry.\nYou need to access the console logs generated from inside the container in real-time.\nHow should you complete the Azure CLI command? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0047300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: config -\nTo Configure logging for a web app use the command:\naz webapp log config\nBox 2: --docker-container-logging\nSyntax include:\naz webapp log config [--docker-container-logging {filesystem, off}]\n\nBox 3: webapp -\nTo download a web app's log history as a zip file use the command: az webapp log download\n\nBox 4: download -\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/webapp/log",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: config -\nTo Configure logging for a web app use the command:\naz webapp log config\nBox 2: --docker-container-logging\nSyntax include:\naz webapp log config [--docker-container-logging {filesystem, off}]\n\nBox 3: webapp -\nTo download a web app's log history as a zip file use the command: az webapp log download\n\nBox 4: download -\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/webapp/log",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #13 -- Topic 5",
        "questionIs": "You develop and deploy an ASP.NET web app to Azure App Service. You use Application Insights telemetry to monitor the app.\nYou must test the app to ensure that the app is available and responsive from various points around the world and at regular intervals. If the app is not responding, you must send an alert to support staff.\nYou need to configure a test for the web app.\nWhich two test types can you use? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. integration",
            "B. multi-step web",
            "C. URL ping",
            "D. unit",
            "E. load"
        ],
        "answersAre": [
            "B",
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "There are three types of availability tests:\n\u2711 URL ping test: a simple test that you can create in the Azure portal.\n\u2711 Multi-step web test: A recording of a sequence of web requests, which can be played back to test more complex scenarios. Multi-step web tests are created in\nVisual Studio Enterprise and uploaded to the portal for execution.\n\u2711 Custom Track Availability Tests: If you decide to create a custom application to run availability tests, the TrackAvailability() method can be used to send the results to Application Insights.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/monitor-web-app-availability",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #14 -- Topic 5",
        "questionIs": "DRAG DROP -\nA web service provides customer summary information for e-commerce partners. The web service is implemented as an Azure Function app with an HTTP trigger.\nAccess to the API is provided by an Azure API Management instance. The API Management instance is configured in consumption plan mode. All API calls are authenticated by using OAuth.\nAPI calls must be cached. Customers must not be able to view cached data for other customers.\nYou need to configure API Management policies for caching.\nHow should you complete the policy statement?\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0047600001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: internal -\ncaching-type\nChoose between the following values of the attribute:\n\u2711 internal to use the built-in API Management cache,\n\u2711 external to use the external cache as Azure Cache for Redis prefer-external to use external cache if configured or internal cache otherwise.\n\n\nBox 2: private -\ndownstream-caching-type\nThis attribute must be set to one of the following values.\n\u2711 none - downstream caching is not allowed.\n\u2711 private - downstream private caching is allowed.\n\u2711 public - private and shared downstream caching is allowed.\n\nBox 3: Authorization -\n<vary-by-header>Authorization</vary-by-header>\n<!-- should be present when allow-private-response-caching is \"true\"-->\nNote: Start caching responses per value of specified header, such as Accept, Accept-Charset, Accept-Encoding, Accept-Language, Authorization, Expect, From,\n\nHost, If-Match -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-policieshttps://www.examtopics.com/assets/media/exam-media/04273/0047600005.png",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: internal -\ncaching-type\nChoose between the following values of the attribute:\n\u2711 internal to use the built-in API Management cache,\n\u2711 external to use the external cache as Azure Cache for Redis prefer-external to use external cache if configured or internal cache otherwise.\n\n\nBox 2: private -\ndownstream-caching-type\nThis attribute must be set to one of the following values.\n\u2711 none - downstream caching is not allowed.\n\u2711 private - downstream private caching is allowed.\n\u2711 public - private and shared downstream caching is allowed.\n\nBox 3: Authorization -\n<vary-by-header>Authorization</vary-by-header>\n<!-- should be present when allow-private-response-caching is \"true\"-->\nNote: Start caching responses per value of specified header, such as Accept, Accept-Charset, Accept-Encoding, Accept-Language, Authorization, Expect, From,\n\nHost, If-Match -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-caching-policies",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0047600005.png"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #15 -- Topic 5",
        "questionIs": "You are developing applications for a company. You plan to host the applications on Azure App Services.\nThe company has the following requirements:\n\u2711 Every five minutes verify that the websites are responsive.\n\u2711 Verify that the websites respond within a specified time threshold. Dependent requests such as images and JavaScript files must load properly.\n\u2711 Generate alerts if a website is experiencing issues.\n\u2711 If a website fails to load, the system must attempt to reload the site three more times.\nYou need to implement this process with the least amount of effort.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a Selenium web test and configure it to run from your workstation as a scheduled task.",
            "B. Set up a URL ping test to query the home page. Most Voted",
            "C. Create an Azure function to query the home page.",
            "D. Create a multi-step web test to query the home page.",
            "E. Create a Custom Track Availability Test to query the home page."
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "You can monitor a recorded sequence of URLs and interactions with a website via multi-step web tests.\nIncorrect Answers:\nA: Selenium is an umbrella project for a range of tools and libraries that enable and support the automation of web browsers.\nIt provides extensions to emulate user interaction with browsers, a distribution server for scaling browser allocation, and the infrastructure for implementations of the W3C WebDriver specification that lets you write interchangeable code for all major web browsers.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-multistep",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #16 -- Topic 5",
        "questionIs": "You develop and add several functions to an Azure Function app that uses the latest runtime host. The functions contain several REST API endpoints secured by using SSL. The Azure Function app runs in a Consumption plan.\nYou must send an alert when any of the function endpoints are unavailable or responding too slowly.\nYou need to monitor the availability and responsiveness of the functions.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a URL ping test.",
            "B. Create a timer triggered function that calls TrackAvailability() and send the results to Application Insights. Most Voted",
            "C. Create a timer triggered function that calls GetMetric(\"Request Size\") and send the results to Application Insights.",
            "D. Add a new diagnostic setting to the Azure Function app. Enable the FunctionAppLogs and Send to Log Analytics options."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "You can create an Azure Function with TrackAvailability() that will run periodically according to the configuration given in TimerTrigger function with your own business logic. The results of this test will be sent to your Application Insights resource, where you will be able to query for and alert on the availability results data.\nThis allows you to create customized tests similar to what you can do via Availability Monitoring in the portal. Customized tests will allow you to write more complex availability tests than is possible using the portal UI, monitor an app inside of your Azure VNET, change the endpoint address, or create an availability test even if this feature is not available in your region.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/availability-azure-functions",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #17 -- Topic 5",
        "questionIs": "DRAG DROP -\nYou are developing an application to retrieve user profile information. The application will use the Microsoft Graph SDK.\nThe app must retrieve user profile information by using a Microsoft Graph API call.\nYou need to call the Microsoft Graph API from the application.\nIn which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0047900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Register the application with the Microsoft identity platform.\nTo authenticate with the Microsoft identity platform endpoint, you must first register your app at the Azure app registration portal\nStep 2: Build a client by using the client app ID\nStep 3: Create an authentication provider\nCreate an authentication provider by passing in a client application and graph scopes.\nCode example:\nDeviceCodeProvider authProvider = new DeviceCodeProvider(publicClientApplication, graphScopes);\n// Create a new instance of GraphServiceClient with the authentication provider.\nGraphServiceClient graphClient = new GraphServiceClient(authProvider);\nStep 4: Create a new instance of the GraphServiceClient\nStep 5: Invoke the request to the Microsoft Graph API\nReference:\nhttps://docs.microsoft.com/en-us/graph/auth-v2-service\nhttps://docs.microsoft.com/en-us/graph/sdks/create-client",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Register the application with the Microsoft identity platform.\nTo authenticate with the Microsoft identity platform endpoint, you must first register your app at the Azure app registration portal\nStep 2: Build a client by using the client app ID\nStep 3: Create an authentication provider\nCreate an authentication provider by passing in a client application and graph scopes.\nCode example:\nDeviceCodeProvider authProvider = new DeviceCodeProvider(publicClientApplication, graphScopes);\n// Create a new instance of GraphServiceClient with the authentication provider.\nGraphServiceClient graphClient = new GraphServiceClient(authProvider);\nStep 4: Create a new instance of the GraphServiceClient\nStep 5: Invoke the request to the Microsoft Graph API\nReference:\nhttps://docs.microsoft.com/en-us/graph/auth-v2-service\nhttps://docs.microsoft.com/en-us/graph/sdks/create-client",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #18 -- Topic 5",
        "questionIs": "DRAG DROP -\nYou develop and deploy an Azure Logic App that calls an Azure Function app. The Azure Function App includes an OpenAPI (Swagger) definition and uses an\nAzure Blob storage account. All resources are secured by using Azure Active Directory (Azure AD).\nThe Logic App must use Azure Monitor logs to record and store information about runtime data and events. The logs must be stored in the Azure Blob storage account.\nYou need to set up Azure Monitor logs and collect diagnostics data for the Azure Logic App.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0048100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Create a Log Analytics workspace\nBefore you start, you need a Log Analytics workspace.\nStep 2: Install the Logic Apps Management solution\nTo set up logging for your logic app, you can enable Log Analytics when you create your logic app, or you can install the Logic Apps Management solution in your\nLog Analytics workspace for existing logic apps.\nStep 3: Add a diagnostic setting to the Azure Logic App\n\nSet up Azure Monitor logs -\n1. In the Azure portal, find and select your logic app.\n2. On your logic app menu, under Monitoring, select Diagnostic settings > Add diagnostic setting.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/monitor-logic-apps-log-analytics",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Create a Log Analytics workspace\nBefore you start, you need a Log Analytics workspace.\nStep 2: Install the Logic Apps Management solution\nTo set up logging for your logic app, you can enable Log Analytics when you create your logic app, or you can install the Logic Apps Management solution in your\nLog Analytics workspace for existing logic apps.\nStep 3: Add a diagnostic setting to the Azure Logic App\n\nSet up Azure Monitor logs -\n1. In the Azure portal, find and select your logic app.\n2. On your logic app menu, under Monitoring, select Diagnostic settings > Add diagnostic setting.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/monitor-logic-apps-log-analytics",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #19 -- Topic 5",
        "questionIs": "DRAG DROP -\nYou develop an application. You plan to host the application on a set of virtual machines (VMs) in Azure.\nYou need to configure Azure Monitor to collect logs from the application.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0048300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Create a Log Analytics workspace.\nFirst create the workspace.\nStep 2: Add a VMInsights solution.\nBefore a Log Analytics workspace can be used with VM insights, it must have the VMInsights solution installed.\nStep 3: Install agents on the VM and VM scale set to be monitored.\nPrior to onboarding agents, you must create and configure a workspace. Install or update the Application Insights Agent as an extension for Azure virtual machines and VM scalet sets.\nStep 4: Create an Application Insights resource\nSign in to the Azure portal, and create an Application Insights resource.\n\nOnce a workspace-based Application Insights resource has been created, configuring monitoring is relatively straightforward.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-configure-workspace https://docs.microsoft.com/en-us/azure/azure-monitor/app/create-workspace-resourcehttps://www.examtopics.com/assets/media/exam-media/04273/0048500001.jpg",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Create a Log Analytics workspace.\nFirst create the workspace.\nStep 2: Add a VMInsights solution.\nBefore a Log Analytics workspace can be used with VM insights, it must have the VMInsights solution installed.\nStep 3: Install agents on the VM and VM scale set to be monitored.\nPrior to onboarding agents, you must create and configure a workspace. Install or update the Application Insights Agent as an extension for Azure virtual machines and VM scalet sets.\nStep 4: Create an Application Insights resource\nSign in to the Azure portal, and create an Application Insights resource.\n\nOnce a workspace-based Application Insights resource has been created, configuring monitoring is relatively straightforward.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-configure-workspace https://docs.microsoft.com/en-us/azure/azure-monitor/app/create-workspace-resource",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0048500001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #20 -- Topic 5",
        "questionIs": "You develop and deploy an Azure App Service web app. The app is deployed to multiple regions and uses Azure Traffic Manager. Application Insights is enabled for the app.\nYou need to analyze app uptime for each month.\nWhich two solutions will achieve the goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure Monitor logs Most Voted",
            "B. Application Insights alerts Most Voted",
            "C. Azure Monitor metrics Most Voted Most Voted",
            "D. Application Insights web tests"
        ],
        "answersAre": [
            "B",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "B",
            "C"
        ],
        "descriptionIs": "Reference:\nhttps://azure.microsoft.com/en-us/blog/creating-a-web-test-alert-programmatically-with-application-insights/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #21 -- Topic 5",
        "questionIs": "DRAG DROP -\nYou develop and deploy an Azure App Service web app. The web app accesses data in an Azure SQL database.\nYou must update the web app to store frequently used data in a new Azure Cache for Redis Premium instance.\nYou need to implement the Azure Cache for Redis features.\nWhich feature should you implement? To answer, drag the appropriate feature to the correct requirements. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0048700001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://www.red-gate.com/simple-talk/development/dotnet-development/overview-of-azure-cache-for-redis/ https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://www.red-gate.com/simple-talk/development/dotnet-development/overview-of-azure-cache-for-redis/ https://docs.microsoft.com/en-us/azure/architecture/best-practices/caching",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #22 -- Topic 5",
        "questionIs": "You are developing an ASP.NET Core Web API web service. The web service uses Azure Application Insights for all telemetry and dependency tracking. The web service reads and writes data to a database other than Microsoft SQL Server.\nYou need to ensure that dependency tracking works for calls to the third-party database.\nWhich two dependency telemetry properties should you use? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Telemetry.Context.Cloud.RoleInstance",
            "B. Telemetry.Id",
            "C. Telemetry.Name",
            "D. Telemetry.Context.Operation.Id",
            "E. Telemetry.Context.Session.Id"
        ],
        "answersAre": [
            "B",
            "D"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Example:\npublic async Task Enqueue(string payload)\n{\n// StartOperation is a helper method that initializes the telemetry item\n// and allows correlation of this operation with its parent and children. var operation = telemetryClient.StartOperation<DependencyTelemetry>(\"enqueue \" + queueName);\n\noperation.Telemetry.Type = \"Azure Service Bus\";\noperation.Telemetry.Data = \"Enqueue \" + queueName;\nvar message = new BrokeredMessage(payload);\n// Service Bus queue allows the property bag to pass along with the message.\n// We will use them to pass our correlation identifiers (and other context)\n// to the consumer.\nmessage.Properties.Add(\"ParentId\", operation.Telemetry.Id);\nmessage.Properties.Add(\"RootId\", operation.Telemetry.Context.Operation.Id);\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-operations-tracking",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #23 -- Topic 5",
        "questionIs": "HOTSPOT -\nYou are using Azure Front Door Service.\nYou are expecting inbound files to be compressed by using Brotli compression. You discover that inbound XML files are not compressed. The files are 9 megabytes (MB) in size.\nYou need to determine the root cause for the issue.\nTo answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0048900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: No -\nFront Door can dynamically compress content on the edge, resulting in a smaller and faster response to your clients. All files are eligible for compression.\nHowever, a file must be of a MIME type that is eligible for compression list.\n\nBox 2: No -\nSometimes you may wish to purge cached content from all edge nodes and force them all to retrieve new updated assets. This might be due to updates to your web application, or to quickly update assets that contain incorrect information.\n\nBox 3: Yes -\nThese profiles support the following compression encodings: Gzip (GNU zip), Brotli\nReference:\nhttps://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: No -\nFront Door can dynamically compress content on the edge, resulting in a smaller and faster response to your clients. All files are eligible for compression.\nHowever, a file must be of a MIME type that is eligible for compression list.\n\nBox 2: No -\nSometimes you may wish to purge cached content from all edge nodes and force them all to retrieve new updated assets. This might be due to updates to your web application, or to quickly update assets that contain incorrect information.\n\nBox 3: Yes -\nThese profiles support the following compression encodings: Gzip (GNU zip), Brotli\nReference:\nhttps://docs.microsoft.com/en-us/azure/frontdoor/front-door-caching",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #24 -- Topic 5",
        "questionIs": "HOTSPOT -\nYou are developing an Azure App Service hosted ASP.NET Core web app to deliver video-on-demand streaming media. You enable an Azure Content Delivery\nNetwork (CDN) Standard for the web endpoint. Customer videos are downloaded from the web app by using the following example URL: http://www.contoso.com/ content.mp4?quality=1.\nAll media content must expire from the cache after one hour. Customer videos with varying quality must be delivered to the closest regional point of presence\n(POP) node.\nYou need to configure Azure CDN caching rules.\nWhich options should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0049200001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Override -\nOverride: Ignore origin-provided cache duration; use the provided cache duration instead. This will not override cache-control: no-cache.\nSet if missing: Honor origin-provided cache-directive headers, if they exist; otherwise, use the provided cache duration.\nIncorrect:\nBypass cache: Do not cache and ignore origin-provided cache-directive headers.\n\nBox 2: 1 hour -\nAll media content must expire from the cache after one hour.\n\nBox 3: Cache every unique URL -\nCache every unique URL: In this mode, each request with a unique URL, including the query string, is treated as a unique asset with its own cache. For example, the response from the origin server for a request for example.ashx?q=test1 is cached at the POP node and returned for subsequent caches with the same query string. A request for example.ashx?q=test2 is cached as a separate asset with its own time-to-live setting.\nIncorrect Answers:\nBypass caching for query strings: In this mode, requests with query strings are not cached at the CDN POP node. The POP node retrieves the asset directly from the origin server and passes it to the requestor with each request.\nIgnore query strings: Default mode. In this mode, the CDN point-of-presence (POP) node passes the query strings from the requestor to the origin server on the first request and caches the asset. All subsequent requests for the asset that are served from the POP ignore the query strings until the cached asset expires.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-query-string",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Override -\nOverride: Ignore origin-provided cache duration; use the provided cache duration instead. This will not override cache-control: no-cache.\nSet if missing: Honor origin-provided cache-directive headers, if they exist; otherwise, use the provided cache duration.\nIncorrect:\nBypass cache: Do not cache and ignore origin-provided cache-directive headers.\n\nBox 2: 1 hour -\nAll media content must expire from the cache after one hour.\n\nBox 3: Cache every unique URL -\nCache every unique URL: In this mode, each request with a unique URL, including the query string, is treated as a unique asset with its own cache. For example, the response from the origin server for a request for example.ashx?q=test1 is cached at the POP node and returned for subsequent caches with the same query string. A request for example.ashx?q=test2 is cached as a separate asset with its own time-to-live setting.\nIncorrect Answers:\nBypass caching for query strings: In this mode, requests with query strings are not cached at the CDN POP node. The POP node retrieves the asset directly from the origin server and passes it to the requestor with each request.\nIgnore query strings: Default mode. In this mode, the CDN point-of-presence (POP) node passes the query strings from the requestor to the origin server on the first request and caches the asset. All subsequent requests for the asset that are served from the POP ignore the query strings until the cached asset expires.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-query-string",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #25 -- Topic 5",
        "questionIs": "HOTSPOT -\nYou are developing an ASP.NET Core time sheet application that runs as an Azure Web App. Users of the application enter their time sheet information on the first day of every month.\nThe application uses a third-party web service to validate data.\nThe application encounters periodic server errors due to errors that result from calling a third-party web server. Each request to the third-party server has the same chance of failure.\nYou need to configure an Azure Monitor alert to detect server errors unrelated to the third-party service. You must minimize false-positive alerts.\nHow should you complete the Azure Resource Manager template? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0049500001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: DynamicThresholdCriterion\n\nBox 2: Http5xx -\nServer errors are in the 5xx range.\nClient errors are in the 4xx range\n\nBox 3: Low -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-dynamic-thresholds",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: DynamicThresholdCriterion\n\nBox 2: Http5xx -\nServer errors are in the 5xx range.\nClient errors are in the 4xx range\n\nBox 3: Low -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-dynamic-thresholds",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #26 -- Topic 5",
        "questionIs": "You are developing a web application that uses Azure Cache for Redis. You anticipate that the cache will frequently fill and that you will need to evict keys.\nYou must configure Azure Cache for Redis based on the following predicted usage pattern: A small subset of elements will be accessed much more often than the rest.\nYou need to configure the Azure Cache for Redis to optimize performance for the predicted usage pattern.\nWhich two eviction policies will achieve the goal?\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. noeviction",
            "B. allkeys-lru Most Voted",
            "C. volatile-lru Most Voted",
            "D. allkeys-random",
            "E. volatile-ttl",
            "F. volatile-random"
        ],
        "answersAre": [
            "B",
            "C"
        ],
        "mostVotedAre": [
            "B",
            "C"
        ],
        "descriptionIs": "B: The allkeys-lru policy evict keys by trying to remove the less recently used (LRU) keys first, in order to make space for the new data added. Use the allkeys-lru policy when you expect a power-law distribution in the popularity of your requests, that is, you expect that a subset of elements will be accessed far more often than the rest.\nC: volatile-lru: evict keys by trying to remove the less recently used (LRU) keys first, but only among keys that have an expire set, in order to make space for the new data added.\nNote: The allkeys-lru policy is more memory efficient since there is no need to set an expire for the key to be evicted under memory pressure.\nReference:\nhttps://redis.io/topics/lru-cache",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #27 -- Topic 5",
        "questionIs": "DRAG DROP -\nAn organization has web apps hosted in Azure.\nThe organization wants to track events and telemetry data in the web apps by using Application Insights.\nYou need to configure the web apps for Application Insights.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0049800001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Create an Application Insights resource\nCreating an Application Insights workspace-based resource us a prerequisite.\nStep 2: Copy the connection string\nA connection string identifies the resource that you want to associate with your telemetry data. It also allows you to modify the endpoints that your resource will use as a destination for your telemetry. You'll need to copy the connection string and add it to your application's code or to an environment variable.\nStep 3: Configure the Application Insights SDK in the app\nThe Application Insights SDK for ASP.NET Core can monitor your applications no matter where or how they run.\nInstall the Application Insights SDK NuGet package for ASP.NET Core.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-core",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Create an Application Insights resource\nCreating an Application Insights workspace-based resource us a prerequisite.\nStep 2: Copy the connection string\nA connection string identifies the resource that you want to associate with your telemetry data. It also allows you to modify the endpoints that your resource will use as a destination for your telemetry. You'll need to copy the connection string and add it to your application's code or to an environment variable.\nStep 3: Configure the Application Insights SDK in the app\nThe Application Insights SDK for ASP.NET Core can monitor your applications no matter where or how they run.\nInstall the Application Insights SDK NuGet package for ASP.NET Core.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-core",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #28 -- Topic 5",
        "questionIs": "An organization hosts web apps in Azure. The organization uses Azure Monitor.\nYou discover that configuration changes were made to some of the web apps.\nYou need to identify the configuration changes.\nWhich Azure Monitor log should you review?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. AppServiceAppLogs",
            "B. AppServiceEnvironmentPlatformlogs",
            "C. AppServiceConsoleLogs",
            "D. AppServiceAuditLogs"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "The log type AppServiceEnvironmentPlatformLogs handles the App Service Environment: scaling, configuration changes, and status logs.\nIncorrect:\nAppServiceAppLogs contains logs generated through your application.\nAppServiceAuditLogs logs generated when publishing users successfully log on via one of the App Service publishing protocols.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #29 -- Topic 5",
        "questionIs": "You develop and deploy an Azure App Service web app to a production environment. You enable the Always On setting and the Application Insights site extensions.\nYou deploy a code update and receive multiple failed requests and exceptions in the web app.\nYou need to validate the performance and failure counts of the web app in near real time.\nWhich Application Insights tool should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Profiler",
            "B. Smart Detection",
            "C. Live Metrics Stream Most Voted",
            "D. Application Map",
            "E. Snapshot Debugger"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Live Metrics Stream -\nDeploying the latest build can be an anxious experience. If there are any problems, you want to know about them right away, so that you can back out if necessary. Live Metrics Stream gives you key metrics with a latency of about one second.\nWith Live Metrics Stream, you can:\n* Validate a fix while it's released, by watching performance and failure counts.\n* Etc.\n\nIncorrect:\n* Profiler\nAzure Application Insights Profiler provides performance traces for applications running in production in Azure. Profiler:\nCaptures the data automatically at scale without negatively affecting your users.\nHelps you identify the \u05d2\u20achot\u05d2\u20ac code path spending the most time handling a particular web request.\n* Snapshot debugger\nWhen an exception occurs, you can automatically collect a debug snapshot from your live web application. The snapshot shows the state of source code and variables at the moment the exception was thrown. The Snapshot Debugger in Azure Application Insights monitors exception telemetry from your web app. It collects snapshots on your top-throwing exceptions so that you have the information you need to diagnose issues in production.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/live-stream",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0050100001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #30 -- Topic 5",
        "questionIs": "HOTSPOT -\nYou deploy an ASP.NET web app to Azure App Service.\nYou must monitor the web app by using Application Insights.\nYou need to configure Application Insights to meet the requirements.\nWhich feature should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0050200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Smart Detection -\nSmart detection automatically warns you of potential performance problems and failure anomalies in your web application. It performs proactive analysis of the telemetry that your app sends to Application Insights. If there is a sudden rise in failure rates, or abnormal patterns in client or server performance, you get an alert. This feature needs no configuration. It operates if your application sends enough telemetry.\n\nBox 2: Snapshot Debugger -\nWhen an exception occurs, you can automatically collect a debug snapshot from your live web application. The snapshot shows the state of source code and variables at the moment the exception was thrown. The Snapshot Debugger in Azure Application Insights monitors exception telemetry from your web app. It collects snapshots on your top-throwing exceptions so that you have the information you need to diagnose issues in production.\n\nBox 3: Profiler -\nAzure Application Insights Profiler provides performance traces for applications running in production in Azure. Profiler:\nCaptures the data automatically at scale without negatively affecting your users.\nHelps you identify the \u05d2\u20achot\u05d2\u20ac code path spending the most time handling a particular web request.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/proactive-diagnostics https://docs.microsoft.com/en-us/azure/azure-monitor/snapshot-debugger/snapshot-debugger https://docs.microsoft.com/en-us/azure/azure-monitor/profiler/profiler-overview",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Smart Detection -\nSmart detection automatically warns you of potential performance problems and failure anomalies in your web application. It performs proactive analysis of the telemetry that your app sends to Application Insights. If there is a sudden rise in failure rates, or abnormal patterns in client or server performance, you get an alert. This feature needs no configuration. It operates if your application sends enough telemetry.\n\nBox 2: Snapshot Debugger -\nWhen an exception occurs, you can automatically collect a debug snapshot from your live web application. The snapshot shows the state of source code and variables at the moment the exception was thrown. The Snapshot Debugger in Azure Application Insights monitors exception telemetry from your web app. It collects snapshots on your top-throwing exceptions so that you have the information you need to diagnose issues in production.\n\nBox 3: Profiler -\nAzure Application Insights Profiler provides performance traces for applications running in production in Azure. Profiler:\nCaptures the data automatically at scale without negatively affecting your users.\nHelps you identify the \u05d2\u20achot\u05d2\u20ac code path spending the most time handling a particular web request.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/proactive-diagnostics https://docs.microsoft.com/en-us/azure/azure-monitor/snapshot-debugger/snapshot-debugger https://docs.microsoft.com/en-us/azure/azure-monitor/profiler/profiler-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #31 -- Topic 5",
        "questionIs": "You are building a web application that performs image analysis on user photos and returns metadata containing objects identified. The image analysis is very costly in terms of time and compute resources. You are planning to use Azure Redis Cache so duplicate uploads do not need to be reprocessed.\nIn case of an Azure data center outage, metadata loss must be kept to a minimum.\nYou need to configure the Azure Redis cache instance.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure Azure Redis with AOF persistence. Most Voted",
            "B. Configure Azure Redis with RDB persistence.",
            "C. Configure second storage account for persistence. Most Voted",
            "D. Set backup frequency to the minimum value."
        ],
        "answersAre": [
            "B",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "C"
        ],
        "descriptionIs": "RDB persistence - When you use RDB persistence, Azure Cache for Redis persists a snapshot of your cache in a binary format. The snapshot is saved in an\nAzure Storage account. The configurable backup frequency determines how often to persist the snapshot. If a catastrophic event occurs that disables both the primary and replica cache, the cache is reconstructed using the most recent snapshot.\nNote: Azure Cache for Redis supports zone redundant configurations in the Premium and Enterprise tiers. A zone redundant cache can place its nodes across different Azure Availability Zones in the same region. It eliminates data center or AZ outage as a single point of failure and increases the overall availability of your cache.\nIncorrect:\nNot A: Zone redundancy doesn't support AOF persistence or work with geo-replication currently.\nNot C: No need for a second storage account.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-cache-for-redis/cache-how-to-premium-persistence",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #32 -- Topic 5",
        "questionIs": "You are developing an Azure-based web application. The application goes offline periodically to perform offline data processing. While the application is offline, numerous Azure Monitor alerts fire which result in the on-call developer being paged.\nThe application must always log when the application is offline for any reason.\nYou need to ensure that the on-call developer is not paged during offline processing.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Add Azure Monitor alert processing rules to suppress notifications. Most Voted",
            "B. Disable Azure Monitor Service Health Alerts during offline processing.",
            "C. Create an Azure Monitor Metric Alert.",
            "D. Build an Azure Monitor action group that suppresses the alerts."
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "You can use alert processing rules to add action groups or remove (suppress) action groups from your fired alerts.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-action-rules",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #33 -- Topic 5",
        "questionIs": "You are developing an online game that includes a feature that allows players to interact with other players on the same team within a certain distance. The calculation to determine the players in range occurs when players move and are cached in an Azure Cache for Redis instance.\n\nThe system should prioritize players based on how recently they have moved and should not prioritize players who have logged out of the game.\n\nYou need to select an eviction policy.\n\nWhich eviction policy should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. allkeys-Iru",
            "B. volatile-Iru Most Voted",
            "C. allkeys-lfu",
            "D. volatile-ttl"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #34 -- Topic 5",
        "questionIs": "You develop an Azure App Service web app and deploy to a production environment. You enable Application Insights for the web app.\n\nThe web app is throwing multiple exceptions in the environment.\n\nYou need to examine the state of the source code and variables when the exceptions are thrown.\n\nWhich Application Insights feature should you configure?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Smart detection",
            "B. Profiler",
            "C. Snapshot Debugger Most Voted",
            "D. Standard test"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #35 -- Topic 5",
        "questionIs": "DRAG DROP\n-\n\nYou develop and deploy a Java application to Azure. The application has been instrumented by using the Application Insights SDK.\n\nThe telemetry data must be enriched and processed before it is sent to the Application Insights service.\n\nYou need to modify the telemetry data.\n\nWhich Application Insights SDK features should you use? To answer, drag the appropriate features to the correct requirements. Each feature may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image410.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #36 -- Topic 5",
        "questionIs": "HOTSPOT\n-\n\nYou develop new functionality in a web application for a company that provides access to seismic data from around the world. The seismic data is stored in Redis Streams within an Azure Cache for Redis instance.\n\nThe new functionality includes a real-time display of seismic events as they occur.\n\nYou need to implement the Azure Cache for Redis command to receive seismic data.\n\nHow should you complete the command? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image412.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #37 -- Topic 5",
        "questionIs": "You develop an ASP.NET Core app that uses Azure App Configuration. You also create an App Configuration containing 100 settings.\n\nThe app must meet the following requirements:\n\n\u2022 Ensure the consistency of all configuration data when changes to individual settings occur.\n\u2022 Handle configuration data changes dynamically without causing the application to restart.\n\u2022 Reduce the overall number of requests made to App Configuration APIs.\n\nYou must implement dynamic configuration updates in the app.\n\nWhat are two ways to achieve this goal? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create and register a sentinel key in the App Configuration store. Set the refreshAll parameter of the Register method to true. Most Voted",
            "B. Increase the App Configuration cache expiration from the default value. Most Voted",
            "C. Decrease the App Configuration cache expiration from the default value.",
            "D. Create and configure Azure Key Vault. Implement the Azure Key Vault configuration provider.",
            "E. Register all keys in the App Configuration store. Set the refreshAll parameter of the Register method to false.",
            "F. Create and implement environment variables for each App Configuration store setting."
        ],
        "answersAre": [
            "A",
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #38 -- Topic 5",
        "questionIs": "HOTSPOT\n-\n\nYou develop and deploy an Azure App Service web app that connects to Azure Cache for Redis as a content cache. All resources have been deployed to the East US 2 region.\n\nThe security team requires the following audit information from Azure Cache for Redis:\n\n\u2022 The number of Redis client connections from an associated IP address.\n\u2022 Redis operations completed on the content cache.\n\u2022 The location (region) in which the Azure Cach3e for Redis instance was accessed.\n\nThe audit information must be captured and analyzed by a security team application deployed to the Central US region.\n\nYou need to log information on all client connections to the cache.\n\nWhich configuration values should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image428.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #39 -- Topic 5",
        "questionIs": "You develop and deploy a web app to Azure App Service. The Azure App Service uses a Basic plan in a single region.\n\nUsers report that the web app is responding slow. You must capture the complete call stack to help identify performance issues in the code. Call stack data must be correlated across app instances. You must minimize cost and impact to users on the web app.\n\nYou need to capture the telemetry.\n\nWhich three actions should you perform? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Restart all apps in the App Service plan.",
            "B. Enable Application Insights site extensions. Most Voted Most Voted",
            "C. Upgrade the Azure App Service plan to Premium.",
            "D. Enable Profiler. Most Voted Most Voted",
            "E. Enable the Always On setting for the app service. Most Voted",
            "F. Enable Snapshot debugger. Most Voted",
            "G. Enable remote debugging."
        ],
        "answersAre": [
            "D",
            "E",
            "F"
        ],
        "mostVotedAre": [
            "B",
            "D",
            "E",
            "F"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #40 -- Topic 5",
        "questionIs": "You are building an application to track cell towers that are available to phones in near real time. A phone will send information to the application by using the Azure Web PubSub service. The data will be processed by using an Azure Functions app. Traffic will be transmitted by using a content delivery network (CDN).\n\nThe Azure function must be protected against misconfigured or unauthorized invocations.\n\nYou need to ensure that the CDN allows for the Azure function protection.\n\nWhich HTTP header should be on the allowed list?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Authorization",
            "B. WebHook-Request-Callback",
            "C. Resource",
            "D. WebHook-Request-Origin Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #41 -- Topic 5",
        "questionIs": "You are developing an Azure App Service web app.\n\nThe web app must securely store session information in Azure Redis Cache.\n\nYou need to connect the web app to Azure Redis Cache.\n\nWhich three Azure Redis Cache properties should you use? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Access key",
            "B. SSL port",
            "C. Subscription name",
            "D. Location",
            "E. Host name",
            "F. Subscription id"
        ],
        "answersAre": [
            "A",
            "B",
            "E"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #42 -- Topic 5",
        "questionIs": "HOTSPOT\n-\n\nYou are developing several microservices to run on Azure Container Apps.\n\nYou need to monitor and diagnose the microservices.\n\nWhich features should you use? To answer, select the appropriate feature in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image455.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #43 -- Topic 5",
        "questionIs": "Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\n\nCurrent environment -\n\n\nCorporate website -\n\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\n\nRetail Store Locations -\n\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n\nCorporate website -\n\n\u2022 Secure the website by using SSL.\n\u2022 Minimize costs for data storage and hosting.\n\u2022 Implement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\n\u2022 Distribute the website content globally for local use.\n\u2022 Implement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\n\u2022 The website must have 99.95 percent uptime.\n\n\nRetail store locations -\n\n\u2022 Azure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\n\u2022 Audit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\n\nDelivery services -\n\n\u2022 Store service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\n\u2022 Store delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\n\nInventory services -\n\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\n\nSecurity -\n\n\u2022 All Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\n\u2022 Authentication and authorization must use Azure AD and services must use managed identities where possible.\n\n\nIssues -\n\n\nRetail Store Locations -\n\n\u2022 You must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\n\u2022 Azure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\n\n\nYou need to test the availability of the corporate website.\n\nWhich two test types can you use? Each correct answer presents a complete solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Standard Most Voted",
            "B. URL ping",
            "C. Custom testing using the TrackAvailability API method Most Voted",
            "D. Multi-step"
        ],
        "answersAre": [
            "A",
            "C"
        ],
        "mostVotedAre": [
            "A",
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #44 -- Topic 5",
        "questionIs": "You have an Azure API Management (APIM) Standard tier instance named APIM1 that uses a managed gateway.\n\nYou plan to use APIM1 to publish an API named API1 that uses a backend database that supports only a limited volume of requests per minute. You also need a policy for API1 that will minimize the possibility that the number of requests to the backend database from an individual IP address you specify exceeds the supported limit.\n\nYou need to identify a policy for API1 that will meet the requirements.\n\nWhich policy should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. ip-filter",
            "B. quota-by-key",
            "C. rate-limit-by-key Most Voted",
            "D. rate-limit"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #45 -- Topic 5",
        "questionIs": "You develop a web application that sells access to last-minute openings for child camps that run on the weekends. The application uses Azure Application Insights for all alerting and monitoring.\n\nThe application must alert operators when a technical issue is preventing sales to camps.\n\nYou need to build an alert to detect technical issues.\n\nWhich alert type should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Metric alert using multiple time series",
            "B. Metric alert using dynamic thresholds Most Voted",
            "C. Log alert using multiple time series",
            "D. Log alert using dynamic thresholds"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #46 -- Topic 5",
        "questionIs": "Case study -\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground -\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment -\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website -\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms -\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors -\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements -\n\nThe application components must meet the following requirements:\n\n\nCorporate website -\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms -\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors -\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff -\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity -\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues -\n\n\nCorporate website -\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors -\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to implement an aggregate of telemetry values for distributor API calls.\n\nWhich Application Insights API method should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. TrackEvent",
            "B. TrackDependency",
            "C. TrackMetric",
            "D. TrackException",
            "E. TrackTrace"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #47 -- Topic 5",
        "questionIs": "DRAG DROP\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment\n-\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website\n-\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms\n-\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors\n-\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms\n-\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors\n-\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff\n-\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity\n-\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues\n-\n\n\nCorporate website\n-\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors\n-\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to correct the internal staff issue with webpages.\n\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image513.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #48 -- Topic 5",
        "questionIs": "HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment\n-\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website\n-\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms\n-\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors\n-\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms\n-\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors\n-\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff\n-\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity\n-\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues\n-\n\n\nCorporate website\n-\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors\n-\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to correct the errors for farmers and distributors.\n\nWhich solution should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image515.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #49 -- Topic 5",
        "questionIs": "HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment\n-\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website\n-\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms\n-\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors\n-\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms\n-\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors\n-\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff\n-\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity\n-\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues\n-\n\n\nCorporate website\n-\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors\n-\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to resolve the authentication errors for developers.\n\nWhich Service Bus security configuration should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image517.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\nYou must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Service Bus. Configure a topic to receive the device data by using a correlation filter.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "A message is raw data produced by a service to be consumed or stored elsewhere. The Service Bus is for high-value enterprise messaging, and is used for order processing and financial transactions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\nYou must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Grid. Configure event filtering to evaluate the device identifier.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Instead use an Azure Service Bus, which is used order processing and financial transactions.\nNote: An event is a lightweight notification of a condition or a state change. Event hubs is usually used reacting to status changes.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou manage several existing Logic Apps.\nYou need to change definitions, add new logic, and optimize these apps on a regular basis.\nWhat should you use? To answer, drag the appropriate tools to the correct functionalities. Each tool may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0054900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Enterprise Integration Pack\nFor business-to-business (B2B) solutions and seamless communication between organizations, you can build automated scalable enterprise integration workflows by using the Enterprise Integration Pack (EIP) with Azure Logic Apps.\n\nBox 2: Code View Editor -\n\nEdit JSON - Azure portal -\n1. Sign in to the Azure portal.\n2. From the left menu, choose All services. In the search box, find \"logic apps\", and then from the results, select your logic app.\n3. On your logic app's menu, under Development Tools, select Logic App Code View.\n4. The Code View editor opens and shows your logic app definition in JSON format.\n\nBox 3: Logic Apps Designer -\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-overview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-definitions",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Enterprise Integration Pack\nFor business-to-business (B2B) solutions and seamless communication between organizations, you can build automated scalable enterprise integration workflows by using the Enterprise Integration Pack (EIP) with Azure Logic Apps.\n\nBox 2: Code View Editor -\n\nEdit JSON - Azure portal -\n1. Sign in to the Azure portal.\n2. From the left menu, choose All services. In the search box, find \"logic apps\", and then from the results, select your logic app.\n3. On your logic app's menu, under Development Tools, select Logic App Code View.\n4. The Code View editor opens and shows your logic app definition in JSON format.\n\nBox 3: Logic Apps Designer -\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-overview https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-author-definitions",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?\nA.\n\nB.\n\nC.\n\nD.",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0055000001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055000002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055000003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "A service bus instance has already been created (Step 2 below). Next is step 3, Create a Service Bus queue.\nNote:\nSteps:\nStep 1: # Create a resource group\nresourceGroupName=\"myResourceGroup\"\naz group create --name $resourceGroupName --location eastus\nStep 2: # Create a Service Bus messaging namespace with a unique name namespaceName=myNameSpace$RANDOM az servicebus namespace create --resource-group $resourceGroupName --name $namespaceName --location eastus\nStep 3: # Create a Service Bus queue\naz servicebus queue create --resource-group $resourceGroupName --namespace-name $namespaceName --name BasicQueue\nStep 4: # Get the connection string for the namespace\nconnectionString=$(az servicebus namespace authorization-rule keys list --resource-group $resourceGroupName --namespace-name $namespaceName --name\nRootManageSharedAccessKey --query primaryConnectionString --output tsv)\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-cli",
        "mostVotedAre": [],
        "descriptionIs": "A service bus instance has already been created (Step 2 below). Next is step 3, Create a Service Bus queue.\nNote:\nSteps:\nStep 1: # Create a resource group\nresourceGroupName=\"myResourceGroup\"\naz group create --name $resourceGroupName --location eastus\nStep 2: # Create a Service Bus messaging namespace with a unique name namespaceName=myNameSpace$RANDOM az servicebus namespace create --resource-group $resourceGroupName --name $namespaceName --location eastus\nStep 3: # Create a Service Bus queue\naz servicebus queue create --resource-group $resourceGroupName --namespace-name $namespaceName --name BasicQueue\nStep 4: # Get the connection string for the namespace\nconnectionString=$(az servicebus namespace authorization-rule keys list --resource-group $resourceGroupName --namespace-name $namespaceName --name\nRootManageSharedAccessKey --query primaryConnectionString --output tsv)\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-cli",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 6",
        "questionIs": "HOTSPOT -\nYou are developing an application that uses Azure Storage Queues.\nYou have the following code:\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0055200001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055200002.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: No -\nThe QueueDescription.LockDuration property gets or sets the duration of a peek lock; that is, the amount of time that the message is locked for other receivers.\nThe maximum value for LockDuration is 5 minutes; the default value is 1 minute.\n\nBox 2: Yes -\nYou can peek at the message in the front of a queue without removing it from the queue by calling the PeekMessage method.\n\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicebus.messaging.queuedescription.lockduration",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: No -\nThe QueueDescription.LockDuration property gets or sets the duration of a peek lock; that is, the amount of time that the message is locked for other receivers.\nThe maximum value for LockDuration is 5 minutes; the default value is 1 minute.\n\nBox 2: Yes -\nYou can peek at the message in the front of a queue without removing it from the queue by calling the PeekMessage method.\n\nBox 3: Yes -\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues https://docs.microsoft.com/en-us/dotnet/api/microsoft.servicebus.messaging.queuedescription.lockduration",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\nYou need to complete the configuration.\nWhich Azure CLI or PowerShell command should you run?\nA.\n\nB.\n\nC.\n\nD.",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0055400001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055400002.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055400003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0055400004.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "A service bus instance has already been created (Step 2 below). Next is step 3, Create a Service Bus queue.\nNote:\nSteps:\nStep 1: # Create a resource group\nresourceGroupName=\"myResourceGroup\"\naz group create --name $resourceGroupName --location eastus\nStep 2: # Create a Service Bus messaging namespace with a unique name namespaceName=myNameSpace$RANDOM az servicebus namespace create --resource-group $resourceGroupName --name $namespaceName --location eastus\nStep 3: # Create a Service Bus queue\naz servicebus queue create --resource-group $resourceGroupName --namespace-name $namespaceName --name BasicQueue\nStep 4: # Get the connection string for the namespace\nconnectionString=$(az servicebus namespace authorization-rule keys list --resource-group $resourceGroupName --namespace-name $namespaceName --name\nRootManageSharedAccessKey --query primaryConnectionString --output tsv)\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-cli",
        "mostVotedAre": [],
        "descriptionIs": "A service bus instance has already been created (Step 2 below). Next is step 3, Create a Service Bus queue.\nNote:\nSteps:\nStep 1: # Create a resource group\nresourceGroupName=\"myResourceGroup\"\naz group create --name $resourceGroupName --location eastus\nStep 2: # Create a Service Bus messaging namespace with a unique name namespaceName=myNameSpace$RANDOM az servicebus namespace create --resource-group $resourceGroupName --name $namespaceName --location eastus\nStep 3: # Create a Service Bus queue\naz servicebus queue create --resource-group $resourceGroupName --namespace-name $namespaceName --name BasicQueue\nStep 4: # Get the connection string for the namespace\nconnectionString=$(az servicebus namespace authorization-rule keys list --resource-group $resourceGroupName --namespace-name $namespaceName --name\nRootManageSharedAccessKey --query primaryConnectionString --output tsv)\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-quickstart-cli",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #7 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.\nYou have the following requirements:\n\u2711 Queue size must not grow larger than 80 gigabytes (GB).\n\u2711 Use first-in-first-out (FIFO) ordering of messages.\n\u2711 Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Storage Queue from the mobile application. Create an Azure Function App that uses an Azure Storage\nQueue trigger.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Create an Azure Function App that uses an Azure Service Bus Queue trigger.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-queue-triggered-function",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #8 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou develop software solutions for a mobile delivery service. You are developing a mobile app that users can use to order from a restaurant in their area. The app uses the following workflow:\n1. A driver selects the restaurants for which they will deliver orders.\n2. Orders are sent to all available drivers in an area.\n3. Only orders for the selected restaurants will appear for the driver.\n4. The first driver to accept an order removes it from the list of available orders.\nYou need to implement an Azure Service Bus solution.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0055600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Create a single Service Bus Namespace\nTo begin using Service Bus messaging entities in Azure, you must first create a namespace with a name that is unique across Azure. A namespace provides a scoping container for addressing Service Bus resources within your application.\nBox 2: Create a Service Bus Topic for each restaurant for which a driver can receive messages.\nCreate topics.\nBox 3: Create a Service Bus subscription for each restaurant for which a driver can receive orders.\nTopics can have multiple, independent subscriptions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Create a single Service Bus Namespace\nTo begin using Service Bus messaging entities in Azure, you must first create a namespace with a name that is unique across Azure. A namespace provides a scoping container for addressing Service Bus resources within your application.\nBox 2: Create a Service Bus Topic for each restaurant for which a driver can receive messages.\nCreate topics.\nBox 3: Create a Service Bus subscription for each restaurant for which a driver can receive orders.\nTopics can have multiple, independent subscriptions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-messaging-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #9 -- Topic 6",
        "questionIs": "HOTSPOT -\nYou develop a news and blog content app for Windows devices.\nA notification must arrive on a user's device when there is a new article available for them to view.\nYou need to implement push notifications.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0055900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: NotificationHubClient -\n\nBox 2: NotificationHubClient -\nBox 3: CreateClientFromConnectionString\n// Initialize the Notification Hub\nNotificationHubClient hub = NotificationHubClient.CreateClientFromConnectionString(listenConnString, hubName);\nBox 4: SendWindowsNativeNotificationAsync\nSend the push notification.\nvar result = await hub.SendWindowsNativeNotificationAsync(windowsToastPayload);\nReference:\nhttps://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-push-notification-registration-management https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/app-service-mobile/app-service-mobile-windows-store-dotnet-get-started-push.md",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: NotificationHubClient -\n\nBox 2: NotificationHubClient -\nBox 3: CreateClientFromConnectionString\n// Initialize the Notification Hub\nNotificationHubClient hub = NotificationHubClient.CreateClientFromConnectionString(listenConnString, hubName);\nBox 4: SendWindowsNativeNotificationAsync\nSend the push notification.\nvar result = await hub.SendWindowsNativeNotificationAsync(windowsToastPayload);\nReference:\nhttps://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-push-notification-registration-management https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/app-service-mobile/app-service-mobile-windows-store-dotnet-get-started-push.md",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #10 -- Topic 6",
        "questionIs": "You are developing an Azure messaging solution.\nYou need to ensure that the solution meets the following requirements:\n\u2711 Provide transactional support.\n\u2711 Provide duplicate detection.\n\u2711 Store the messages for an unlimited period of time.\nWhich two technologies will meet the requirements? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure Service Bus Topic Most Voted",
            "B. Azure Service Bus Queue Most Voted",
            "C. Azure Storage Queue",
            "D. Azure Event Hub"
        ],
        "answersAre": [
            "A",
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "The Azure Service Bus Queue and Topic has duplicate detection.\nEnabling duplicate detection helps keep track of the application-controlled MessageId of all messages sent into a queue or topic during a specified time window.\nIncorrect Answers:\nC: There is just no mechanism that can query a Storage queue and find out if a message with the same contents is already there or was there before.\nD: Azure Event Hub does not have duplicate detection\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/duplicate-detection",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #11 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou develop a gateway solution for a public facing news API.\nThe news API back end is implemented as a RESTful service and hosted in an Azure App Service instance.\nYou need to configure back-end authentication for the API Management service instance.\nWhich target and gateway credential type should you use? To answer, drag the appropriate values to the correct parameters. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0056200001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Resource -\n\nBox 2: Client cert -\nAPI Management allows to secure access to the back-end service of an API using client certificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Resource -\n\nBox 2: Client cert -\nAPI Management allows to secure access to the back-end service of an API using client certificates.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/apimanagement/apimanagementrest/azure-api-management-rest-api-backend-entity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #12 -- Topic 6",
        "questionIs": "HOTSPOT -\nYou are creating an app that uses Event Grid to connect with other services. Your app's event data will be sent to a serverless function that checks compliance.\nThis function is maintained by your company.\nYou write a new event subscription at the scope of your resource. The event must be invalidated after a specific period of time.\nYou need to configure Event Grid.\nWhat should you do? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0056400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: SAS tokens -\nCustom topics use either Shared Access Signature (SAS) or key authentication. Microsoft recommends SAS, but key authentication provides simple programming, and is compatible with many existing webhook publishers.\nIn this case we need the expiration time provided by SAS tokens.\n\nBox 2: ValidationCode handshake -\nEvent Grid supports two ways of validating the subscription: ValidationCode handshake (programmatic) and ValidationURL handshake (manual).\nIf you control the source code for your endpoint, this method is recommended.\nIncorrect Answers:\nValidationURL handshake (manual): In certain cases, you can't access the source code of the endpoint to implement the ValidationCode handshake. For example, if you use a third-party service (like Zapier or IFTTT), you can't programmatically respond with the validation code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/security-authentication",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: SAS tokens -\nCustom topics use either Shared Access Signature (SAS) or key authentication. Microsoft recommends SAS, but key authentication provides simple programming, and is compatible with many existing webhook publishers.\nIn this case we need the expiration time provided by SAS tokens.\n\nBox 2: ValidationCode handshake -\nEvent Grid supports two ways of validating the subscription: ValidationCode handshake (programmatic) and ValidationURL handshake (manual).\nIf you control the source code for your endpoint, this method is recommended.\nIncorrect Answers:\nValidationURL handshake (manual): In certain cases, you can't access the source code of the endpoint to implement the ValidationCode handshake. For example, if you use a third-party service (like Zapier or IFTTT), you can't programmatically respond with the validation code.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/security-authentication",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #13 -- Topic 6",
        "questionIs": "HOTSPOT -\nYou are working for Contoso, Ltd.\nYou define an API Policy object by using the following XML markup:\n\nFor each of the following statements, select Yes if the statement is true. Otherwise, select No.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0056600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0056600002.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Yes -\nUse the set-backend-service policy to redirect an incoming request to a different backend than the one specified in the API settings for that operation. Syntax:\n<set-backend-service base-url=\"base URL of the backend service\" />\n\nBox 2: No -\nThe condition is on 512k, not on 256k.\n\nBox 3: No -\nThe set-backend-service policy changes the backend service base URL of the incoming request to the one specified in the policy.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-transformation-policies",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Yes -\nUse the set-backend-service policy to redirect an incoming request to a different backend than the one specified in the API settings for that operation. Syntax:\n<set-backend-service base-url=\"base URL of the backend service\" />\n\nBox 2: No -\nThe condition is on 512k, not on 256k.\n\nBox 3: No -\nThe set-backend-service policy changes the backend service base URL of the incoming request to the one specified in the policy.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-transformation-policies",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #14 -- Topic 6",
        "questionIs": "You are developing a solution that will use Azure messaging services.\nYou need to ensure that the solution uses a publish-subscribe model and eliminates the need for constant polling.\nWhat are two possible ways to achieve the goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Service Bus Most Voted",
            "B. Event Hub",
            "C. Event Grid Most Voted",
            "D. Queue"
        ],
        "answersAre": [
            "A",
            "C"
        ],
        "mostVotedAre": [
            "A",
            "C"
        ],
        "descriptionIs": "It is strongly recommended to use available messaging products and services that support a publish-subscribe model, rather than building your own. In Azure, consider using Service Bus or Event Grid. Other technologies that can be used for pub/sub messaging include Redis, RabbitMQ, and Apache Kafka.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #15 -- Topic 6",
        "questionIs": "A company is implementing a publish-subscribe (Pub/Sub) messaging component by using Azure Service Bus. You are developing the first subscription application.\nIn the Azure portal you see that messages are being sent to the subscription for each topic. You create and initialize a subscription client object by supplying the correct details, but the subscription application is still not consuming the messages.\nYou need to ensure that the subscription client processes all messages.\nWhich code segment should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. await subscriptionClient.AddRuleAsync(new RuleDescription(RuleDescription.DefaultRuleName, new TrueFilter()));",
            "B. subscriptionClient = new SubscriptionClient(ServiceBusConnectionString, TopicName, SubscriptionName);",
            "C. await subscriptionClient.CloseAsync();",
            "D. subscriptionClient.RegisterMessageHandler(ProcessMessagesAsync, messageHandlerOptions); Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Using topic client, call RegisterMessageHandler which is used to receive messages continuously from the entity. It registers a message handler and begins a new thread to receive messages. This handler is waited on every time a new message is received by the receiver. subscriptionClient.RegisterMessageHandler(ReceiveMessagesAsync, messageHandlerOptions);\nReference:\nhttps://www.c-sharpcorner.com/article/azure-service-bus-topic-and-subscription-pub-sub/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #16 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.\nYou have the following requirements:\n\u2711 Queue size must not grow larger than 80 gigabytes (GB).\n\u2711 Use first-in-first-out (FIFO) ordering of messages.\n\u2711 Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Storage Queue from the mobile application. Create an Azure VM that is triggered from Azure Storage\nQueue events.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus Queue trigger.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-queue-triggered-function",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #17 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.\nYou have the following requirements:\n\u2711 Queue size must not grow larger than 80 gigabytes (GB).\n\u2711 Use first-in-first-out (FIFO) ordering of messages.\n\u2711 Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Service Bus Queue from the mobile application. Create an Azure Windows VM that is triggered from\nAzure Service Bus Queue.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Don't use a VM, instead create an Azure Function App that uses an Azure Service Bus Queue trigger.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-queue-triggered-function",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #18 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou are developing a REST web service. Customers will access the service by using an Azure API Management instance.\nThe web service does not correctly handle conflicts. Instead of returning an HTTP status code of 409, the service returns a status code of 500. The body of the status message contains only the word conflict.\nYou need to ensure that conflicts produce the correct response.\nHow should you complete the policy? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0057100001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: on-error -\nPolicies in Azure API Management are divided into inbound, backend, outbound, and on-error.\nIf there is no on-error section, callers will receive 400 or 500 HTTP response messages if an error condition occurs.\n\nBox 2: context -\n\nBox 3: context -\n\nBox 4: set-status -\nThe return-response policy aborts pipeline execution and returns either a default or custom response to the caller. Default response is 200 OK with no body.\nCustom response can be specified via a context variable or policy statements.\nSyntax:\n<return-response response-variable-name=\"existing context variable\">\n<set-header/>\n<set-body/>\n<set-status/>\n</return-response>\n\nBox 5: on-error -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-error-handling-policies https://docs.microsoft.com/en-us/azure/api-management/api-management-transformation-policies",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: on-error -\nPolicies in Azure API Management are divided into inbound, backend, outbound, and on-error.\nIf there is no on-error section, callers will receive 400 or 500 HTTP response messages if an error condition occurs.\n\nBox 2: context -\n\nBox 3: context -\n\nBox 4: set-status -\nThe return-response policy aborts pipeline execution and returns either a default or custom response to the caller. Default response is 200 OK with no body.\nCustom response can be specified via a context variable or policy statements.\nSyntax:\n<return-response response-variable-name=\"existing context variable\">\n<set-header/>\n<set-body/>\n<set-status/>\n</return-response>\n\nBox 5: on-error -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-error-handling-policies https://docs.microsoft.com/en-us/azure/api-management/api-management-transformation-policies",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #19 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou are a developer for a Software as a Service (SaaS) company. You develop solutions that provide the ability to send notifications by using Azure Notification\nHubs.\nYou need to create sample code that customers can use as a reference for how to send raw notifications to Windows Push Notification Services (WNS) devices.\nThe sample code must not use external packages.\nHow should you complete the code segment? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0057300001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: windows -\nExample code:\nvar request = new HttpRequestMessage(method, $\"{resourceUri}?api-version=2017-04\"); request.Headers.Add(\"Authorization\", createToken(resourceUri, KEY_NAME,\nKEY_VALUE));\nrequest.Headers.Add(\"X-WNS-Type\", \"wns/raw\");\nrequest.Headers.Add(\"ServiceBusNotification-Format\", \"windows\"); return request;\n\nBox 2: application/octet-stream -\nExample code capable of sending a raw notification:\nstring resourceUri = $\"https://{NH_NAMESPACE}.servicebus.windows.net/{HUB_NAME}/messages/\"; using (var request = CreateHttpRequest(HttpMethod.Post, resourceUri))\n{\nrequest.Content = new StringContent(content, Encoding.UTF8,\n\"application/octet-stream\");\nrequest.Content.Headers.ContentType.CharSet = string.Empty;\nvar httpClient = new HttpClient();\nvar response = await httpClient.SendAsync(request);\nConsole.WriteLine(response.StatusCode);\n}\nReference:\nhttps://stackoverflow.com/questions/31346714/how-to-send-raw-notification-to-azure-notification-hub/31347901",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: windows -\nExample code:\nvar request = new HttpRequestMessage(method, $\"{resourceUri}?api-version=2017-04\"); request.Headers.Add(\"Authorization\", createToken(resourceUri, KEY_NAME,\nKEY_VALUE));\nrequest.Headers.Add(\"X-WNS-Type\", \"wns/raw\");\nrequest.Headers.Add(\"ServiceBusNotification-Format\", \"windows\"); return request;\n\nBox 2: application/octet-stream -\nExample code capable of sending a raw notification:\nstring resourceUri = $\"https://{NH_NAMESPACE}.servicebus.windows.net/{HUB_NAME}/messages/\"; using (var request = CreateHttpRequest(HttpMethod.Post, resourceUri))\n{\nrequest.Content = new StringContent(content, Encoding.UTF8,\n\"application/octet-stream\");\nrequest.Content.Headers.ContentType.CharSet = string.Empty;\nvar httpClient = new HttpClient();\nvar response = await httpClient.SendAsync(request);\nConsole.WriteLine(response.StatusCode);\n}\nReference:\nhttps://stackoverflow.com/questions/31346714/how-to-send-raw-notification-to-azure-notification-hub/31347901",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #20 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce\n2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\nYou must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Event Hub. Configure the machine identifier as the partition key and enable capture.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-programming-guide",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #21 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou are developing an Azure solution to collect inventory data from thousands of stores located around the world. Each store location will send the inventory data hourly to an Azure Blob storage account for processing.\nThe solution must meet the following requirements:\n\u2711 Begin processing when data is saved to Azure Blob storage.\n\u2711 Filter data based on store location information.\n\u2711 Trigger an Azure Logic App to process the data for output to Azure Cosmos DB.\n\u2711 Enable high availability and geographic distribution.\n\u2711 Allow 24-hours for retries.\n\u2711 Implement an exponential back off data processing.\nYou need to configure the solution.\nWhat should you implement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0057600007.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Event Grid -\nBlob storage events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. Event Grid provides reliable event delivery to your applications through rich retry policies and dead-lettering.\n\nBox 2: Azure Logic App -\nEvent Grid uses event subscriptions to route event messages to subscribers. This image illustrates the relationship between event publishers, event subscriptions, and event handlers.\n\n\nBox 3: Azure Service Bus -\nThe Event Grid service doesn't store events. Instead, events are stored in the Event Handlers, including ServiceBus, EventHubs, Storage Queue, WebHook endpoint, or many other supported Azure Services.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview https://docs.microsoft.com/en-us/java/api/overview/azure/messaging-eventgrid-readmehttps://www.examtopics.com/assets/media/exam-media/04273/0057800001.png",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Event Grid -\nBlob storage events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. Event Grid provides reliable event delivery to your applications through rich retry policies and dead-lettering.\n\nBox 2: Azure Logic App -\nEvent Grid uses event subscriptions to route event messages to subscribers. This image illustrates the relationship between event publishers, event subscriptions, and event handlers.\n\n\nBox 3: Azure Service Bus -\nThe Event Grid service doesn't store events. Instead, events are stored in the Event Handlers, including ServiceBus, EventHubs, Storage Queue, WebHook endpoint, or many other supported Azure Services.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview https://docs.microsoft.com/en-us/java/api/overview/azure/messaging-eventgrid-readme",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0057800001.png"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #22 -- Topic 6",
        "questionIs": "You are creating an app that will use CosmosDB for data storage. The app will process batches of relational data.\nYou need to select an API for the app.\nWhich API should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. MongoDB API",
            "B. Table API",
            "C. SQL API Most Voted",
            "D. Cassandra API"
        ],
        "answersAre": [],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "For relational data you will need the SQL API\nA: The MongoDB API is not used for relational data.\nB: The Table API only supports data in the key/value format\nD: The Cassandra API only supports OLTP (Online Transactional Processing) and not batch processing.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/choose-api",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #23 -- Topic 6",
        "questionIs": "HOTSPOT -\nYou are developing a .NET application that communicates with Azure Storage.\nA message must be stored when the application initializes.\nYou need to implement the message.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0058000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues?tabs=dotnetv11",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/storage/queues/storage-dotnet-how-to-use-queues?tabs=dotnetv11",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #24 -- Topic 6",
        "questionIs": "HOTSPOT -\nA software as a service (SaaS) company provides document management services. The company has a service that consists of several Azure web apps. All\nAzure web apps run in an Azure App Service Plan named PrimaryASP.\nYou are developing a new web service by using a web app named ExcelParser. The web app contains a third-party library for processing Microsoft Excel files.\nThe license for the third-party library stipulates that you can only run a single instance of the library.\nYou need to configure the service.\nHow should you complete the script? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0058300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/en-us/azure/app-service/manage-scale-per-app",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/app-service/manage-scale-per-app",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #25 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou have an application that provides weather forecasting data to external partners. You use Azure API Management to publish APIs.\nYou must change the behavior of the API to meet the following requirements:\n\u2711 Support alternative input parameters\n\u2711 Remove formatting text from responses\n\u2711 Provide additional context to back-end services\nWhich types of policies should you implement? To answer, drag the policy types to the correct requirements. Each policy type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0058500004.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Reference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-policies https://docs.microsoft.com/en-us/azure/api-management/api-management-transformation-policies#forward-context-information-to-the-backend-service",
        "mostVotedAre": [],
        "descriptionIs": "Reference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-howto-policies https://docs.microsoft.com/en-us/azure/api-management/api-management-transformation-policies#forward-context-information-to-the-backend-service",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #26 -- Topic 6",
        "questionIs": "You are developing an e-commerce solution that uses a microservice architecture.\nYou need to design a communication backplane for communicating transactional messages between various parts of the solution. Messages must be communicated in first-in-first-out (FIFO) order.\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure Storage Queue",
            "B. Azure Event Hub",
            "C. Azure Service Bus Most Voted",
            "D. Azure Event Grid"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "As a solution architect/developer, you should consider using Service Bus queues when:\n\u2711 Your solution requires the queue to provide a guaranteed first-in-first-out (FIFO) ordered delivery.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #27 -- Topic 6",
        "questionIs": "DRAG DROP -\nA company backs up all manufacturing data to Azure Blob Storage. Admins move blobs from hot storage to archive tier storage every month.\nYou must automatically move blobs to Archive tier after they have not been modified within 180 days. The path for any item that is not archived must be placed in an existing queue. This operation must be performed automatically once a month. You set the value of TierAgeInDays to -180.\nHow should you configure the Logic App? To answer, drag the appropriate triggers or action blocks to the correct trigger or action slots. Each trigger or action block may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0058800001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Reoccurance..\nTo regularly run tasks, processes, or jobs on specific schedule, you can start your logic app workflow with the built-in Recurrence - Schedule trigger. You can set a date and time as well as a time zone for starting the workflow and a recurrence for repeating that workflow.\nSet the interval and frequency for the recurrence. In this example, set these properties to run your workflow every week.\n\nBox 2: Condition..\nTo run specific actions in your logic app only after passing a specified condition, add a conditional statement. This control structure compares the data in your workflow against specific values or fields. You can then specify different actions that run based on whether or not the data meets the condition.\n\nBox 3: Put a message on a queue -\nThe path for any item that is not archived must be placed in an existing queue.\nNote: Under If true and If false, add the steps to perform based on whether the condition is met.\nBox 4: ..tier it to Cool or Archive tier.\nArchive item.\n\nBox 5: List blobs 2 -\nReference:\nhttps://docs.microsoft.com/en-us/azure/connectors/connectors-native-recurrence https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-loops https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-conditional-statementhttps://www.examtopics.com/assets/media/exam-media/04273/0059000001.jpg",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Reoccurance..\nTo regularly run tasks, processes, or jobs on specific schedule, you can start your logic app workflow with the built-in Recurrence - Schedule trigger. You can set a date and time as well as a time zone for starting the workflow and a recurrence for repeating that workflow.\nSet the interval and frequency for the recurrence. In this example, set these properties to run your workflow every week.\n\nBox 2: Condition..\nTo run specific actions in your logic app only after passing a specified condition, add a conditional statement. This control structure compares the data in your workflow against specific values or fields. You can then specify different actions that run based on whether or not the data meets the condition.\n\nBox 3: Put a message on a queue -\nThe path for any item that is not archived must be placed in an existing queue.\nNote: Under If true and If false, add the steps to perform based on whether the condition is met.\nBox 4: ..tier it to Cool or Archive tier.\nArchive item.\n\nBox 5: List blobs 2 -\nReference:\nhttps://docs.microsoft.com/en-us/azure/connectors/connectors-native-recurrence https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-loops https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-control-flow-conditional-statement",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0059000001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #28 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure Service application that processes queue data when it receives a message from a mobile application. Messages may not be sent to the service consistently.\nYou have the following requirements:\n\u2711 Queue size must not grow larger than 80 gigabytes (GB).\n\u2711 Use first-in-first-out (FIFO) ordering of messages.\n\u2711 Minimize Azure costs.\nYou need to implement the messaging solution.\nSolution: Use the .Net API to add a message to an Azure Service Bus Queue from the mobile application. Create an Azure Function App that uses an Azure\nService Bus Queue trigger.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "You can create a function that is triggered when messages are submitted to an Azure Storage queue.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-queue-triggered-function",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #29 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\nYou are developing an Azure solution to collect point-of-sale (POS) device data from 2,000 stores located throughout the world. A single device can produce 2 megabytes (MB) of data every 24 hours. Each store location has one to five devices that send data.\nYou must store the device data in Azure Blob storage. Device data must be correlated based on a device identifier. Additional stores are expected to open in the future.\nYou need to implement a solution to receive the device data.\nSolution: Provision an Azure Notification Hub. Register all devices with the hub.\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Instead use an Azure Service Bus, which is used order processing and financial transactions.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #30 -- Topic 6",
        "questionIs": "You are building a loyalty program for a major snack producer. When customers buy a snack at any of 100 participating retailers the event is recorded in Azure\nEvent Hub. Each retailer is given a unique identifier that is used as the primary identifier for the loyalty program.\nRetailers must be able to be added or removed at any time. Retailers must only be able to record sales for themselves.\nYou need to ensure that retailers can record sales.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Use publisher policies for retailers.",
            "B. Create a partition for each retailer.",
            "C. Define a namespace for each retailer."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Event Hubs enables granular control over event publishers through publisher policies. Publisher policies are run-time features designed to facilitate large numbers of independent event publishers. With publisher policies, each publisher uses its own unique identifier when publishing events to an event hub.\nIncorrect:\nNot C: An Event Hubs namespace is a management container for event hubs (or topics, in Kafka parlance). It provides DNS-integrated network endpoints and a range of access control and network integration management features such as IP filtering, virtual network service endpoint, and Private Link.\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #31 -- Topic 6",
        "questionIs": "DRAG DROP -\nYou develop and deploy a web app to Azure App Service in a production environment. You scale out the web app to four instances and configure a staging slot to support changes.\nYou must monitor the web app in the environment to include the following requirements:\n\u2711 Increase web app availability by re-routing requests away from instances with error status codes and automatically replace instances if they remain in an error state after one hour.\n\u2711 Send web server logs, application logs, standard output, and standard error messaging to an Azure Storage blob account.\nYou need to configure Azure App Service.\nWhich values should you use? To answer, drag the appropriate configuration value to the correct requirements. Each configuration value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0059400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Health check -\nHealth check increases your application's availability by re-routing requests away from unhealthy instances, and replacing instances if they remain unhealthy. Your\nApp Service plan should be scaled to two or more instances to fully utilize Health check.\n\nBox 2: Diagnostic setting -\nAzure provides built-in diagnostics to assist with debugging an App Service app.\nWith the new Azure Monitor integration, you can create Diagnostic Settings to send logs to Storage Accounts, Event Hubs and Log Analytics.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check https://docs.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Health check -\nHealth check increases your application's availability by re-routing requests away from unhealthy instances, and replacing instances if they remain unhealthy. Your\nApp Service plan should be scaled to two or more instances to fully utilize Health check.\n\nBox 2: Diagnostic setting -\nAzure provides built-in diagnostics to assist with debugging an App Service app.\nWith the new Azure Monitor integration, you can create Diagnostic Settings to send logs to Storage Accounts, Event Hubs and Log Analytics.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/monitor-instances-health-check https://docs.microsoft.com/en-us/azure/app-service/troubleshoot-diagnostic-logs",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #32 -- Topic 6",
        "questionIs": "You develop a solution that uses Azure Virtual Machines (VMs).\nThe VMs contain code that must access resources in an Azure resource group. You grant the VM access to the resource group in Resource Manager.\nYou need to obtain an access token that uses the VM's system-assigned managed identity.\nWhich two actions should you perform? Each correct answer presents part of the solution.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. From the code on the VM, call Azure Resource Manager using an access token. Most Voted",
            "B. Use PowerShell on a remote machine to make a request to the local managed identity for Azure resources endpoint.",
            "C. Use PowerShell on the VM to make a request to the local managed identity for Azure resources endpoint. Most Voted",
            "D. From the code on the VM, call Azure Resource Manager using a SAS token.",
            "E. From the code on the VM, generate a user delegation SAS token."
        ],
        "answersAre": [
            "B",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #33 -- Topic 6",
        "questionIs": "You are developing a road tollway tracking application that sends tracking events by using Azure Event Hubs using premium tier.\nEach road must have a throttling policy uniquely assigned.\nYou need to configure the event hub to allow for per-road throttling.\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Use a unique consumer group for each road.",
            "B. Ensure each road stores events in a different partition.",
            "C. Ensure each road has a unique connection string.",
            "D. Use a unique application group for each road. Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #34 -- Topic 6",
        "questionIs": "You develop and deploy an ASP.NET Core application that connects to an Azure Database for MySQL instance.\nConnections to the database appear to drop intermittently and the application code does not handle the connection failure.\nYou need to handle the transient connection errors in code by implementing retries.\nWhat are three possible ways to achieve this goal? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Close the database connection and immediately report an error.",
            "B. Disable connection pooling and configure a second Azure Database for MySQL instance.",
            "C. Wait five seconds before repeating the connection attempt to the database. Most Voted",
            "D. Set a maximum number of connection attempts to 10 and report an error on subsequent connections. Most Voted",
            "E. Increase connection repeat attempts exponentially up to 120 seconds. Most Voted"
        ],
        "answersAre": [
            "A",
            "C",
            "D"
        ],
        "mostVotedAre": [
            "C",
            "D",
            "E"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #35 -- Topic 6",
        "questionIs": "You are building a B2B web application that uses Azure B2B collaboration for authentication. Paying customers authenticate to Azure B2B using federation.\nThe application allows users to sign up for trial accounts using any email address.\nWhen a user converts to a paying customer, the data associated with the trial should be kept, but the user must authenticate using federation.\nYou need to update the user in Azure Active Directory (Azure AD) when they convert to a paying customer.\nWhich Graph API parameter is used to change authentication from one-time passcodes to federation?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. resetRedemption Most Voted",
            "B. Status",
            "C. userFlowType",
            "D. invitedUser"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #36 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou develop an image upload service that is exposed using Azure API Management. Images are analyzed after upload for automatic tagging.\n\nImages over 500 KB are processed by a different backend that offers a lower tier of service that costs less money. The lower tier of service is denoted by a header named x-large-request. Images over 500 KB must never be processed by backends for smaller images and must always be charged the lower price.\n\nYou need to implement API Management policies to ensure that images are processed correctly.\nHow should you complete the API Management inbound policy? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image414.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #37 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou develop several Azure Functions app functions to process JSON documents from a third-party system. The third-party system publishes events to Azure Event Grid to include hundreds of event types, such as billing, inventory, and shipping updates.\n\nEvents must be sent to a single endpoint for the Azure Functions app to process. The events must be filtered by event type before processing. You must have authorization and authentication control to partition your tenants to receive the event data.\n\nYou need to configure Azure Event Grid.\n\nWhich configuration should you use? To answer, select the appropriate values in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image416.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #38 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.",
            "B.\nMost Voted",
            "C.",
            "D.\nMost Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B",
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #39 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.",
            "B.",
            "C.\nMost Voted",
            "D."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #40 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.",
            "B.",
            "C.",
            "D."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #41 -- Topic 6",
        "questionIs": "DRAG DROP\n-\n\nYou develop and deploy several APIs to Azure API Management.\n\nYou create the following policy fragment named APICounts:\n\n\n\nThe policy fragment must be reused across various scopes and APIs. The policy fragment must be applied to all APIs and run when a calling system invokes any API.\n\nYou need to implement the policy fragment.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image438.png",
            "https://img.examtopics.com/az-204/image439.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #42 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a solution that uses several Azure Service Bus queues. You create an Azure Event Grid subscription for the Azure Service Bus namespace. You use Azure Functions as subscribers to process the messages.\n\nYou need to emit events to Azure Event Grid from the queues. You must use the principal of least privilege and minimize costs.\n\nWhich Azure Service Bus values should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image441.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #43 -- Topic 6",
        "questionIs": "You are developing several Azure API Management (APIM) hosted APIs.\n\nThe APIs have the following requirements:\n\n\u2022 Require a subscription key to access all APIs.\n\u2022 Include terms of use that subscribers must accept to use the APIs.\n\u2022 Administrators must review and accept or reject subscription attempts.\n\u2022 Limit the count of multiple simultaneous subscriptions.\n\nYou need to implement the APIs.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure and apply header-based versioning.",
            "B. Create and publish a product. Most Voted",
            "C. Configure and apply query string-based versioning.",
            "D. Add a new revision to all APIs. Make the revisions current and add a change log entry."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #44 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a solution by using the Azure Event Hubs SDK. You create a standard Azure Event Hub with 16 partitions. You implement eight event processor clients.\n\nYou must balance the load dynamically when an event processor client fails. When an event processor client fails, another event processor must continue processing from the exact point at which the failure occurred. All events must be aggregate and upload to an Azure Blob storage account.\n\nYou need to implement event processing recovery for the solution.\n\nWhich SDK features should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image457.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #45 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou are developing a new API to be hosted by Azure API Management (APIM). The backend service that implements the API has not been completed. You are creating a test API and operation.\n\nYou must enable developers to continue with the implementation and testing of the APIM instance integrations while you complete the backend API development.\n\nYou need to configure a test API response.\n\nHow should you complete the configuration? To answer, select the appropriate options in the answer area.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image459.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #46 -- Topic 6",
        "questionIs": "You are developing several Azure API Management (APIM) hosted APIs.\n\nYou must inspect request processing of the APIs in APIM. Requests to APIM by using a REST client must also be included. The request inspection must include the following information:\n\n\u2022 requests APIM sent to the API backend and the response it received\n\u2022 policies applied to the response before sending back to the caller\n\u2022 errors that occurred during the processing of the request and the policies applied to the errors\n\u2022 original request APIM received from the caller and the policies applied to the request\n\nYou need to inspect the APIs.\n\nWhich three actions should you do? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Enable the Allow tracing setting for the subscription used to inspect the API. Most Voted",
            "B. Add the Ocp-Apim-Trace header value to the API call whit a value set to true. Most Voted",
            "C. Add the Ocp-Apim-Subscription-Key header value to the key for a subscription that allows access to the API. Most Voted",
            "D. Create and configure a custom policy. Apply the policy to the inbound policy section with a global scope.",
            "E. Create and configure a custom policy. Apply the policy to the outbound policy section with an API scope."
        ],
        "answersAre": [
            "A",
            "C",
            "E"
        ],
        "mostVotedAre": [
            "A",
            "B",
            "C"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #47 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.",
            "B.",
            "C.",
            "D."
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #48 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou plan to implement an Azure Functions app.\n\nThe Azure Functions app has the following requirements:\n\n\u2022 Must be triggered by a message placed in an Azure Storage queue.\n\u2022 Must use the queue name set by an app setting named input_queue.\n\u2022 Must create an Azure Blob Storage named the same as the content of the message.\n\nYou need to identify how to reference the queue and blob name in the function.json file of the Azure Functions app.\n\nHow should you reference the names? To answer, select the appropriate values in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image471.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #49 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou have an Azure API Management instance named API1 that uses a managed gateway.\n\nYou plan to implement a policy that will apply at a product scope and will set the header of inbound requests to include information about the region hosting the gateway of API1. The policy definition contains the following content:\n\n\n\nYou have the following requirements for the policy definition:\n\n\u2022 Ensure that the header contains the information about the region hosting the gateway of API1.\n\u2022 Ensure the policy applies only after any global level policies are processed first.\n\nYou need to complete the policy definition.\n\nWhich values should you choose? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image473.png",
            "https://img.examtopics.com/az-204/image474.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #50 -- Topic 6",
        "questionIs": "You are developing several Azure API Management (APIM) hosted APIs.\n\nYou must make several minor and non-breaking changes to one of the APIs. The API changes include the following requirements:\n\n\u2022 Must not disrupt callers of the API.\n\u2022 Enable roll back if you find issues.\n\u2022 Documented to enable developers to understand what is new.\n\u2022 Tested before publishing.\n\nYou need to update the API.\n\nWhat should you do?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Configure and apply header-based versioning.",
            "B. Create and publish a product.",
            "C. Configure and apply a custom policy.",
            "D. Add a new revision to the API. Most Voted",
            "E. Configure and apply query string-based versioning."
        ],
        "answersAre": [
            "E"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #51 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\nYou are developing an application to store millions of images in Azure blob storage.\n\nThe application has the following requirements:\n\n\u2022 Store the Exif (exchangeable image file format) data from the image as blob metadata when the application uploads the image.\n\u2022 Retrieve the Exif data from the image while minimizing bandwidth and processing time.\n\u2022 Utilizes the REST API.\n\nYou need to use the image Exif data as blob metadata in the application.\n\nWhich HTTP verbs should you use? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image476.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #52 -- Topic 6",
        "questionIs": "You are developing several microservices to run on Azure Container Apps for a company. External TCP ingress traffic from the internet has been enabled for the microservices.\n\nThe company requires that the microservices must scale based on an Azure Event Hub trigger.\n\nYou need to scale the microservices by using a custom scaling rule.\n\nWhich two Kubernetes Event-driven Autoscaling (KEDA) trigger fields should you use? Each correct answer presents part of the solution.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. metadata Most Voted",
            "B. type Most Voted",
            "C. authenticationRef",
            "D. name",
            "E. metricType"
        ],
        "answersAre": [
            "A",
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #53 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.",
            "B.",
            "C.",
            "D."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #54 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.\nMost Voted",
            "B.",
            "C.",
            "D."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #55 -- Topic 6",
        "questionIs": "A company is developing a solution that allows smart refrigerators to send temperature information to a central location.\n\nThe solution must receive and store messages until they can be processed. You create an Azure Service Bus instance by providing a name, pricing tier, subscription, resource group, and location.\n\nYou need to complete the configuration.\n\nWhich Azure CLI or PowerShell command should you run?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A.",
            "B.",
            "C.",
            "D."
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #56 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are implementing an application by using Azure Event Grid to push near-real-time information to customers.\n\nYou have the following requirements:\n\u2022 You must send events to thousands of customers that include hundreds of various event types.\n\u2022 The events must be filtered by event type before processing.\n\u2022 Authentication and authorization must be handled by using Microsoft Entra ID.\n\u2022 The events must be published to a single endpoint.\n\nYou need to implement Azure Event Grid.\n\nSolution: Publish events to an event domain. Create a custom topic for each customer.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes Most Voted",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "A",
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #57 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are implementing an application by using Azure Event Grid to push near-real-time information to customers.\n\nYou have the following requirements:\n\u2022 You must send events to thousands of customers that include hundreds of various event types.\n\u2022 The events must be filtered by event type before processing.\n\u2022 Authentication and authorization must be handled by using Microsoft Entra ID.\n\u2022 The events must be published to a single endpoint.\n\nYou need to implement Azure Event Grid.\n\nSolution: Publish events to a custom topic. Create an event subscription for each customer.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #58 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are implementing an application by using Azure Event Grid to push near-real-time information to customers.\n\nYou have the following requirements:\n\u2022 You must send events to thousands of customers that include hundreds of various event types.\n\u2022 The events must be filtered by event type before processing.\n\u2022 Authentication and authorization must be handled by using Microsoft Entra ID.\n\u2022 The events must be published to a single endpoint.\n\nYou need to implement Azure Event Grid.\n\nSolution: Enable ingress, create a TCP scale rule, and apply the rule to the container app.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #59 -- Topic 6",
        "questionIs": "HOTSPOT\n-\n\n\nCase study\n-\n\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\n\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\n\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\n\nTo start the case study\n-\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\n\nBackground\n-\n\nMunson\u2019s Pickles and Preserves Farm is an agricultural cooperative corporation based in Washington, US, with farms located across the United States. The company supports agricultural production resources by distributing seeds fertilizers, chemicals, fuel, and farm machinery to the farms.\n\n\nCurrent Environment\n-\n\nThe company is migrating all applications from an on-premises datacenter to Microsoft Azure. Applications support distributors, farmers, and internal company staff.\n\n\nCorporate website\n-\n\u2022 The company hosts a public website located at http://www.munsonspicklesandpreservesfarm.com. The site supports farmers and distributors who request agricultural production resources.\n\n\nFarms\n-\n\u2022 The company created a new customer tenant in the Microsoft Entra admin center to support authentication and authorization for applications.\n\n\nDistributors\n-\n\u2022 Distributors integrate their applications with data that is accessible by using APIs hosted at http://www.munsonspicklesandpreservesfarm.com/api to receive and update resource data.\n\n\nRequirements\n-\n\nThe application components must meet the following requirements:\n\n\nCorporate website\n-\n\u2022 The site must be migrated to Azure App Service.\n\u2022 Costs must be minimized when hosting in Azure.\n\u2022 Applications must automatically scale independent of the compute resources.\n\u2022 All code changes must be validated by internal staff before release to production.\n\u2022 File transfer speeds must improve, and webpage-load performance must increase.\n\u2022 All site settings must be centrally stored, secured without using secrets, and encrypted at rest and in transit.\n\u2022 A queue-based load leveling pattern must be implemented by using Azure Service Bus queues to support high volumes of website agricultural production resource requests.\n\n\nFarms\n-\n\u2022 Farmers must authenticate to applications by using Microsoft Entra ID.\n\n\nDistributors\n-\n\u2022 The company must track a custom telemetry value with each API call and monitor performance of all APIs.\n\u2022 API telemetry values must be charted to evaluate variations and trends for resource data.\n\n\nInternal staff\n-\n\u2022 App and API updates must be validated before release to production.\n\u2022 Staff must be able to select a link to direct them back to the production app when validating an app or API update.\n\u2022 Staff profile photos and email must be displayed on the website once they authenticate to applications by using their Microsoft Entra ID.\n\n\nSecurity\n-\n\u2022 All web communications must be secured by using TLS/HTTPS.\n\u2022 Web content must be restricted by country/region to support corporate compliance standards.\n\u2022 The principle of least privilege must be applied when providing any user rights or process access rights.\n\u2022 Managed identities for Azure resources must be used to authenticate services that support Microsoft Entra ID authentication.\n\n\nIssues\n-\n\n\nCorporate website\n-\n\u2022 Farmers report HTTP 503 errors at the same time as internal staff report that CPU and memory usage are high.\n\u2022 Distributors report HTTP 502 errors at the same time as internal staff report that average response times and networking traffic are high.\n\u2022 Internal staff report webpage load sizes are large and take a long time to load.\n\u2022 Developers receive authentication errors to Service Bus when they debug locally.\n\n\nDistributors\n-\n\u2022 Many API telemetry values are sent in a short period of time. Telemetry traffic, data costs, and storage costs must be reduced while preserving a statistically correct analysis of the data points sent by the APIs.\n\n\nYou need to provide internal staff access to the production site after a validation.\n\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\n\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://img.examtopics.com/az-204/image519.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "",
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #60 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are implementing an application by using Azure Event Grid to push near-real-time information to customers.\n\nYou have the following requirements:\n\u2022 You must send events to thousands of customers that include hundreds of various event types.\n\u2022 The events must be filtered by event type before processing.\n\u2022 Authentication and authorization must be handled by using Microsoft Entra ID.\n\u2022 The events must be published to a single endpoint.\n\nYou need to implement Azure Event Grid.\n\nSolution: Publish events to a partner topic. Create an event subscription for each customer.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No Most Voted"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #61 -- Topic 6",
        "questionIs": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.\n\nAfter you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.\n\nYou are implementing an application by using Azure Event Grid to push near-real-time information to customers.\n\nYou have the following requirements:\n\u2022 You must send events to thousands of customers that include hundreds of various event types.\n\u2022 The events must be filtered by event type before processing.\n\u2022 Authentication and authorization must be handled by using Microsoft Entra ID.\n\u2022 The events must be published to a single endpoint.\n\nYou need to implement Azure Event Grid.\n\nSolution: Publish events to a system topic. Create an event subscription for each customer.\n\nDoes the solution meet the goal?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Yes",
            "B. No"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 7",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible.\n\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:\nOcean Transport `\" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight, contents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports of call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display on the shipping website.\n\nShipping Data -\nThe application uses MongoDB JSON document storage database for all container and transport information.\n\nShipping Web Site -\nThe site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/\n\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:\n\n\nRequirements -\n\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic App.\nSupport industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final BizTalk migrations.\n\nShipping Function app -\nImplement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring additional costs.\nAutomatically scale to double capacity during peak shipping times while not causing application downtime.\nMinimize costs when selecting an Azure payment model.\n\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\n\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nIssues -\n\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\n\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.\nQuestion\nHOTSPOT -\nYou need to configure Azure CDN for the Shipping web site.\nWhich configuration options should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0004000001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0004200001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: Shipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nTier: Standard -\n\nProfile: Akamai -\nOptimization: Dynamic site acceleration\nDynamic site acceleration (DSA) is available for Azure CDN Standard from Akamai, Azure CDN Standard from Verizon, and Azure CDN Premium from Verizon profiles.\nDSA includes various techniques that benefit the latency and performance of dynamic content. Techniques include route and network optimization, TCP optimization, and more.\nYou can use this optimization to accelerate a web app that includes numerous responses that aren't cacheable. Examples are search results, checkout transactions, or real-time data. You can continue to use core Azure CDN caching capabilities for static data.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-optimization-overview",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: Shipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nTier: Standard -\n\nProfile: Akamai -\nOptimization: Dynamic site acceleration\nDynamic site acceleration (DSA) is available for Azure CDN Standard from Akamai, Azure CDN Standard from Verizon, and Azure CDN Premium from Verizon profiles.\nDSA includes various techniques that benefit the latency and performance of dynamic content. Techniques include route and network optimization, TCP optimization, and more.\nYou can use this optimization to accelerate a web app that includes numerous responses that aren't cacheable. Examples are search results, checkout transactions, or real-time data. You can continue to use core Azure CDN caching capabilities for static data.\nReference:\nhttps://docs.microsoft.com/en-us/azure/cdn/cdn-optimization-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 7",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible.\n\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:\nOcean Transport `\" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight, contents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports of call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display on the shipping website.\n\nShipping Data -\nThe application uses MongoDB JSON document storage database for all container and transport information.\n\nShipping Web Site -\nThe site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/\n\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:\n\n\nRequirements -\n\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic App.\nSupport industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final BizTalk migrations.\n\nShipping Function app -\nImplement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring additional costs.\nAutomatically scale to double capacity during peak shipping times while not causing application downtime.\nMinimize costs when selecting an Azure payment model.\n\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\n\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nIssues -\n\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\n\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.\nQuestion\nHOTSPOT -\nYou need to correct the VM issues.\nWhich tools should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0004000001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0004500001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Backup -\nThe VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\nIn-Place restore of disks in IaaS VMs is a feature of Azure Backup.\nPerformance: Accelerated Networking\nScenario: The VM shows high network latency, jitter, and high CPU utilization.\n\nBox 2: Accelerated networking -\nThe VM shows high network latency, jitter, and high CPU utilization.\nAccelerated networking enables single root I/O virtualization (SR-IOV) to a VM, greatly improving its networking performance. This high-performance path bypasses the host from the datapath, reducing latency, jitter, and CPU utilization, for use with the most demanding network workloads on supported VM types.\nReference:\nhttps://azure.microsoft.com/en-us/blog/an-easy-way-to-bring-back-your-azure-vm-with-in-place-restore/",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Backup -\nThe VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\nIn-Place restore of disks in IaaS VMs is a feature of Azure Backup.\nPerformance: Accelerated Networking\nScenario: The VM shows high network latency, jitter, and high CPU utilization.\n\nBox 2: Accelerated networking -\nThe VM shows high network latency, jitter, and high CPU utilization.\nAccelerated networking enables single root I/O virtualization (SR-IOV) to a VM, greatly improving its networking performance. This high-performance path bypasses the host from the datapath, reducing latency, jitter, and CPU utilization, for use with the most demanding network workloads on supported VM types.\nReference:\nhttps://azure.microsoft.com/en-us/blog/an-easy-way-to-bring-back-your-azure-vm-with-in-place-restore/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 8",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nDRAG DROP -\nYou need to add code at line PC32 in Processing.cs to implement the GetCredentials method in the Processing class.\nHow should you complete the code? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0018400001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019000001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: AzureServiceTokenProvider()\nBox 2: tp.GetAccessTokenAsync(\"..\")\nAcquiring an access token is then quite easy. Example code:\nprivate async Task<string> GetAccessTokenAsync()\n{\nvar tokenProvider = new AzureServiceTokenProvider();\nreturn await tokenProvider.GetAccessTokenAsync(\"https://storage.azure.com/\");\n}\nReference:\nhttps://joonasw.net/view/azure-ad-authentication-with-azure-storage-and-managed-service-identity",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: AzureServiceTokenProvider()\nBox 2: tp.GetAccessTokenAsync(\"..\")\nAcquiring an access token is then quite easy. Example code:\nprivate async Task<string> GetAccessTokenAsync()\n{\nvar tokenProvider = new AzureServiceTokenProvider();\nreturn await tokenProvider.GetAccessTokenAsync(\"https://storage.azure.com/\");\n}\nReference:\nhttps://joonasw.net/view/azure-ad-authentication-with-azure-storage-and-managed-service-identity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 8",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nDRAG DROP -\nYou need to ensure disaster recovery requirements are met.\nWhat code should you add at line PC16?\nTo answer, drag the appropriate code fragments to the correct locations. Each code fragment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0018400001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: Disaster recovery. Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nBox 1: DirectoryTransferContext -\nWe transfer all files in the directory.\nNote: The TransferContext object comes in two forms: SingleTransferContext and DirectoryTransferContext. The former is for transferring a single file and the latter is for transferring a directory of files.\nBox 2: ShouldTransferCallbackAsync\nThe DirectoryTransferContext.ShouldTransferCallbackAsync delegate callback is invoked to tell whether a transfer should be done.\n\nBox 3: False -\nIf you want to use the retry policy in Copy, and want the copy can be resume if break in the middle, you can use SyncCopy (isServiceCopy = false).\nNote that if you choose to use service side copy ('isServiceCopy' set to true), Azure (currently) doesn't provide SLA for that. Setting 'isServiceCopy' to false will download the source blob loca\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-use-data-movement-library https://docs.microsoft.com/en-us/dotnet/api/microsoft.windowsazure.storage.datamovement.directorytransfercontext.shouldtransfercallbackasync?view=azure- dotnet",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: Disaster recovery. Regional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nBox 1: DirectoryTransferContext -\nWe transfer all files in the directory.\nNote: The TransferContext object comes in two forms: SingleTransferContext and DirectoryTransferContext. The former is for transferring a single file and the latter is for transferring a directory of files.\nBox 2: ShouldTransferCallbackAsync\nThe DirectoryTransferContext.ShouldTransferCallbackAsync delegate callback is invoked to tell whether a transfer should be done.\n\nBox 3: False -\nIf you want to use the retry policy in Copy, and want the copy can be resume if break in the middle, you can use SyncCopy (isServiceCopy = false).\nNote that if you choose to use service side copy ('isServiceCopy' set to true), Azure (currently) doesn't provide SLA for that. Setting 'isServiceCopy' to false will download the source blob loca\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-use-data-movement-library https://docs.microsoft.com/en-us/dotnet/api/microsoft.windowsazure.storage.datamovement.directorytransfercontext.shouldtransfercallbackasync?view=azure- dotnet",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 9",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You are a developer implementing highly scalable and resilient applications to support online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles. The application sends data to several printers. The application consists of five modules that run independently on virtual machines (VMs). Coho Winery plans to move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork and text for custom label designs.\n\nRequirements. Data -\nYou identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\n\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to documents, resources, and applications to external partners.\n\nExternal partners must use their own credentials and authenticate with their organization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user account administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key Vault.\nE-commerce application sign-ins must be secured by using Azure App Service authentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect company content.\nThe LabelMaker application must be secured by using an AAD account that has full access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\n\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\n\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\n\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included for reference only.\nThis JSON file contains a representation of the data for an order that includes a single item.\n\nOrder.json -\nQuestion\nHOTSPOT -\nYou need to configure Azure Cosmos DB.\nWhich settings should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0019400003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019800001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Strong -\nWhen the consistency level is set to strong, the staleness window is equivalent to zero, and the clients are guaranteed to read the latest committed value of the write operation.\nScenario: Changes to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\nNote: You can choose from five well-defined models on the consistency spectrum. From strongest to weakest, the models are: Strong, Bounded staleness,\nSession, Consistent prefix, Eventual\n\nBox 2: SQL -\nScenario: You identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using Structured Query Language (SQL).",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Strong -\nWhen the consistency level is set to strong, the staleness window is equivalent to zero, and the clients are guaranteed to read the latest committed value of the write operation.\nScenario: Changes to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\nNote: You can choose from five well-defined models on the consistency spectrum. From strongest to weakest, the models are: Strong, Bounded staleness,\nSession, Consistent prefix, Eventual\n\nBox 2: SQL -\nScenario: You identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using Structured Query Language (SQL).",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 9",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You are a developer implementing highly scalable and resilient applications to support online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles. The application sends data to several printers. The application consists of five modules that run independently on virtual machines (VMs). Coho Winery plans to move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork and text for custom label designs.\n\nRequirements. Data -\nYou identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\n\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to documents, resources, and applications to external partners.\n\nExternal partners must use their own credentials and authenticate with their organization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user account administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key Vault.\nE-commerce application sign-ins must be secured by using Azure App Service authentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect company content.\nThe LabelMaker application must be secured by using an AAD account that has full access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\n\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\n\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\n\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included for reference only.\nThis JSON file contains a representation of the data for an order that includes a single item.\n\nOrder.json -\nQuestion\nHOTSPOT -\nYou need to retrieve all order line items from Order.json and sort the data alphabetically by the city.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0019400003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0019800001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0020200001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: orders o -\nScenario: Order data is stored as nonrelational JSON and must be queried using SQL.\n\nBox 2:li -\n\nBox 3: o.line_items -\n\nBox 4: o.city -\nThe city field is in Order, not in the 2s.",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: orders o -\nScenario: Order data is stored as nonrelational JSON and must be queried using SQL.\n\nBox 2:li -\n\nBox 3: o.line_items -\n\nBox 4: o.city -\nThe city field is in Order, not in the 2s.",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 10",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nHOTSPOT -\nYou need to implement the Azure Function for delivery driver profile information.\nWhich configurations should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0027400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Identity library -\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\nWe recommend that you use a managed identity for applications deployed to Azure.\nThe preceding authentication scenarios are supported by the Azure Identity client library and integrated with Key Vault SDKs.\nNote: What is Managed Service Identity?\nAzure Key Vault avoids the need to store keys and secrets in application code or source control. However, in order to retrieve keys and secrets from Azure Key\nVault, you need to authorize a user or application with Azure Key Vault, which in its turn needs another credential. Managed Service Identity avoids the need of storing credentials for Azure Key Vault in application or environment settings by creating a Service Principal for each application or cloud service on which\nManaged Service Identity is enabled. This Service Principal enables you to call a local MSI endpoint to get an access token from Azure AD using the credentials of the Service Principal. This token is then used to authenticate to an Azure Service, for example Azure Key Vault.\n\nBox 2: Azure Key Vault -\nAzure Key Vault allows you to securely access sensitive information from within your applications:\n* Keys, secrets, and certificates are protected without your having to write the code yourself, and you can easily use them from your applications.\nUse Azure Key Vault to store only secrets for your application. Examples of secrets that should be stored in Key Vault include:\n\nClient application secrets -\n\nConnection strings -\n\nPasswords -\n\nShared access keys -\n\nSSH keys -\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/developers-guide https://integration.team/blog/retrieve-azure-key-vault-secrets-using-azure-functions-and-managed-service-identity",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Identity library -\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\nWe recommend that you use a managed identity for applications deployed to Azure.\nThe preceding authentication scenarios are supported by the Azure Identity client library and integrated with Key Vault SDKs.\nNote: What is Managed Service Identity?\nAzure Key Vault avoids the need to store keys and secrets in application code or source control. However, in order to retrieve keys and secrets from Azure Key\nVault, you need to authorize a user or application with Azure Key Vault, which in its turn needs another credential. Managed Service Identity avoids the need of storing credentials for Azure Key Vault in application or environment settings by creating a Service Principal for each application or cloud service on which\nManaged Service Identity is enabled. This Service Principal enables you to call a local MSI endpoint to get an access token from Azure AD using the credentials of the Service Principal. This token is then used to authenticate to an Azure Service, for example Azure Key Vault.\n\nBox 2: Azure Key Vault -\nAzure Key Vault allows you to securely access sensitive information from within your applications:\n* Keys, secrets, and certificates are protected without your having to write the code yourself, and you can easily use them from your applications.\nUse Azure Key Vault to store only secrets for your application. Examples of secrets that should be stored in Key Vault include:\n\nClient application secrets -\n\nConnection strings -\n\nPasswords -\n\nShared access keys -\n\nSSH keys -\nReference:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/developers-guide https://integration.team/blog/retrieve-azure-key-vault-secrets-using-azure-functions-and-managed-service-identity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 10",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nYou need to grant access to the retail store location data for the inventory service development effort.\nWhat should you use?",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure AD access token",
            "B. Azure RBAC role",
            "C. Shared access signature (SAS) token",
            "D. Azure AD ID token",
            "E. Azure AD refresh token"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "A shared access signature (SAS) provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:\nWhat resources the client may access.\nWhat permissions they have to those resources.\nHow long the SAS is valid.\nNote: Inventory services:\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 10",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nHOTSPOT -\nYou need to reliably identify the delivery driver profile information.\nHow should you configure the system? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0027800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: ID -\nScenario: Store delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\nID token - A JWT that contains claims that you can use to identify users in your application. This token is securely sent in HTTP requests for communication between two components of the same application or service. You can use the claims in an ID token as you see fit. They're commonly used to display account information or to make access control decisions in an application. ID tokens are signed, but the're not encrypted. When your application or API receives an ID token, it must validate the signature to prove that the token is authentic. Your application or API must also validate a few claims in the token to prove that it's valid.\nDepending on the scenario requirements, the claims validated by an application can vary, but your application must perform some common claim validations in every scenario.\n\nBox 2: Oid -\nOid - The immutable identifier for the \"principal\" of the request - the user or service principal whose identity has been verified. In ID tokens and app+user tokens, this is the object ID of the user. In app-only tokens, this is the object ID of the calling service principal. It can also be used to perform authorization checks safely and as a key in database tables. This ID uniquely identifies the principal across applications - two different applications signing in the same user will receive the same value in the oid claim.\nIncorrect:\nAud - Identifies the intended recipient of the token. For Azure AD B2C, the audience is the application ID. Your application should validate this value and reject the token if it doesn't match. Audience is synonymous with resource.\nIdp - Records the identity provider that authenticated the subject of the token. This value is identical to the value of the Issuer claim unless the user account not in the same tenant as the issuer - guests, for instance. If the claim isn't present, it means that the value of iss can be used instead. For personal accounts being used in an organizational context (for instance, a personal account invited to an Azure AD tenant), the idp claim may be 'live.com' or an STS URI containing the\nMicrosoft account tenant.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory-b2c/tokens-overview https://docs.microsoft.com/en-us/azure/active-directory/develop/access-tokens",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: ID -\nScenario: Store delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\nID token - A JWT that contains claims that you can use to identify users in your application. This token is securely sent in HTTP requests for communication between two components of the same application or service. You can use the claims in an ID token as you see fit. They're commonly used to display account information or to make access control decisions in an application. ID tokens are signed, but the're not encrypted. When your application or API receives an ID token, it must validate the signature to prove that the token is authentic. Your application or API must also validate a few claims in the token to prove that it's valid.\nDepending on the scenario requirements, the claims validated by an application can vary, but your application must perform some common claim validations in every scenario.\n\nBox 2: Oid -\nOid - The immutable identifier for the \"principal\" of the request - the user or service principal whose identity has been verified. In ID tokens and app+user tokens, this is the object ID of the user. In app-only tokens, this is the object ID of the calling service principal. It can also be used to perform authorization checks safely and as a key in database tables. This ID uniquely identifies the principal across applications - two different applications signing in the same user will receive the same value in the oid claim.\nIncorrect:\nAud - Identifies the intended recipient of the token. For Azure AD B2C, the audience is the application ID. Your application should validate this value and reject the token if it doesn't match. Audience is synonymous with resource.\nIdp - Records the identity provider that authenticated the subject of the token. This value is identical to the value of the Issuer claim unless the user account not in the same tenant as the issuer - guests, for instance. If the claim isn't present, it means that the value of iss can be used instead. For personal accounts being used in an organizational context (for instance, a personal account invited to an Azure AD tenant), the idp claim may be 'live.com' or an STS URI containing the\nMicrosoft account tenant.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory-b2c/tokens-overview https://docs.microsoft.com/en-us/azure/active-directory/develop/access-tokens",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 10",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nYou need to secure the Azure Functions to meet the security requirements.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Store the RSA-HSM key in Azure Key Vault with soft-delete and purge-protection features enabled.",
            "B. Store the RSA-HSM key in Azure Blob storage with an immutability policy applied to the container.",
            "C. Create a free tier Azure App Configuration instance with a new Azure AD service principal.",
            "D. Create a standard tier Azure App Configuration instance with an assigned Azure AD managed identity.",
            "E. Store the RSA-HSM key in Azure Cosmos DB. Apply the built-in policies for customer-managed keys and allowed locations."
        ],
        "answersAre": [
            "A",
            "D"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Scenario: All Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nMicrosoft Azure Key Vault is a cloud-hosted management service that allows users to encrypt keys and small secrets by using keys that are protected by hardware security modules (HSMs).\nYou need to create a managed identity for your application.\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/app-service-key-vault-references",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 11",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nDRAG DROP -\nYou need to add markup at line AM04 to implement the ContentReview role.\nHow should you complete the markup? To answer, drag the appropriate json segments to the correct locations. Each json segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0028500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028700001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: allowedMemberTypes -\nallowedMemberTypes specifies whether this app role definition can be assigned to users and groups by setting to \"User\", or to other applications (that are accessing this application in daemon service scenarios) by setting to \"Application\", or to both.\nNote: The following example shows the appRoles that you can assign to users.\n\"appId\": \"8763f1c4-f988-489c-a51e-158e9ef97d6a\",\n\"appRoles\": [\n{\n\"allowedMemberTypes\": [\n\"User\"\n],\n\"displayName\": \"Writer\",\n\"id\": \"d1c2ade8-98f8-45fd-aa4a-6d06b947c66f\",\n\"isEnabled\": true,\n\"description\": \"Writers Have the ability to create tasks.\",\n\"value\": \"Writer\"\n}\n],\n\"availableToOtherTenants\": false,\n\nBox 2: User -\nScenario: In order to review content a user must be part of a ContentReviewer role.\n\nBox 3: value -\nvalue specifies the value which will be included in the roles claim in authentication and access tokens.\nReference:\nhttps://docs.microsoft.com/en-us/graph/api/resources/approle",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: allowedMemberTypes -\nallowedMemberTypes specifies whether this app role definition can be assigned to users and groups by setting to \"User\", or to other applications (that are accessing this application in daemon service scenarios) by setting to \"Application\", or to both.\nNote: The following example shows the appRoles that you can assign to users.\n\"appId\": \"8763f1c4-f988-489c-a51e-158e9ef97d6a\",\n\"appRoles\": [\n{\n\"allowedMemberTypes\": [\n\"User\"\n],\n\"displayName\": \"Writer\",\n\"id\": \"d1c2ade8-98f8-45fd-aa4a-6d06b947c66f\",\n\"isEnabled\": true,\n\"description\": \"Writers Have the ability to create tasks.\",\n\"value\": \"Writer\"\n}\n],\n\"availableToOtherTenants\": false,\n\nBox 2: User -\nScenario: In order to review content a user must be part of a ContentReviewer role.\n\nBox 3: value -\nvalue specifies the value which will be included in the roles claim in authentication and access tokens.\nReference:\nhttps://docs.microsoft.com/en-us/graph/api/resources/approle",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 11",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nHOTSPOT -\nYou need to add code at line AM09 to ensure that users can review content using ContentAnalysisService.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0028500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: \"oauth2Permissions\": [\"login\"]\noauth2Permissions specifies the collection of OAuth 2.0 permission scopes that the web API (resource) app exposes to client apps. These permission scopes may be granted to client apps during consent.\nBox 2: \"oauth2AllowImplicitFlow\":true\nFor applications (Angular, Ember.js, React.js, and so on), Microsoft identity platform supports the OAuth 2.0 Implicit Grant flow.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-manifest",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: \"oauth2Permissions\": [\"login\"]\noauth2Permissions specifies the collection of OAuth 2.0 permission scopes that the web API (resource) app exposes to client apps. These permission scopes may be granted to client apps during consent.\nBox 2: \"oauth2AllowImplicitFlow\":true\nFor applications (Angular, Ember.js, React.js, and so on), Microsoft identity platform supports the OAuth 2.0 Implicit Grant flow.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/reference-app-manifest",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 11",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nHOTSPOT -\nYou need to ensure that network security policies are met.\nHow should you configure network security? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0028500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0029100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Valid root certificate -\nScenario: All websites and services must use SSL from a valid root certificate authority.\nBox 2: Azure Application Gateway\nScenario:\n\u2711 Any web service accessible over the Internet must be protected from cross site scripting attacks.\n\u2711 All Internal services must only be accessible from Internal Virtual Networks (VNets)\nAll parts of the system must support inbound and outbound traffic restrictions.\n\nAzure Web Application Firewall (WAF) on Azure Application Gateway provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks.\nApplication Gateway supports autoscaling, SSL offloading, and end-to-end SSL, a web application firewall (WAF), cookie-based session affinity, URL path-based routing, multisite hosting, redirection, rewrite HTTP headers and other features.\nNote: Both Nginx and Azure Application Gateway act as a reverse proxy with Layer 7 load-balancing features plus a WAF to ensure strong protection against common web vulnerabilities and exploits.\nYou can modify Nginx web server configuration/SSL for X-XSS protection. This helps to prevent cross-site scripting exploits by forcing the injection of HTTP headers with X-XSS protection.\nReference:\nhttps://docs.microsoft.com/en-us/azure/web-application-firewall/ag/ag-overview https://www.upguard.com/articles/10-tips-for-securing-your-nginx-deploymenthttps://www.examtopics.com/assets/media/exam-media/04273/0029200004.png",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Valid root certificate -\nScenario: All websites and services must use SSL from a valid root certificate authority.\nBox 2: Azure Application Gateway\nScenario:\n\u2711 Any web service accessible over the Internet must be protected from cross site scripting attacks.\n\u2711 All Internal services must only be accessible from Internal Virtual Networks (VNets)\nAll parts of the system must support inbound and outbound traffic restrictions.\n\nAzure Web Application Firewall (WAF) on Azure Application Gateway provides centralized protection of your web applications from common exploits and vulnerabilities. Web applications are increasingly targeted by malicious attacks that exploit commonly known vulnerabilities. SQL injection and cross-site scripting are among the most common attacks.\nApplication Gateway supports autoscaling, SSL offloading, and end-to-end SSL, a web application firewall (WAF), cookie-based session affinity, URL path-based routing, multisite hosting, redirection, rewrite HTTP headers and other features.\nNote: Both Nginx and Azure Application Gateway act as a reverse proxy with Layer 7 load-balancing features plus a WAF to ensure strong protection against common web vulnerabilities and exploits.\nYou can modify Nginx web server configuration/SSL for X-XSS protection. This helps to prevent cross-site scripting exploits by forcing the injection of HTTP headers with X-XSS protection.\nReference:\nhttps://docs.microsoft.com/en-us/azure/web-application-firewall/ag/ag-overview https://www.upguard.com/articles/10-tips-for-securing-your-nginx-deployment",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0029200004.png"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 11",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nDRAG DROP -\nYou need to add YAML markup at line CS17 to ensure that the ContentUploadService can access Azure Storage access keys.\nHow should you complete the YAML markup? To answer, drag the appropriate YAML segments to the correct locations. Each YAML segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0028500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0029400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: volumeMounts -\nExample:\nvolumeMounts:\n- mountPath: /mnt/secrets\nname: secretvolume1\nvolumes:\n- name: secretvolume1\nsecret:\nmysecret1: TXkgZmlyc3Qgc2VjcmV0IEZPTwo=\n\nBox 2: volumes -\n\nBox 3: secret -\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-volume-secret",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: volumeMounts -\nExample:\nvolumeMounts:\n- mountPath: /mnt/secrets\nname: secretvolume1\nvolumes:\n- name: secretvolume1\nsecret:\nmysecret1: TXkgZmlyc3Qgc2VjcmV0IEZPTwo=\n\nBox 2: volumes -\n\nBox 3: secret -\nReference:\nhttps://docs.microsoft.com/en-us/azure/container-instances/container-instances-volume-secret",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 11",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nHOTSPOT -\nYou need to add code at line AM10 of the application manifest to ensure that the requirement for manually reviewing content can be met.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0028500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0028600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0029600001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: sid -\nSid: Session ID, used for per-session user sign-out. Personal and Azure AD accounts.\n\nScenario: Manual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role.\n\nBox 2: email -\nScenario: All completed reviews must include the reviewer's email address for auditing purposes.",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: sid -\nSid: Session ID, used for per-session user sign-out. Personal and Azure AD accounts.\n\nScenario: Manual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role.\n\nBox 2: email -\nScenario: All completed reviews must include the reviewer's email address for auditing purposes.",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 12",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible.\n\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:\nOcean Transport `\" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight, contents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports of call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display on the shipping website.\n\nShipping Data -\nThe application uses MongoDB JSON document storage database for all container and transport information.\n\nShipping Web Site -\nThe site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/\n\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:\n\n\nRequirements -\n\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic App.\nSupport industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final BizTalk migrations.\n\nShipping Function app -\nImplement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring additional costs.\nAutomatically scale to double capacity during peak shipping times while not causing application downtime.\nMinimize costs when selecting an Azure payment model.\n\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\n\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nIssues -\n\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\n\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.\nQuestion\nHOTSPOT -\nYou need to secure the Shipping Function app.\nHow should you configure the app? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030000001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030200001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: Shipping Function app: Implement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nBox 1: Function -\nBox 2: JSON based Token (JWT)\nAzure AD uses JSON based tokens (JWTs) that contain claims\n\nBox 3: HTTP -\nHow a web app delegates sign-in to Azure AD and obtains a token\nUser authentication happens via the browser. The OpenID protocol uses standard HTTP protocol messages.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/authentication-scenarios",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: Shipping Function app: Implement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nBox 1: Function -\nBox 2: JSON based Token (JWT)\nAzure AD uses JSON based tokens (JWTs) that contain claims\n\nBox 3: HTTP -\nHow a web app delegates sign-in to Azure AD and obtains a token\nUser authentication happens via the browser. The OpenID protocol uses standard HTTP protocol messages.\nReference:\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/authentication-scenarios",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 12",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible.\n\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:\nOcean Transport `\" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight, contents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports of call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display on the shipping website.\n\nShipping Data -\nThe application uses MongoDB JSON document storage database for all container and transport information.\n\nShipping Web Site -\nThe site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/\n\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:\n\n\nRequirements -\n\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic App.\nSupport industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final BizTalk migrations.\n\nShipping Function app -\nImplement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring additional costs.\nAutomatically scale to double capacity during peak shipping times while not causing application downtime.\nMinimize costs when selecting an Azure payment model.\n\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\n\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nIssues -\n\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\n\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.\nQuestion\nYou need to secure the Shipping Logic App.\nWhat should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030000001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure App Service Environment (ASE)",
            "B. Integration Service Environment (ISE)",
            "C. VNet service endpoint",
            "D. Azure AD B2B integration"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Scenario: The Shipping Logic App requires secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nYou can access to Azure Virtual Network resources from Azure Logic Apps by using integration service environments (ISEs).\nSometimes, your logic apps and integration accounts need access to secured resources, such as virtual machines (VMs) and other systems or services, that are inside an Azure virtual network. To set up this access, you can create an integration service environment (ISE) where you can run your logic apps and create your integration accounts.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/connect-virtual-network-vnet-isolated-environment-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 13",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to retrieve the database connection string.\nWhich values should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900002.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Azure database connection string retrieve REST API vault.azure.net/secrets/\n\nBox 1: cpandlkeyvault -\nWe specify the key vault, cpandlkeyvault.\nScenario: The database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\n\nSecret name: PostgreSQLConn -\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\n\nBox 2: PostgreSQLConn -\nWe specify the secret, PostgreSQLConn\nExample, sample request:\nhttps://myvault.vault.azure.net//secrets/mysecretname/4387e9f3d6e14c459867679a90fd0f79?api-version=7.1\n\nBox 3: Querystring -\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/keyvault/getsecret/getsecret",
        "mostVotedAre": [],
        "descriptionIs": "Azure database connection string retrieve REST API vault.azure.net/secrets/\n\nBox 1: cpandlkeyvault -\nWe specify the key vault, cpandlkeyvault.\nScenario: The database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\n\nSecret name: PostgreSQLConn -\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\n\nBox 2: PostgreSQLConn -\nWe specify the secret, PostgreSQLConn\nExample, sample request:\nhttps://myvault.vault.azure.net//secrets/mysecretname/4387e9f3d6e14c459867679a90fd0f79?api-version=7.1\n\nBox 3: Querystring -\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/keyvault/getsecret/getsecret",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 13",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nDRAG DROP -\nYou need to correct the corporate website error.\nWhich four actions should you recommend be performed in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0031100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: Corporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nStep 1: Generate a certificate -\nStep 2: Upload the certificate to Azure Key Vault\nScenario: All SSL certificates and credentials must be stored in Azure Key Vault.\nStep 3: Import the certificate to Azure App Service\nStep 4: Update line SCO5 of Security.cs to include error handling and then redeploy the code\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/configure-ssl-certificate",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: Corporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nStep 1: Generate a certificate -\nStep 2: Upload the certificate to Azure Key Vault\nScenario: All SSL certificates and credentials must be stored in Azure Key Vault.\nStep 3: Import the certificate to Azure App Service\nStep 4: Update line SCO5 of Security.cs to include error handling and then redeploy the code\nReference:\nhttps://docs.microsoft.com/en-us/azure/app-service/configure-ssl-certificate",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 13",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to configure API Management for authentication.\nWhich policy values should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0031300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Validate JWT -\nThe validate-jwt policy enforces existence and validity of a JWT extracted from either a specified HTTP Header or a specified query parameter.\nScenario: User authentication (see step 5 below)\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\nIncorrect Answers:\n\u2711 Limit call rate by key - Prevents API usage spikes by limiting call rate, on a per key basis.\n\u2711 Restrict caller IPs - Filters (allows/denies) calls from specific IP addresses and/or address ranges.\n\u2711 Check HTTP header - Enforces existence and/or value of a HTTP Header.\n\nBox 2: Outbound -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-access-restriction-policies",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Validate JWT -\nThe validate-jwt policy enforces existence and validity of a JWT extracted from either a specified HTTP Header or a specified query parameter.\nScenario: User authentication (see step 5 below)\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\nIncorrect Answers:\n\u2711 Limit call rate by key - Prevents API usage spikes by limiting call rate, on a per key basis.\n\u2711 Restrict caller IPs - Filters (allows/denies) calls from specific IP addresses and/or address ranges.\n\u2711 Check HTTP header - Enforces existence and/or value of a HTTP Header.\n\nBox 2: Outbound -\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-access-restriction-policies",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 13",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nYou need to authenticate the user to the corporate website as indicated by the architectural diagram.\nWhich two values should you use? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. ID token signature Most Voted",
            "B. ID token claims",
            "C. HTTP response code",
            "D. Azure AD endpoint URI Most Voted",
            "E. Azure AD tenant ID"
        ],
        "answersAre": [
            "A",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "D"
        ],
        "descriptionIs": "A: Claims in access tokens -\nJWTs (JSON Web Tokens) are split into three pieces:\n\u2711 Header - Provides information about how to validate the token including information about the type of token and how it was signed.\n\u2711 Payload - Contains all of the important data about the user or app that is attempting to call your service.\n\u2711 Signature - Is the raw material used to validate the token.\nE: Your client can get an access token from either the v1.0 endpoint or the v2.0 endpoint using a variety of protocols.\nScenario: User authentication (see step 5 below)\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\nReference:\nhttps://docs.microsoft.com/en-us/azure/api-management/api-management-access-restriction-policies",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #5 -- Topic 13",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to correct the Azure Logic app error message.\nWhich configuration values should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0031600001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: You test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\nNote: If the inbound call's request body doesn't match your schema, the trigger returns an HTTP 400 Bad Request error.\n\nBox 1: function -\nIf you have an Azure function where you want to use the system-assigned identity, first enable authentication for Azure functions.\n\nBox 2: system-assigned -\nYour logic app or individual connections can use either the system-assigned identity or a single user-assigned identity, which you can share across a group of logic apps, but not both.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: You test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\nNote: If the inbound call's request body doesn't match your schema, the trigger returns an HTTP 400 Bad Request error.\n\nBox 1: function -\nIf you have an Azure function where you want to use the system-assigned identity, first enable authentication for Azure functions.\n\nBox 2: system-assigned -\nYour logic app or individual connections can use either the system-assigned identity or a single user-assigned identity, which you can share across a group of logic apps, but not both.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/create-managed-service-identity",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #6 -- Topic 13",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to configure Azure Service Bus to Event Grid integration.\nWhich Azure Service Bus settings should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0030600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0030900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0031800001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Premium -\nService Bus can now emit events to Event Grid when there are messages in a queue or a subscription when no receivers are present. You can create Event Grid subscriptions to your Service Bus namespaces, listen to these events, and then react to the events by starting a receiver. With this feature, you can use Service\nBus in reactive programming models.\nTo enable the feature, you need the following items:\nA Service Bus Premium namespace with at least one Service Bus queue or a Service Bus topic with at least one subscription.\nContributor access to the Service Bus namespace.\n\nBox 2: Contributor -\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-to-event-grid-integration-concept",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Premium -\nService Bus can now emit events to Event Grid when there are messages in a queue or a subscription when no receivers are present. You can create Event Grid subscriptions to your Service Bus namespaces, listen to these events, and then react to the events by starting a receiver. With this feature, you can use Service\nBus in reactive programming models.\nTo enable the feature, you need the following items:\nA Service Bus Premium namespace with at least one Service Bus queue or a Service Bus topic with at least one subscription.\nContributor access to the Service Bus namespace.\n\nBox 2: Contributor -\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-to-event-grid-integration-concept",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 14",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nHOTSPOT -\nYou need to add code at line PC26 of Processing.cs to ensure that security policies are met.\nHow should you complete the code that you will add at line PC26? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0032100001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032300001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032400001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032600001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032700001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: var key = await Resolver.ResolveKeyAsyn(keyBundle,KeyIdentifier.CancellationToken.None);\nBox 2: var x = new BlobEncryptionPolicy(key,resolver);\nExample:\n// We begin with cloudKey1, and a resolver capable of resolving and caching Key Vault secrets.\nBlobEncryptionPolicy encryptionPolicy = new BlobEncryptionPolicy(cloudKey1, cachingResolver); client.DefaultRequestOptions.EncryptionPolicy = encryptionPolicy;\nBox 3: cloudblobClient. DefaultRequestOptions.EncryptionPolicy = x;\nReference:\nhttps://github.com/Azure/azure-storage-net/blob/master/Samples/GettingStarted/EncryptionSamples/KeyRotation/Program.cs",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: var key = await Resolver.ResolveKeyAsyn(keyBundle,KeyIdentifier.CancellationToken.None);\nBox 2: var x = new BlobEncryptionPolicy(key,resolver);\nExample:\n// We begin with cloudKey1, and a resolver capable of resolving and caching Key Vault secrets.\nBlobEncryptionPolicy encryptionPolicy = new BlobEncryptionPolicy(cloudKey1, cachingResolver); client.DefaultRequestOptions.EncryptionPolicy = encryptionPolicy;\nBox 3: cloudblobClient. DefaultRequestOptions.EncryptionPolicy = x;\nReference:\nhttps://github.com/Azure/azure-storage-net/blob/master/Samples/GettingStarted/EncryptionSamples/KeyRotation/Program.cs",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 14",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nYou need to ensure the security policies are met.\nWhat code do you add at line CS07 of ConfigureSSE.ps1?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0032100001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032300001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032400001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032500001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0032600001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. \u05d2\u20ac\"PermissionsToKeys create, encrypt, decrypt",
            "B. \u05d2\u20ac\"PermissionsToCertificates create, encrypt, decrypt",
            "C. \u05d2\u20ac\"PermissionsToCertificates wrapkey, unwrapkey, get",
            "D. \u05d2\u20ac\"PermissionsToKeys wrapkey, unwrapkey, get Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Scenario: All certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nThe Set-AzureRmKeyValutAccessPolicy parameter -PermissionsToKeys specifies an array of key operation permissions to grant to a user or service principal.\nThe acceptable values for this parameter: decrypt, encrypt, unwrapKey, wrapKey, verify, sign, get, list, update, create, import, delete, backup, restore, recover, purge\nIncorrect Answers:\nA, C: The Set-AzureRmKeyValutAccessPolicy parameter -PermissionsToCertificates specifies an array of certificate permissions to grant to a user or service principal. The acceptable values for this parameter: get, list, delete, create, import, update, managecontacts, getissuers, listissuers, setissuers, deleteissuers, manageissuers, recover, purge, backup, restore\nReference:\nhttps://docs.microsoft.com/en-us/powershell/module/azurerm.keyvault/set-azurermkeyvaultaccesspolicy",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 15",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nYou need to reduce read latency for the retail store solution.\nWhat are two possible ways to achieve the goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create a new composite index for the store location data queries in Azure Cosmos DB. Modify the queries to support parameterized SQL and update the Azure Function app to call the new queries.",
            "B. Provision an Azure Cosmos DB dedicated gateway. Update the Azure Function app connection string to use the new dedicated gateway endpoint.",
            "C. Configure Azure Cosmos DB consistency to session consistency. Cache session tokens in a new Azure Redis cache instance after every write. Update reads to use the session token stored in Azure Redis.",
            "D. Provision an Azure Cosmos DB dedicated gateway. Update blob storage to use the new dedicated gateway endpoint.",
            "E. Configure Azure Cosmos DB consistency to strong consistency. Increase the RUs for the container supporting store location data."
        ],
        "answersAre": [
            "B",
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Azure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nB: A dedicated gateway is server-side compute that is a front-end to your Azure Cosmos DB account. When you connect to the dedicated gateway, it both routes requests and caches data.\nYou can provision a dedicated gateway to improve performance at scale.\nYou must connect to Azure Cosmos DB using the dedicated gateway in order to use the integrated cache. The dedicated gateway has a different endpoint from the standard one provided with your Azure Cosmos DB account. When you connect to your dedicated gateway endpoint, your application sends a request to the dedicated gateway, which then routes the request to different backend nodes. If possible, the integrated cache will serve the result.\nC: Azure Cache for Redis perfectly complements Azure database services such as Cosmos DB. It provides a cost-effective solution to scale read and write throughput of your data tier. Store and share database query results, session states, static contents, and more using a common cache-aside pattern.\nReference:\nhttps://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/data-cache-with-redis-cache https://docs.microsoft.com/en-us/azure/cosmos-db/dedicated-gateway",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 15",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nYou need to audit the retail store sales transactions.\nWhat are two possible ways to achieve the goal? Each correct answer presents a complete solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Update the retail store location data upload process to include blob index tags. Create an Azure Function to process the blob index tags and filter by store location.",
            "B. Process the change feed logs of the Azure Blob storage account by using an Azure Function. Specify a time range for the change feed data. Most Voted",
            "C. Enable blob versioning for the storage account. Use an Azure Function to process a list of the blob versions per day.",
            "D. Process an Azure Storage blob inventory report by using an Azure Function. Create rule filters on the blob inventory report.",
            "E. Subscribe to blob storage events by using an Azure Function and Azure Event Grid. Filter the events by store location. Most Voted"
        ],
        "answersAre": [
            "B",
            "E"
        ],
        "mostVotedAre": [
            "B",
            "E"
        ],
        "descriptionIs": "Scenario: Audit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\"Process the change feed logs of the Azure Blob storage account by using an Azure Function. Specify a time range for the change feed data\": Change feed support is well-suited for scenarios that process data based on objects that have changed. For example, applications can:\nStore, audit, and analyze changes to your objects, over any period of time, for security, compliance or intelligence for enterprise data management.\n\"Subscribe to blob storage events by using an Azure Function and Azure Event Grid. Filter the events by store location\": Azure Storage events allow applications to react to events, such as the creation and deletion of blobs. It does so without the need for complicated code or expensive and inefficient polling services. The best part is you only pay for what you use.\nBlob storage events are pushed using Azure Event Grid to subscribers such as Azure Functions, Azure Logic Apps, or even to your own http listener. Event Grid provides reliable event delivery to your applications through rich retry policies and dead-lettering.\nIncorrect Answers:\n\"Enable blob versioning for the storage account. Use an Azure Function to process a list of the blob versions per day\": You can enable Blob storage versioning to automatically maintain previous versions of an object. When blob versioning is enabled, you can access earlier versions of a blob to recover your data if it is modified or deleted.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-event-overview",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 16",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nYou need to monitor ContentUploadService according to the requirements.\nWhich command should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0042100001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042200001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. az monitor metrics alert create \u05d2\u20ac\"n alert \u05d2\u20ac\"g \u05d2\u20ac\u00a6 - -scopes \u05d2\u20ac\u00a6 - -condition \"avg Percentage CPU > 8\"",
            "B. az monitor metrics alert create \u05d2\u20ac\"n alert \u05d2\u20ac\"g \u05d2\u20ac\u00a6 - -scopes \u05d2\u20ac\u00a6 - -condition \"avg Percentage CPU > 800\"",
            "C. az monitor metrics alert create \u05d2\u20ac\"n alert \u05d2\u20ac\"g \u05d2\u20ac\u00a6 - -scopes \u05d2\u20ac\u00a6 - -condition \"CPU Usage > 800\" Most Voted",
            "D. az monitor metrics alert create \u05d2\u20ac\"n alert \u05d2\u20ac\"g \u05d2\u20ac\u00a6 - -scopes \u05d2\u20ac\u00a6 - -condition \"CPU Usage > 8\""
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Scenario: An alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\nReference:\nhttps://docs.microsoft.com/sv-se/cli/azure/monitor/metrics/alert",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 16",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nYou need to investigate the http server log output to resolve the issue with the ContentUploadService.\nWhich command should you use first?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0042100001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042200001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. az webapp log",
            "B. az ams live-output",
            "C. az monitor activity-log",
            "D. az container attach Most Voted"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Scenario: Users of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\"502 bad gateway\" and \"503 service unavailable\" are common errors in your app hosted in Azure App Service.\nMicrosoft Azure publicizes each time there is a service interruption or performance degradation.\nThe az monitor activity-log command manages activity logs.\nNote: Troubleshooting can be divided into three distinct tasks, in sequential order:\n1. Observe and monitor application behavior\n2. Collect data\n3. Mitigate the issue\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/monitor/activity-log",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 17",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nYou need to investigate the Azure Function app error message in the development environment.\nWhat should you do?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0042500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042800001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Connect Live Metrics Stream from Application Insights to the Azure Function app and filter the metrics.",
            "B. Create a new Azure Log Analytics workspace and instrument the Azure Function app with Application Insights.",
            "C. Update the Azure Function app with extension methods from Microsoft.Extensions.Logging to log events by using the log instance.",
            "D. Add a new diagnostic setting to the Azure Function app to send logs to Log Analytics."
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Azure Functions offers built-in integration with Azure Application Insights to monitor functions.\nThe following areas of Application Insights can be helpful when evaluating the behavior, performance, and errors in your functions:\nLive Metrics: View metrics data as it's created in near real-time.\n\nFailures -\n\nPerformance -\n\nMetrics -\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-monitoring",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 17",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to configure security and compliance for the corporate website files.\nWhich Azure Blob storage settings should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0042500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0042900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: role-based access control (RBAC)\nAzure Storage supports authentication and authorization with Azure AD for the Blob and Queue services via Azure role-based access control (Azure RBAC).\nScenario: File access must restrict access by IP, protocol, and Azure AD rights.\n\nBox 2: storage account type -\nScenario: The website uses files stored in Azure Storage\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR).\nCreating a diagnostic setting:\n1. Sign in to the Azure portal.\n2. Navigate to your storage account.\n3. In the Monitoring section, click Diagnostic settings (preview).\n\n4. Choose file as the type of storage that you want to enable logs for.\n5. Click Add diagnostic setting.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-introduction https://docs.microsoft.com/en-us/azure/storage/files/storage-files-monitoringhttps://www.examtopics.com/assets/media/exam-media/04273/0043100001.jpg",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: role-based access control (RBAC)\nAzure Storage supports authentication and authorization with Azure AD for the Blob and Queue services via Azure role-based access control (Azure RBAC).\nScenario: File access must restrict access by IP, protocol, and Azure AD rights.\n\nBox 2: storage account type -\nScenario: The website uses files stored in Azure Storage\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR).\nCreating a diagnostic setting:\n1. Sign in to the Azure portal.\n2. Navigate to your storage account.\n3. In the Monitoring section, click Diagnostic settings (preview).\n\n4. Choose file as the type of storage that you want to enable logs for.\n5. Click Add diagnostic setting.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-introduction https://docs.microsoft.com/en-us/azure/storage/files/storage-files-monitoring",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0043100001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 18",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nYou need to correct the RequestUserApproval Function app error.\nWhat should you do?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0004900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0005100001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0005200001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Update line RA13 to use the async keyword and return an HttpRequest object value.",
            "B. Configure the Function app to use an App Service hosting plan. Enable the Always On setting of the hosting plan.",
            "C. Update the function to be stateful by using Durable Functions to process the request payload. Most Voted",
            "D. Update the functionTimeout property of the host.json project file to 15 minutes."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Async operation tracking -\nThe HTTP response mentioned previously is designed to help implement long-running HTTP async APIs with Durable Functions. This pattern is sometimes referred to as the polling consumer pattern.\nBoth the client and server implementations of this pattern are built into the Durable Functions HTTP APIs.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/durable/durable-functions-http-features",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 19",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nDRAG DROP -\nYou need to implement the Log policy.\nHow should you complete the Azure Event Grid subscription? To answer, drag the appropriate JSON segments to the correct locations. Each JSON segment may be used once, more than once, or not at all. You may need to drag the split bar between panes to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0043500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0043600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0043700001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1:WebHook -\nScenario: If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook. endpointType: The type of endpoint for the subscription (webhook/HTTP, Event Hub, or queue).\n\nBox 2: SubjectBeginsWith -\nBox 3: Microsoft.Storage.BlobCreated\n\nScenario: Log Policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nExample subscription schema -\n{\n\"properties\": {\n\"destination\": {\n\"endpointType\": \"webhook\",\n\"properties\": {\n\"endpointUrl\": \"https://example.azurewebsites.net/api/HttpTriggerCSharp1?code=VXbGWce53l48Mt8wuotr0GPmyJ/nDT4hgdFj9DpBiRt38qqnnm5OFg==\"\n}\n},\n\"filter\": {\n\"includedEventTypes\": [ \"Microsoft.Storage.BlobCreated\", \"Microsoft.Storage.BlobDeleted\" ],\n\"subjectBeginsWith\": \"blobServices/default/containers/mycontainer/log\",\n[1]\n\"isSubjectCaseSensitive \": \"true\"\n}\n}\n}\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/subscription-creation-schema",
        "mostVotedAre": [],
        "descriptionIs": "Box 1:WebHook -\nScenario: If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook. endpointType: The type of endpoint for the subscription (webhook/HTTP, Event Hub, or queue).\n\nBox 2: SubjectBeginsWith -\nBox 3: Microsoft.Storage.BlobCreated\n\nScenario: Log Policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nExample subscription schema -\n{\n\"properties\": {\n\"destination\": {\n\"endpointType\": \"webhook\",\n\"properties\": {\n\"endpointUrl\": \"https://example.azurewebsites.net/api/HttpTriggerCSharp1?code=VXbGWce53l48Mt8wuotr0GPmyJ/nDT4hgdFj9DpBiRt38qqnnm5OFg==\"\n}\n},\n\"filter\": {\n\"includedEventTypes\": [ \"Microsoft.Storage.BlobCreated\", \"Microsoft.Storage.BlobDeleted\" ],\n\"subjectBeginsWith\": \"blobServices/default/containers/mycontainer/log\",\n[1]\n\"isSubjectCaseSensitive \": \"true\"\n}\n}\n}\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/subscription-creation-schema",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 19",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nYou need to ensure that the solution can meet the scaling requirements for Policy Service.\nWhich Azure Application Insights data model should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0043500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0043600001.png"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. an Application Insights dependency",
            "B. an Application Insights event",
            "C. an Application Insights trace",
            "D. an Application Insights metric Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "Application Insights provides three additional data types for custom telemetry:\nTrace - used either directly, or through an adapter to implement diagnostics logging using an instrumentation framework that is familiar to you, such as Log4Net or\nSystem.Diagnostics.\nEvent - typically used to capture user interaction with your service, to analyze usage patterns.\nMetric - used to report periodic scalar measurements.\nScenario:\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/data-model",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 19",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nDRAG DROP -\nYou need to implement telemetry for non-user actions.\nHow should you complete the Filter class? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0043500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0043600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: Exclude non-user actions from Application Insights telemetry.\n\nBox 1: ITelemetryProcessor -\nTo create a filter, implement ITelemetryProcessor. This technique gives you more direct control over what is included or excluded from the telemetry stream.\n\nBox 2: ITelemetryProcessor -\n\nBox 3: ITelemetryProcessor -\n\nBox 4: RequestTelemetry -\n\nBox 5: /health -\nTo filter out an item, just terminate the chain.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: Exclude non-user actions from Application Insights telemetry.\n\nBox 1: ITelemetryProcessor -\nTo create a filter, implement ITelemetryProcessor. This technique gives you more direct control over what is included or excluded from the telemetry stream.\n\nBox 2: ITelemetryProcessor -\n\nBox 3: ITelemetryProcessor -\n\nBox 4: RequestTelemetry -\n\nBox 5: /health -\nTo filter out an item, just terminate the chain.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #4 -- Topic 19",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nDRAG DROP -\nYou need to ensure that PolicyLib requirements are met.\nHow should you complete the code segment? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0043500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0043600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: You have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\n\u2711 Exclude non-user actions from Application Insights telemetry.\n\u2711 Provide methods that allow a web service to scale itself.\n\u2711 Ensure that scaling actions do not disrupt application usage.\n\nBox 1: ITelemetryInitializer -\nUse telemetry initializers to define global properties that are sent with all telemetry; and to override selected behavior of the standard telemetry modules.\n\nBox 2: Initialize -\n\nBox 3: Telemetry.Context -\nBox 4: ((EventTelemetry)telemetry).Properties[\"EventID\"]\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: You have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\n\u2711 Exclude non-user actions from Application Insights telemetry.\n\u2711 Provide methods that allow a web service to scale itself.\n\u2711 Ensure that scaling actions do not disrupt application usage.\n\nBox 1: ITelemetryInitializer -\nUse telemetry initializers to define global properties that are sent with all telemetry; and to override selected behavior of the standard telemetry modules.\n\nBox 2: Initialize -\n\nBox 3: Telemetry.Context -\nBox 4: ((EventTelemetry)telemetry).Properties[\"EventID\"]\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 20",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nYou need to ensure receipt processing occurs correctly.\nWhat should you do?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0044500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044900002.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Use blob properties to prevent concurrency problems",
            "B. Use blob SnapshotTime to prevent concurrency problems",
            "C. Use blob metadata to prevent concurrency problems",
            "D. Use blob leases to prevent concurrency problems Most Voted"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "You can create a snapshot of a blob. A snapshot is a read-only version of a blob that's taken at a point in time. Once a snapshot has been created, it can be read, copied, or deleted, but not modified. Snapshots provide a way to back up a blob as it appears at a moment in time.\nScenario: Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in\nAzure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\nReference:\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/creating-a-snapshot-of-a-blob",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 20",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nYou need to resolve the capacity issue.\nWhat should you do?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0044500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044900002.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Convert the trigger on the Azure Function to an Azure Blob storage trigger",
            "B. Ensure that the consumption plan is configured correctly to allow scaling",
            "C. Move the Azure Function to a dedicated App Service Plan",
            "D. Update the loop starting on line PC09 to process items in parallel Most Voted"
        ],
        "answersAre": [
            "D"
        ],
        "mostVotedAre": [
            "D"
        ],
        "descriptionIs": "If you want to read the files in parallel, you cannot use forEach. Each of the async callback function calls does return a promise. You can await the array of promises that you'll get with Promise.all.\nScenario: Capacity issue: During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nReference:\nhttps://stackoverflow.com/questions/37576685/using-async-await-with-a-foreach-loop",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0045100001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 20",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Litware Inc., a SaaS company that provides a solution for managing employee expenses. The solution consists of an ASP.NET Core Web\nAPI project that is deployed as an Azure Web App.\n\nOverall architecture -\nEmployees upload receipts for the system to process. When processing is complete, the employee receives a summary report email that details the processing results. Employees then use a web application to manage their receipts and perform any additional tasks needed for reimbursement.\n\nReceipt processing -\nEmployees may upload receipts in two ways:\nUploading using an Azure Files mounted folder\nUploading using the web application\n\nData Storage -\nReceipt and employee information is stored in an Azure SQL database.\n\nDocumentation -\nEmployees are provided with a getting started document when they first use the solution. The documentation includes details on supported operating systems for\nAzure File upload, and instructions on how to configure the mounted folder.\n\nSolution details -\n\nUsers table -\n\n\nWeb Application -\nYou enable MSI for the Web App and configure the Web App to use the security principal name WebAppIdentity.\n\nProcessing -\nProcessing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in Azure Blob\nStorage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\n\nLogging -\nAzure Application Insights is used for telemetry and logging in both the processor and the web application. The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\n\nRequirements -\n\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\n\nDisaster recovery -\nRegional outage must not impact application availability. All DR operations must not be dependent on application running and must ensure that data in the DR region is up to date.\n\nSecurity -\nUser's SecurityPin must be stored in such a way that access to the database does not allow the viewing of SecurityPins. The web application is the only system that should have access to SecurityPins.\nAll certificates and secrets used to secure data must be stored in Azure Key Vault.\nYou must adhere to the principle of least privilege and provide privileges which are essential to perform the intended function.\nAll access to Azure Storage and Azure SQL database must use the application's Managed Service Identity (MSI).\nReceipt data must always be encrypted at rest.\nAll data must be protected in transit.\nUser's expense account number must be visible only to logged in users. All other views of the expense account number should include only the last segment, with the remaining parts obscured.\nIn the case of a security breach, access to all summary reports must be revoked without impacting other parts of the system.\n\nIssues -\n\nUpload format issue -\nEmployees occasionally report an issue with uploading a receipt using the web application. They report that when they upload a receipt using the Azure File\nShare, the receipt does not appear in their profile. When this occurs, they delete the file in the file share and use the web application, which returns a 500 Internal\nServer error page.\n\nCapacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nLog capacity issue -\nDevelopers report that the number of log messages in the trace output for the processor is too high, resulting in lost log messages.\n\nApplication code -\n\nProcessing.cs -\n\n\nDatabase.cs -\n\n\nReceiptUploader.cs -\n\n\nConfigureSSE.ps1 -\nQuestion\nYou need to resolve the log capacity issue.\nWhat should you do?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0044500001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044700001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0044900002.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Create an Application Insights Telemetry Filter",
            "B. Change the minimum log level in the host.json file for the function",
            "C. Implement Application Insights Sampling",
            "D. Set a LogCategoryFilter during startup"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Scenario, the log capacity issue: Developers report that the number of log message in the trace output for the processor is too high, resulting in lost log messages.\nSampling is a feature in Azure Application Insights. It is the recommended way to reduce telemetry traffic and storage, while preserving a statistically correct analysis of application data. The filter selects items that are related, so that you can navigate between items when you are doing diagnostic investigations. When metric counts are presented to you in the portal, they are renormalized to take account of the sampling, to minimize any effect on the statistics.\nSampling reduces traffic and data costs, and helps you avoid throttling.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 21",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nHOTSPOT -\nYou need to implement event routing for retail store location data.\nWhich configurations should you use? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0050900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Azure Blob Storage -\nAzure event publishers and event handlers are at the core of the Event Grid routing-service. Event Grid listens to Azure event publishers, such as Blog Storage, then reacts by routing specific events to Azure event handlers, such as WebHooks. You can easily control this entire process at a granular level through event subscriptions and event filters.\n\nBox 2: Azure Event Grid -\nAzure Event Grid is a highly scalable event-routing service that listens for specific system events, then reacts to them according to your precise specifications. In the past, event handling has relied largely on polling \u05d2\u20ac\" a high latency, low-efficiency approach that can prove prohibitively expensive at scale.\n\nBox 3: Azure Logic App -\nEvent Grid's supported event handlers currently include Event Hubs, WebHooks, Logic Apps, Azure Functions, Azure Automation and Microsoft Flow.\nReference:\nhttps://www.appliedi.net/blog/using-azure-event-grid-for-highly-scalable-event-routing",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Azure Blob Storage -\nAzure event publishers and event handlers are at the core of the Event Grid routing-service. Event Grid listens to Azure event publishers, such as Blog Storage, then reacts by routing specific events to Azure event handlers, such as WebHooks. You can easily control this entire process at a granular level through event subscriptions and event filters.\n\nBox 2: Azure Event Grid -\nAzure Event Grid is a highly scalable event-routing service that listens for specific system events, then reacts to them according to your precise specifications. In the past, event handling has relied largely on polling \u05d2\u20ac\" a high latency, low-efficiency approach that can prove prohibitively expensive at scale.\n\nBox 3: Azure Logic App -\nEvent Grid's supported event handlers currently include Event Hubs, WebHooks, Logic Apps, Azure Functions, Azure Automation and Microsoft Flow.\nReference:\nhttps://www.appliedi.net/blog/using-azure-event-grid-for-highly-scalable-event-routing",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 22",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You are a developer implementing highly scalable and resilient applications to support online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles. The application sends data to several printers. The application consists of five modules that run independently on virtual machines (VMs). Coho Winery plans to move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork and text for custom label designs.\n\nRequirements. Data -\nYou identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\n\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to documents, resources, and applications to external partners.\n\nExternal partners must use their own credentials and authenticate with their organization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user account administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key Vault.\nE-commerce application sign-ins must be secured by using Azure App Service authentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect company content.\nThe LabelMaker application must be secured by using an AAD account that has full access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\n\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\n\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\n\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included for reference only.\nThis JSON file contains a representation of the data for an order that includes a single item.\nQuestion\nYou need to troubleshoot the order workflow.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0051200003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0051400001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0051600001.png"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Review the API connections.",
            "B. Review the activity log.",
            "C. Review the run history.",
            "D. Review the trigger history."
        ],
        "answersAre": [
            "C",
            "D"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Scenario: The order workflow fails to run upon initial deployment to Azure.\nCheck runs history: Each time that the trigger fires for an item or event, the Logic Apps engine creates and runs a separate workflow instance for each item or event. If a run fails, follow these steps to review what happened during that run, including the status for each step in the workflow plus the inputs and outputs for each step.\nCheck the workflow's run status by checking the runs history. To view more information about a failed run, including all the steps in that run in their status, select the failed run.\nExample:\n\nCheck the trigger's status by checking the trigger history\nTo view more information about the trigger attempt, select that trigger event, for example:\n\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-diagnosing-failures",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0051800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0051800002.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 22",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You are a developer implementing highly scalable and resilient applications to support online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles. The application sends data to several printers. The application consists of five modules that run independently on virtual machines (VMs). Coho Winery plans to move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork and text for custom label designs.\n\nRequirements. Data -\nYou identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\n\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to documents, resources, and applications to external partners.\n\nExternal partners must use their own credentials and authenticate with their organization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user account administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key Vault.\nE-commerce application sign-ins must be secured by using Azure App Service authentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect company content.\nThe LabelMaker application must be secured by using an AAD account that has full access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\n\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\n\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\n\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included for reference only.\nThis JSON file contains a representation of the data for an order that includes a single item.\nQuestion\nHOTSPOT -\nYou need to update the order workflow to address the issue when calling the Printer API App.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0051200003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0051400001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0051600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0051900001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: fixed -\nThe 'Default' policy does 4 exponential retries and from my experience the interval times are often too short in situations.\n\nBox 2: PT60S -\nWe could set a fixed interval, e.g. 5 retries every 60 seconds (PT60S).\nPT60S is 60 seconds.\nScenario: Calls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\n\nBox 3: 5 -\nReference:\nhttps://michalsacewicz.com/error-handling-in-power-automate/",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: fixed -\nThe 'Default' policy does 4 exponential retries and from my experience the interval times are often too short in situations.\n\nBox 2: PT60S -\nWe could set a fixed interval, e.g. 5 retries every 60 seconds (PT60S).\nPT60S is 60 seconds.\nScenario: Calls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\n\nBox 3: 5 -\nReference:\nhttps://michalsacewicz.com/error-handling-in-power-automate/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 23",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible.\n\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:\nOcean Transport `\" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight, contents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports of call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display on the shipping website.\n\nShipping Data -\nThe application uses MongoDB JSON document storage database for all container and transport information.\n\nShipping Web Site -\nThe site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/\n\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:\n\n\nRequirements -\n\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic App.\nSupport industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final BizTalk migrations.\n\nShipping Function app -\nImplement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring additional costs.\nAutomatically scale to double capacity during peak shipping times while not causing application downtime.\nMinimize costs when selecting an Azure payment model.\n\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\n\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nIssues -\n\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\n\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.\nQuestion\nDRAG DROP -\nYou need to support the message processing for the ocean transport workflow.\nWhich four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0052300001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0052500001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Create an integration account in the Azure portal\nYou can define custom metadata for artifacts in integration accounts and get that metadata during runtime for your logic app to use. For example, you can provide metadata for artifacts, such as partners, agreements, schemas, and maps - all store metadata using key-value pairs.\nStep 2: Link the Logic App to the integration account\nA logic app that's linked to the integration account and artifact metadata you want to use.\nStep 3: Add partners, schemas, certificates, maps, and agreements\nStep 4: Create a custom connector for the Logic App.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/azure/logic-apps/logic-apps-enterprise-integration-metadata",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Create an integration account in the Azure portal\nYou can define custom metadata for artifacts in integration accounts and get that metadata during runtime for your logic app to use. For example, you can provide metadata for artifacts, such as partners, agreements, schemas, and maps - all store metadata using key-value pairs.\nStep 2: Link the Logic App to the integration account\nA logic app that's linked to the integration account and artifact metadata you want to use.\nStep 3: Add partners, schemas, certificates, maps, and agreements\nStep 4: Create a custom connector for the Logic App.\nReference:\nhttps://docs.microsoft.com/bs-latn-ba/azure/logic-apps/logic-apps-enterprise-integration-metadata",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 23",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nWide World Importers is moving all their datacenters to Azure. The company has developed several applications and services to support supply chain operations and would like to leverage serverless computing where possible.\n\nCurrent environment -\nWindows Server 2016 virtual machine\nThis virtual machine (VM) runs BizTalk Server 2016. The VM runs the following workflows:\nOcean Transport `\" This workflow gathers and validates container information including container contents and arrival notices at various shipping ports.\nInland Transport `\" This workflow gathers and validates trucking information including fuel usage, number of stops, and routes.\nThe VM supports the following REST API calls:\nContainer API `\" This API provides container information including weight, contents, and other attributes.\nLocation API `\" This API provides location information regarding shipping ports of call and trucking stops.\nShipping REST API `\" This API provides shipping information for use and display on the shipping website.\n\nShipping Data -\nThe application uses MongoDB JSON document storage database for all container and transport information.\n\nShipping Web Site -\nThe site displays shipping container tracking information and container contents. The site is located at http://shipping.wideworldimporters.com/\n\nProposed solution -\nThe on-premises shipping application must be moved to Azure. The VM has been migrated to a new Standard_D16s_v3 Azure VM by using Azure Site Recovery and must remain running in Azure to complete the BizTalk component migrations. You create a Standard_D16s_v3 Azure VM to host BizTalk Server. The Azure architecture diagram for the proposed solution is shown below:\n\n\nRequirements -\n\nShipping Logic app -\nThe Shipping Logic app must meet the following requirements:\nSupport the ocean transport and inland transport workflows by using a Logic App.\nSupport industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\nSecure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\nMaintain on-premises connectivity to support legacy applications and final BizTalk migrations.\n\nShipping Function app -\nImplement secure function endpoints by using app-level security and include Azure Active Directory (Azure AD).\n\nREST APIs -\nThe REST API's that support the solution must meet the following requirements:\nSecure resources to the corporate VNet.\nAllow deployment to a testing location within Azure while not incurring additional costs.\nAutomatically scale to double capacity during peak shipping times while not causing application downtime.\nMinimize costs when selecting an Azure payment model.\n\nShipping data -\nData migration from on-premises to Azure must minimize costs and downtime.\n\nShipping website -\nUse Azure Content Delivery Network (CDN) and ensure maximum performance for dynamic content while minimizing latency and costs.\n\nIssues -\n\nWindows Server 2016 VM -\nThe VM shows high network latency, jitter, and high CPU utilization. The VM is critical and has not been backed up in the past. The VM must enable a quick restore from a 7-day snapshot to include in-place restore of disks in case of failure.\n\nShipping website and REST APIs -\nThe following error message displays while you are testing the website:\nFailed to load http://test-shippingapi.wideworldimporters.com/: No 'Access-Control-Allow-Origin' header is present on the requested resource. Origin 'http://test.wideworldimporters.com/' is therefore not allowed access.\nQuestion\nYou need to support the requirements for the Shipping Logic App.\nWhat should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0052300001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure Active Directory Application Proxy",
            "B. Site-to-Site (S2S) VPN connection",
            "C. On-premises Data Gateway",
            "D. Point-to-Site (P2S) VPN connection"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Before you can connect to on-premises data sources from Azure Logic Apps, download and install the on-premises data gateway on a local computer. The gateway works as a bridge that provides quick data transfer and encryption between data sources on premises (not in the cloud) and your logic apps.\nThe gateway supports BizTalk Server 2016.\nNote: Microsoft have now fully incorporated the Azure BizTalk Services capabilities into Logic Apps and Azure App Service Hybrid Connections.\nLogic Apps Enterprise Integration pack bring some of the enterprise B2B capabilities like AS2 and X12, EDI standards support\nScenario: The Shipping Logic app must meet the following requirements:\n\u2711 Support the ocean transport and inland transport workflows by using a Logic App.\n\u2711 Support industry-standard protocol X12 message format for various messages including vessel content details and arrival notices.\n\u2711 Secure resources to the corporate VNet and use dedicated storage resources with a fixed costing model.\n\u2711 Maintain on-premises connectivity to support legacy applications and final BizTalk migrations.\nReference:\nhttps://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-gateway-install",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 24",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to configure the integration for Azure Service Bus and Azure Event Grid.\nHow should you complete the CLI statement? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0052900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053100001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053200001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053200002.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: eventgrid -\nTo create event subscription use: az eventgrid event-subscription create\n\nBox 2: event-subscription -\n\nBox 3: servicebusqueue -\nScenario: Azure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/eventgrid/event-subscription?view=azure-cli-latest#az_eventgrid_event_subscription_create",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: eventgrid -\nTo create event subscription use: az eventgrid event-subscription create\n\nBox 2: event-subscription -\n\nBox 3: servicebusqueue -\nScenario: Azure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\nReference:\nhttps://docs.microsoft.com/en-us/cli/azure/eventgrid/event-subscription?view=azure-cli-latest#az_eventgrid_event_subscription_create",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 24",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nYou need to ensure that all messages from Azure Event Grid are processed.\nWhat should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0052900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053100001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053200001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure Event Grid topic",
            "B. Azure Service Bus topic",
            "C. Azure Service Bus queue",
            "D. Azure Storage queue",
            "E. Azure Logic App custom connector"
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [],
        "descriptionIs": "As a solution architect/developer, you should consider using Service Bus queues when:\n\u2711 Your solution needs to receive messages without having to poll the queue. With Service Bus, you can achieve it by using a long-polling receive operation using the TCP-based protocols that Service Bus supports.\nReference:\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 25",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nDRAG DROP -\nYou need to add code at line EG15 in EventGridController.cs to ensure that the Log policy applies to all services.\nHow should you complete the code? To answer, drag the appropriate code segments to the correct locations. Each code segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.\nNOTE: Each correct selection is worth one point.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0053800001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0054000001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario, Log policy: All Azure App Service Web Apps must write logs to Azure Blob storage.\n\nBox 1: Status -\n\nBox 2: Succeeded -\n\nBox 3: operationName -\nMicrosoft.Web/sites/write is resource provider operation. It creates a new Web App or updates an existing one.\nReference:\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations",
        "mostVotedAre": [],
        "descriptionIs": "Scenario, Log policy: All Azure App Service Web Apps must write logs to Azure Blob storage.\n\nBox 1: Status -\n\nBox 2: Succeeded -\n\nBox 3: operationName -\nMicrosoft.Web/sites/write is resource provider operation. It creates a new Web App or updates an existing one.\nReference:\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 25",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nHOTSPOT -\nYou need to insert code at line LE03 of LoginEvent.cs to ensure that all authentication events are processed correctly.\nHow should you complete the code? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0053800001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0054200001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: id -\nid is a unique identifier for the event.\n\nBox 2: eventType -\neventType is one of the registered event types for this event source.\n\nBox 3: dataVersion -\ndataVersion is the schema version of the data object. The publisher defines the schema version.\nScenario: Authentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\nThe following example shows the properties that are used by all event publishers:\n[\n{\n\"topic\": string,\n\"subject\": string,\n\"id\": string,\n\"eventType\": string,\n\"eventTime\": string,\n\"data\":{\nobject-unique-to-each-publisher\n},\n\"dataVersion\": string,\n\"metadataVersion\": string\n}\n]\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/event-schema",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: id -\nid is a unique identifier for the event.\n\nBox 2: eventType -\neventType is one of the registered event types for this event source.\n\nBox 3: dataVersion -\ndataVersion is the schema version of the data object. The publisher defines the schema version.\nScenario: Authentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\nThe following example shows the properties that are used by all event publishers:\n[\n{\n\"topic\": string,\n\"subject\": string,\n\"id\": string,\n\"eventType\": string,\n\"eventTime\": string,\n\"data\":{\nobject-unique-to-each-publisher\n},\n\"dataVersion\": string,\n\"metadataVersion\": string\n}\n]\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/event-schema",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 25",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nHOTSPOT -\nYou need to implement the Log policy.\nHow should you complete the EnsureLogging method in EventGridController.cs? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0053800001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0053900001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0054500001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: logdrop -\nAll log files should be saved to a container named logdrop.\n\nBox 2: 15 -\nLogs must remain in the container for 15 days.\nBox 3: UpdateApplicationSettings\nAll Azure App Service Web Apps must write logs to Azure Blob storage.\nReference:\nhttps://blog.hompus.nl/2017/05/29/adding-application-logging-blob-to-a-azure-web-app-service-using-powershell/",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: logdrop -\nAll log files should be saved to a container named logdrop.\n\nBox 2: 15 -\nLogs must remain in the container for 15 days.\nBox 3: UpdateApplicationSettings\nAll Azure App Service Web Apps must write logs to Azure Blob storage.\nReference:\nhttps://blog.hompus.nl/2017/05/29/adding-application-logging-blob-to-a-azure-web-app-service-using-powershell/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 26",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nYou are a developer for Proseware, Inc. You are developing an application that applies a set of governance policies for Proseware's internal services, external services, and applications. The application will also provide a shared library for common functionality.\n\nRequirements -\n\nPolicy service -\nYou develop and deploy a stateful ASP.NET Core 2.1 web application named Policy service to an Azure App Service Web App. The application reacts to events from Azure Event Grid and performs policy actions based on those events.\nThe application must include the Event Grid Event ID field in all Application Insights telemetry.\nPolicy service must use Application Insights to automatically scale with the number of policy actions that it is performing.\n\nPolicies -\n\nLog policy -\nAll Azure App Service Web Apps must write logs to Azure Blob storage. All log files should be saved to a container named logdrop. Logs must remain in the container for 15 days.\n\nAuthentication events -\nAuthentication events are used to monitor users signing in and signing out. All authentication events must be processed by Policy service. Sign outs must be processed as quickly as possible.\n\nPolicyLib -\nYou have a shared library named PolicyLib that contains functionality common to all ASP.NET Core web services and applications. The PolicyLib library must:\nExclude non-user actions from Application Insights telemetry.\nProvide methods that allow a web service to scale itself.\nEnsure that scaling actions do not disrupt application usage.\n\nOther -\n\nAnomaly detection service -\nYou have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service. If an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\n\nHealth monitoring -\nAll web applications and services have health monitoring at the /health service endpoint.\n\nIssues -\n\nPolicy loss -\nWhen you deploy Policy service, policies may not be applied if they were in the process of being applied during the deployment.\n\nPerformance issue -\nWhen under heavy load, the anomaly detection service undergoes slowdowns and rejects connections.\n\nNotification latency -\nUsers report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\n\nApp code -\n\nEventGridController.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\n\n\nLoginEvent.cs -\nRelevant portions of the app files are shown below. Line numbers are included for reference only and include a two-character prefix that denotes the specific file to which they belong.\nQuestion\nYou need to resolve a notification latency issue.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0005700001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0005800001.png"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Set Always On to true.",
            "B. Ensure that the Azure Function is using an App Service plan.",
            "C. Set Always On to false.",
            "D. Ensure that the Azure Function is set to use a consumption plan."
        ],
        "answersAre": [
            "A",
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Azure Functions can run on either a Consumption Plan or a dedicated App Service Plan. If you run in a dedicated mode, you need to turn on the Always On setting for your Function App to run properly. The Function runtime will go idle after a few minutes of inactivity, so only HTTP triggers will actually \"wake up\" your functions. This is similar to how WebJobs must have Always On enabled.\nScenario: Notification latency: Users report that anomaly detection emails can sometimes arrive several minutes after an anomaly is detected.\nAnomaly detection service: You have an anomaly detection service that analyzes log information for anomalies. It is implemented as an Azure Machine Learning model. The model is deployed as a web service.\nIf an anomaly is detected, an Azure Function that emails administrators is called by using an HTTP WebHook.\nReference:\nhttps://github.com/Azure/Azure-Functions/wiki/Enable-Always-On-when-running-on-dedicated-App-Service-Plan",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 27",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\nMessages are sent to ContentUploadService.\nContent is processed by ContentAnalysisService.\nAfter processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nHOTSPOT -\nYou need to ensure that validation testing is triggered per the requirements.\nHow should you complete the code segment? To answer, select the appropriate values in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0006200001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0006300001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0006400001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: RepositoryUpdated -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nBox 2: service -\n\nBox 3: imageCollection -\nReference:\nhttps://docs.microsoft.com/en-us/azure/devops/notifications/oob-supported-event-types",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: RepositoryUpdated -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nBox 2: service -\n\nBox 3: imageCollection -\nReference:\nhttps://docs.microsoft.com/en-us/azure/devops/notifications/oob-supported-event-types",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 27",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\nMessages are sent to ContentUploadService.\nContent is processed by ContentAnalysisService.\nAfter processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nYou need to deploy the CheckUserContent Azure Function. The solution must meet the security and cost requirements.\nWhich hosting model should you use?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0006200001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0006300001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Premium plan",
            "B. App Service plan Most Voted",
            "C. Consumption plan"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [
            "B"
        ],
        "descriptionIs": "Scenario:\nYou must minimize costs for all Azure services.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nBest for long-running scenarios where Durable Functions can't be used. Consider an App Service plan in the following situations:\n\u2711 You have existing, underutilized VMs that are already running other App Service instances.\n\u2711 You want to provide a custom image on which to run your functions.\n\u2711 Predictive scaling and costs are required.\nNote: When you create a function app in Azure, you must choose a hosting plan for your app. There are three basic hosting plans available for Azure Functions:\nConsumption plan, Premium plan, and Dedicated (App Service) plan.\nIncorrect Answers:\nA: A Premium plan would be more costly.\nC: Need the VNET functionality.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-scale",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 28",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You are a developer implementing highly scalable and resilient applications to support online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles. The application sends data to several printers. The application consists of five modules that run independently on virtual machines (VMs). Coho Winery plans to move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork and text for custom label designs.\n\nRequirements. Data -\nYou identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\n\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to documents, resources, and applications to external partners.\n\nExternal partners must use their own credentials and authenticate with their organization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user account administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key Vault.\nE-commerce application sign-ins must be secured by using Azure App Service authentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect company content.\nThe LabelMaker application must be secured by using an AAD account that has full access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\n\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\n\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\n\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included for reference only.\nThis JSON file contains a representation of the data for an order that includes a single item.\n\nOrder.json -\nQuestion\nDRAG DROP -\nYou need to deploy a new version of the LabelMaker application to ACR.\nWhich three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.\nSelect and Place:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0006800003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0007000001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0007200001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0007300001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Step 1: Build a new application image by using dockerfile\nStep 2: Create an alias if the image with the fully qualified path to the registry\nBefore you can push the image to a private registry, you've to ensure a proper image name. This can be achieved using the docker tag command. For demonstration purpose, we'll use Docker's hello world image, rename it and push it to ACR.\n# pulls hello-world from the public docker hub\n$ docker pull hello-world\n# tag the image in order to be able to push it to a private registry\n$ docker tag hello-word <REGISTRY_NAME>/hello-world\n# push the image\n$ docker push <REGISTRY_NAME>/hello-world\nStep 3: Log in to the registry and push image\nIn order to push images to the newly created ACR instance, you need to login to ACR form the Docker CLI. Once logged in, you can push any existing docker image to your ACR instance.\nScenario:\nCoho Winery plans to move the application to Azure and continue to support label creation.\n\nLabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\nReference:\nhttps://thorsten-hans.com/how-to-use-a-private-azure-container-registry-with-kubernetes-9b86e67b93b6 https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tutorial-quick-task",
        "mostVotedAre": [],
        "descriptionIs": "Step 1: Build a new application image by using dockerfile\nStep 2: Create an alias if the image with the fully qualified path to the registry\nBefore you can push the image to a private registry, you've to ensure a proper image name. This can be achieved using the docker tag command. For demonstration purpose, we'll use Docker's hello world image, rename it and push it to ACR.\n# pulls hello-world from the public docker hub\n$ docker pull hello-world\n# tag the image in order to be able to push it to a private registry\n$ docker tag hello-word <REGISTRY_NAME>/hello-world\n# push the image\n$ docker push <REGISTRY_NAME>/hello-world\nStep 3: Log in to the registry and push image\nIn order to push images to the newly created ACR instance, you need to login to ACR form the Docker CLI. Once logged in, you can push any existing docker image to your ACR instance.\nScenario:\nCoho Winery plans to move the application to Azure and continue to support label creation.\n\nLabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\nReference:\nhttps://thorsten-hans.com/how-to-use-a-private-azure-container-registry-with-kubernetes-9b86e67b93b6 https://docs.microsoft.com/en-us/azure/container-registry/container-registry-tutorial-quick-task",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 28",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nLabelMaker app -\nCoho Winery produces, bottles, and distributes a variety of wines globally. You are a developer implementing highly scalable and resilient applications to support online order processing by using Azure solutions.\nCoho Winery has a LabelMaker application that prints labels for wine bottles. The application sends data to several printers. The application consists of five modules that run independently on virtual machines (VMs). Coho Winery plans to move the application to Azure and continue to support label creation.\nExternal partners send data to the LabelMaker application to include artwork and text for custom label designs.\n\nRequirements. Data -\nYou identify the following requirements for data management and manipulation:\nOrder data is stored as nonrelational JSON and must be queried using SQL.\nChanges to the Order data must reflect immediately across all partitions. All reads to the Order data must fetch the most recent writes.\n\nRequirements. Security -\nYou have the following security requirements:\nUsers of Coho Winery applications must be able to provide access to documents, resources, and applications to external partners.\n\nExternal partners must use their own credentials and authenticate with their organization's identity management solution.\nExternal partner logins must be audited monthly for application use by a user account administrator to maintain company compliance.\nStorage of e-commerce application settings must be maintained in Azure Key Vault.\nE-commerce application sign-ins must be secured by using Azure App Service authentication and Azure Active Directory (AAD).\nConditional access policies must be applied at the application level to protect company content.\nThe LabelMaker application must be secured by using an AAD account that has full access to all namespaces of the Azure Kubernetes Service (AKS) cluster.\n\nRequirements. LabelMaker app -\nAzure Monitor Container Health must be used to monitor the performance of workloads that are deployed to Kubernetes environments and hosted on Azure\nKubernetes Service (AKS).\nYou must use Azure Container Registry to publish images that support the AKS deployment.\n\nArchitecture -\n\n\nIssues -\nCalls to the Printer API App fail periodically due to printer communication timeouts.\nPrinter communication timeouts occur after 10 seconds. The label printer must only receive up to 5 attempts within one minute.\nThe order workflow fails to run upon initial deployment to Azure.\n\nOrder.json -\nRelevant portions of the app files are shown below. Line numbers are included for reference only.\nThis JSON file contains a representation of the data for an order that includes a single item.\n\nOrder.json -\nQuestion\nYou need to access data from the user claim object in the e-commerce web app.\nWhat should you do first?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0006800003.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0007000001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0007200001.png"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Write custom code to make a Microsoft Graph API call from the e-commerce web app.",
            "B. Assign the Contributor RBAC role to the e-commerce web app by using the Resource Manager create role assignment API.",
            "C. Update the e-commerce web app to read the HTTP request header values. Most Voted",
            "D. Using the Azure CLI, enable Cross-origin resource sharing (CORS) from the e-commerce checkout API to the e-commerce web app."
        ],
        "answersAre": [
            "C"
        ],
        "mostVotedAre": [
            "C"
        ],
        "descriptionIs": "Methods to Get User Identity and Claims in a .NET Azure Functions App include:\n\u2711 ClaimsPrincipal from the Request Context\nThe ClaimsPrincipal object is also available as part of the request context and can be extracted from the HttpRequest.HttpContext.\n\u2711 User Claims from the Request Headers.\nApp Service passes user claims to the app by using special request headers.\nReference:\nhttps://levelup.gitconnected.com/four-alternative-methods-to-get-user-identity-and-claims-in-a-net-azure-functions-app-df98c40424bb",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 29",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nHOTSPOT -\nYou need to implement the retail store location Azure Function.\nHow should you configure the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0007900001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Scenario: Retail store locations: Azure Functions must process data immediately when data is uploaded to Blob storage.\n\nBox 1: HTTP -\nBinding configuration example: https://<storage_account_name>.blob.core.windows.net\n\nBox 2: Input -\nRead blob storage data in a function: Input binding\n\nBox 3: Blob storage -\nThe Blob storage trigger starts a function when a new or updated blob is detected.\nAzure Functions integrates with Azure Storage via triggers and bindings. Integrating with Blob storage allows you to build functions that react to changes in blob data as well as read and write values.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger",
        "mostVotedAre": [],
        "descriptionIs": "Scenario: Retail store locations: Azure Functions must process data immediately when data is uploaded to Blob storage.\n\nBox 1: HTTP -\nBinding configuration example: https://<storage_account_name>.blob.core.windows.net\n\nBox 2: Input -\nRead blob storage data in a function: Input binding\n\nBox 3: Blob storage -\nThe Blob storage trigger starts a function when a new or updated blob is detected.\nAzure Functions integrates with Azure Storage via triggers and bindings. Integrating with Blob storage allows you to build functions that react to changes in blob data as well as read and write values.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 29",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nHOTSPOT -\nYou need to implement the corporate website.\nHow should you configure the solution? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0008300001.png"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: Standard -\nBelow is a high-level comparison of the features as per the pricing tier for the App Service Plan.\n\n\nNote: Corporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\nCorporate website requirements:\n\u2711 Secure the website by using SSL.\n\u2711 Minimize costs for data storage and hosting.\n\u2711 Implement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\n\u2711 Distribute the website content globally for local use.\n\u2711 Implement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\n\u2711 The website must have 99.95 percent uptime.\n\nBox 2: App Service Web App -\nA Web App is a web application that is hosted in an App Service. The App Service is the managed service in Azure that enables you to deploy a web application and make it available to your customers on the Internet in a very short amount of time.\nIncorrect:\nA Static Web Application is any web application that can be delivered directly to an end user's browser without any server-side alteration of the HTML, CSS, or\nJavaScript content.\nReference:\nhttps://azure-training.com/2018/12/27/understanding-app-services-app-service-plan-and-ase/ https://docs.microsoft.com/en-us/azure/app-service/overviewhttps://www.examtopics.com/assets/media/exam-media/04273/0008500001.jpg",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: Standard -\nBelow is a high-level comparison of the features as per the pricing tier for the App Service Plan.\n\n\nNote: Corporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\nCorporate website requirements:\n\u2711 Secure the website by using SSL.\n\u2711 Minimize costs for data storage and hosting.\n\u2711 Implement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\n\u2711 Distribute the website content globally for local use.\n\u2711 Implement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\n\u2711 The website must have 99.95 percent uptime.\n\nBox 2: App Service Web App -\nA Web App is a web application that is hosted in an App Service. The App Service is the managed service in Azure that enables you to deploy a web application and make it available to your customers on the Internet in a very short amount of time.\nIncorrect:\nA Static Web Application is any web application that can be delivered directly to an end user's browser without any server-side alteration of the HTML, CSS, or\nJavaScript content.\nReference:\nhttps://azure-training.com/2018/12/27/understanding-app-services-app-service-plan-and-ase/ https://docs.microsoft.com/en-us/azure/app-service/overview",
        "descriptionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0008500001.jpg"
        ],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 30",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nVanArsdel, Ltd. is a global office supply company. The company is based in Canada and has retail store locations across the world. The company is developing several cloud-based solutions to support their stores, distributors, suppliers, and delivery services.\n\nCurrent environment -\n\nCorporate website -\nThe company provides a public website located at http://www.vanarsdelltd.com. The website consists of a React JavaScript user interface, HTML, CSS, image assets, and several APIs hosted in Azure Functions.\n\nRetail Store Locations -\nThe company supports thousands of store locations globally. Store locations send data every hour to an Azure Blob storage account to support inventory, purchasing and delivery services. Each record includes a location identifier and sales transaction information.\n\nRequirements -\nThe application components must meet the following requirements:\n\nCorporate website -\nSecure the website by using SSL.\nMinimize costs for data storage and hosting.\nImplement native GitHub workflows for continuous integration and continuous deployment (CI/CD).\nDistribute the website content globally for local use.\nImplement monitoring by using Application Insights and availability web tests including SSL certificate validity and custom header value verification.\nThe website must have 99.95 percent uptime.\n\nRetail store locations -\nAzure Functions must process data immediately when data is uploaded to Blob storage. Azure Functions must update Azure Cosmos DB by using native SQL language queries.\nAudit store sale transaction information nightly to validate data, process sales financials, and reconcile inventory.\n\nDelivery services -\nStore service telemetry data in Azure Cosmos DB by using an Azure Function. Data must include an item id, the delivery vehicle license plate, vehicle package capacity, and current vehicle location coordinates.\nStore delivery driver profile information in Azure Active Directory (Azure AD) by using an Azure Function called from the corporate website.\n\nInventory services -\nThe company has contracted a third-party to develop an API for inventory processing that requires access to a specific blob within the retail store storage account for three months to include read-only access to the data.\n\nSecurity -\nAll Azure Functions must centralize management and distribution of configuration data for different environments and geographies, encrypted by using a company-provided RSA-HSM key.\nAuthentication and authorization must use Azure AD and services must use managed identities where possible.\n\nIssues -\n\nRetail Store Locations -\nYou must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nAzure Cosmos DB queries from the Azure Function exhibit high Request Unit (RU) usage and contain multiple, complex queries that exhibit high point read latency for large items as the function app is scaling.\nQuestion\nYou need to implement a solution to resolve the retail store location data issue.\nWhich three Azure Blob features should you enable? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Soft delete Most Voted",
            "B. Change feed Most Voted",
            "C. Snapshots",
            "D. Versioning Most Voted",
            "E. Object replication",
            "F. Immutability"
        ],
        "answersAre": [
            "A",
            "B",
            "D"
        ],
        "mostVotedAre": [
            "A",
            "B",
            "D"
        ],
        "descriptionIs": "Scenario: You must perform a point-in-time restoration of the retail store location data due to an unexpected and accidental deletion of data.\nBefore you enable and configure point-in-time restore, enable its prerequisites for the storage account: soft delete, change feed, and blob versioning.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-manage",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 31",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nYou need to store the user agreements.\nWhere should you store the agreement after it is completed?",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0016800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0016900001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Azure Storage queue",
            "B. Azure Event Hub",
            "C. Azure Service Bus topic",
            "D. Azure Event Grid topic"
        ],
        "answersAre": [
            "B"
        ],
        "mostVotedAre": [],
        "descriptionIs": "Azure Event Hub is used for telemetry and distributed data streaming.\nThis service provides a single solution that enables rapid data retrieval for real-time processing as well as repeated replay of stored raw data. It can capture the streaming data into a file for processing and analysis.\nIt has the following characteristics:\n\u2711 low latency\n\u2711 capable of receiving and processing millions of events per second\n\u2711 at least once delivery\nReference:\nhttps://docs.microsoft.com/en-us/azure/event-grid/compare-messaging-services",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #2 -- Topic 31",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nHOTSPOT -\nYou need to implement the bindings for the CheckUserContent function.\nHow should you complete the code segment? To answer, select the appropriate options in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0016800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0016900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0017100001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Box 1: [BlobTrigger(..)]\nBox 2: [Blob(..)]\nAzure Blob storage output binding for Azure Functions. The output binding allows you to modify and delete blob storage data in an Azure Function.\nThe attribute's constructor takes the path to the blob and a FileAccess parameter indicating read or write, as shown in the following example:\n[FunctionName(\"ResizeImage\")]\npublic static void Run(\n[BlobTrigger(\"sample-images/{name}\")] Stream image,\n[Blob(\"sample-images-md/{name}\", FileAccess.Write)] Stream imageSmall)\n{\n...\n}\nScenario: You must create an Azure Function named CheckUserContent to perform the content checks.\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-output",
        "mostVotedAre": [],
        "descriptionIs": "Box 1: [BlobTrigger(..)]\nBox 2: [Blob(..)]\nAzure Blob storage output binding for Azure Functions. The output binding allows you to modify and delete blob storage data in an Azure Function.\nThe attribute's constructor takes the path to the blob and a FileAccess parameter indicating read or write, as shown in the following example:\n[FunctionName(\"ResizeImage\")]\npublic static void Run(\n[BlobTrigger(\"sample-images/{name}\")] Stream image,\n[Blob(\"sample-images-md/{name}\", FileAccess.Write)] Stream imageSmall)\n{\n...\n}\nScenario: You must create an Azure Function named CheckUserContent to perform the content checks.\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nReference:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-output",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #3 -- Topic 31",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\n\nOverview -\nYou are a developer for Contoso, Ltd. The company has a social networking website that is developed as a Single Page Application (SPA). The main web application for the social networking website loads user uploaded content from blob storage.\nYou are developing a solution to monitor uploaded data for inappropriate content. The following process occurs when users upload content by using the SPA:\n* Messages are sent to ContentUploadService.\n* Content is processed by ContentAnalysisService.\n* After processing is complete, the content is posted to the social network or a rejection message is posted in its place.\nThe ContentAnalysisService is deployed with Azure Container Instances from a private Azure Container Registry named contosoimages.\nThe solution will use eight CPU cores.\n\nAzure Active Directory -\nContoso, Ltd. uses Azure Active Directory (Azure AD) for both internal and guest accounts.\n\nRequirements -\n\nContentAnalysisService -\nThe company's data science group built ContentAnalysisService which accepts user generated content as a string and returns a probable value for inappropriate content. Any values over a specific threshold must be reviewed by an employee of Contoso, Ltd.\nYou must create an Azure Function named CheckUserContent to perform the content checks.\n\nCosts -\nYou must minimize costs for all Azure services.\n\nManual review -\nTo review content, the user must authenticate to the website portion of the ContentAnalysisService using their Azure AD credentials. The website is built using\nReact and all pages and API endpoints require authentication. In order to review content a user must be part of a ContentReviewer role. All completed reviews must include the reviewer's email address for auditing purposes.\n\nHigh availability -\nAll services must run in multiple regions. The failure of any service in a region must not impact overall application availability.\n\nMonitoring -\nAn alert must be raised if the ContentUploadService uses more than 80 percent of available CPU cores.\n\nSecurity -\nYou have the following security requirements:\nAny web service accessible over the Internet must be protected from cross site scripting attacks.\nAll websites and services must use SSL from a valid root certificate authority.\nAzure Storage access keys must only be stored in memory and must be available only to the service.\nAll Internal services must only be accessible from internal Virtual Networks (VNets).\nAll parts of the system must support inbound and outbound traffic restrictions.\nAll service calls must be authenticated by using Azure AD.\n\nUser agreements -\nWhen a user submits content, they must agree to a user agreement. The agreement allows employees of Contoso, Ltd. to review content, store cookies on user devices, and track user's IP addresses.\nInformation regarding agreements is used by multiple divisions within Contoso, Ltd.\nUser responses must not be lost and must be available to all parties regardless of individual service uptime. The volume of agreements is expected to be in the millions per hour.\n\nValidation testing -\nWhen a new version of the ContentAnalysisService is available the previous seven days of content must be processed with the new version to verify that the new version does not significantly deviate from the old version.\n\nIssues -\nUsers of the ContentUploadService report that they occasionally see HTTP 502 responses on specific pages.\n\nCode -\n\nContentUploadService -\n\n\nApplicationManifest -\nQuestion\nYou need to configure the ContentUploadService deployment.\nWhich two actions should you perform? Each correct answer presents part of the solution.\nNOTE: Each correct selection is worth one point.",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0016800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0016900001.jpg"
        ],
        "isMCQ": true,
        "myOptionsAre": [
            "A. Add the following markup to line CS23: type: Private Most Voted",
            "B. Add the following markup to line CS24: osType: Windows",
            "C. Add the following markup to line CS24: osType: Linux Most Voted",
            "D. Add the following markup to line CS23: type: Public"
        ],
        "answersAre": [
            "A"
        ],
        "mostVotedAre": [
            "A",
            "C"
        ],
        "descriptionIs": "Scenario: All Internal services must only be accessible from Internal Virtual Networks (VNets)\nThere are three Network Location types \u05d2\u20ac\" Private, Public and Domain\nReference:\nhttps://devblogs.microsoft.com/powershell/setting-network-location-to-private/",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    },
    {
        "questionNumber": "Question #1 -- Topic 32",
        "questionIs": "Introductory Info\nCase study -\nThis is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.\nTo answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.\nAt the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.\n\nTo start the case study -\nTo display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. When you are ready to answer a question, click the Question button to return to the question.\n\nBackground -\nCity Power & Light company provides electrical infrastructure monitoring solutions for homes and businesses. The company is migrating solutions to Azure.\n\nCurrent environment -\n\nArchitecture overview -\nThe company has a public website located at http://www.cpandl.com/. The site is a single-page web application that runs in Azure App Service on Linux. The website uses files stored in Azure Storage and cached in Azure Content Delivery Network (CDN) to serve static content.\nAPI Management and Azure Function App functions are used to process and store data in Azure Database for PostgreSQL. API Management is used to broker communications to the Azure Function app functions for Logic app integration. Logic apps are used to orchestrate the data processing while Service Bus and\nEvent Grid handle messaging and events.\nThe solution uses Application Insights, Azure Monitor, and Azure Key Vault.\n\nArchitecture diagram -\nThe company has several applications and services that support their business. The company plans to implement serverless computing where possible. The overall architecture is shown below.\n\n\nUser authentication -\nThe following steps detail the user authentication process:\n1. The user selects Sign in in the website.\n2. The browser redirects the user to the Azure Active Directory (Azure AD) sign in page.\n3. The user signs in.\n4. Azure AD redirects the user's session back to the web application. The URL includes an access token.\n5. The web application calls an API and includes the access token in the authentication header. The application ID is sent as the audience ('aud') claim in the access token.\n6. The back-end API validates the access token.\n\nRequirements -\n\nCorporate website -\nCommunications and content must be secured by using SSL.\nCommunications must use HTTPS.\nData must be replicated to a secondary region and three availability zones.\nData storage costs must be minimized.\n\nAzure Database for PostgreSQL -\nThe database connection string is stored in Azure Key Vault with the following attributes:\nAzure Key Vault name: cpandlkeyvault\nSecret name: PostgreSQLConn\nId: 80df3e46ffcd4f1cb187f79905e9a1e8\nThe connection information is updated frequently. The application must always use the latest information to connect to the database.\nAzure Service Bus and Azure Event Grid\nAzure Event Grid must use Azure Service Bus for queue-based load leveling.\nEvents in Azure Event Grid must be routed directly to Service Bus queues for use in buffering.\nEvents from Azure Service Bus and other Azure services must continue to be routed to Azure Event Grid for processing.\n\nSecurity -\nAll SSL certificates and credentials must be stored in Azure Key Vault.\nFile access must restrict access by IP, protocol, and Azure AD rights.\nAll user accounts and processes must receive only those privileges which are essential to perform their intended function.\n\nCompliance -\nAuditing of the file updates and transfers must be enabled to comply with General Data Protection Regulation (GDPR). The file updates must be read-only, stored in the order in which they occurred, include only create, update, delete, and copy operations, and be retained for compliance reasons.\n\nIssues -\n\nCorporate website -\nWhile testing the site, the following error message displays:\nCryptographicException: The system cannot find the file specified.\n\nFunction app -\nYou perform local testing for the RequestUserApproval function. The following error message displays:\n'Timeout value of 00:10:00 exceeded by function: RequestUserApproval'\nThe same error message displays when you test the function in an Azure development environment when you run the following Kusto query:\n\nFunctionAppLogs -\n| where FunctionName = = \"RequestUserApproval\"\n\nLogic app -\nYou test the Logic app in a development environment. The following error message displays:\n'400 Bad Request'\nTroubleshooting of the error shows an HttpTrigger action to call the RequestUserApproval function.\n\nCode -\n\nCorporate website -\nSecurity.cs:\n\n\nFunction app -\nRequestUserApproval.cs:\nQuestion\nHOTSPOT -\nYou need to configure the Account Kind, Replication, and Access tier options for the corporate website's Azure Storage account.\nHow should you complete the configuration? To answer, select the appropriate options in the dialog box in the answer area.\nNOTE: Each correct selection is worth one point.\nHot Area:",
        "questionImages": [
            "https://www.examtopics.com/assets/media/exam-media/04273/0017600001.png",
            "https://www.examtopics.com/assets/media/exam-media/04273/0017800001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0017900001.jpg",
            "https://www.examtopics.com/assets/media/exam-media/04273/0018000001.jpg"
        ],
        "isMCQ": false,
        "myOptionsAre": null,
        "answersAre": "Account Kind: StorageV2 (general-purpose v2)\nScenario: Azure Storage blob will be used (refer to the exhibit). Data storage costs must be minimized.\nGeneral-purpose v2 accounts: Basic storage account type for blobs, files, queues, and tables. Recommended for most scenarios using Azure Storage.\nIncorrect Answers:\n\u2711 BlockBlobStorage accounts: Storage accounts with premium performance characteristics for block blobs and append blobs. Recommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency.\n\u2711 General-purpose v1 accounts: Legacy account type for blobs, files, queues, and tables. Use general-purpose v2 accounts instead when possible.\nReplication: Geo-redundant Storage\nScenario: Data must be replicated to a secondary region and three availability zones.\nGeo-redundant storage (GRS) copies your data synchronously three times within a single physical location in the primary region using LRS. It then copies your data asynchronously to a single physical location in the secondary region.\nIncorrect Answers:\nGeo-zone-redundant storage (GZRS), but it would be more costly.\n\nAccess tier: Cool -\nData storage costs must be minimized.\nNote: Azure storage offers different access tiers, which allow you to store blob object data in the most cost-effective manner. The available access tiers include:\nHot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30 days.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?tabs=azure-portal",
        "mostVotedAre": [],
        "descriptionIs": "Account Kind: StorageV2 (general-purpose v2)\nScenario: Azure Storage blob will be used (refer to the exhibit). Data storage costs must be minimized.\nGeneral-purpose v2 accounts: Basic storage account type for blobs, files, queues, and tables. Recommended for most scenarios using Azure Storage.\nIncorrect Answers:\n\u2711 BlockBlobStorage accounts: Storage accounts with premium performance characteristics for block blobs and append blobs. Recommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency.\n\u2711 General-purpose v1 accounts: Legacy account type for blobs, files, queues, and tables. Use general-purpose v2 accounts instead when possible.\nReplication: Geo-redundant Storage\nScenario: Data must be replicated to a secondary region and three availability zones.\nGeo-redundant storage (GRS) copies your data synchronously three times within a single physical location in the primary region using LRS. It then copies your data asynchronously to a single physical location in the secondary region.\nIncorrect Answers:\nGeo-zone-redundant storage (GZRS), but it would be more costly.\n\nAccess tier: Cool -\nData storage costs must be minimized.\nNote: Azure storage offers different access tiers, which allow you to store blob object data in the most cost-effective manner. The available access tiers include:\nHot - Optimized for storing data that is accessed frequently.\nCool - Optimized for storing data that is infrequently accessed and stored for at least 30 days.\nReference:\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers?tabs=azure-portal",
        "descriptionImages": [],
        "linkIs": "https://www.examtopics.com/exams/microsoft/az-204/view/"
    }
]